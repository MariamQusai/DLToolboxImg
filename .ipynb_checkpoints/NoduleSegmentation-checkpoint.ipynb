{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nodule Segmentation in Lung Ct-scans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Configuration\" data-toc-modified-id=\"Configuration-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Configuration</a></span></li><li><span><a href=\"#LIDC-Dataset\" data-toc-modified-id=\"LIDC-Dataset-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>LIDC Dataset</a></span><ul class=\"toc-item\"><li><span><a href=\"#Description\" data-toc-modified-id=\"Description-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Description</a></span></li><li><span><a href=\"#Load-Dataset\" data-toc-modified-id=\"Load-Dataset-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Load Dataset</a></span></li><li><span><a href=\"#Split-scans-to-train,-validate,-and-test\" data-toc-modified-id=\"Split-scans-to-train,-validate,-and-test-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Split scans to train, validate, and test</a></span></li><li><span><a href=\"#Filter-annotation-according-to-nodule-diameter\" data-toc-modified-id=\"Filter-annotation-according-to-nodule-diameter-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Filter annotation according to nodule diameter</a></span></li><li><span><a href=\"#Plot-distribution-of-nodule-diameter\" data-toc-modified-id=\"Plot-distribution-of-nodule-diameter-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Plot distribution of nodule diameter</a></span></li><li><span><a href=\"#Save-train,-validate,-and-test-scan-indices-in-a-file\" data-toc-modified-id=\"Save-train,-validate,-and-test-scan-indices-in-a-file-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>Save train, validate, and test scan indices in a file</a></span></li></ul></li><li><span><a href=\"#Nodule-Dataset\" data-toc-modified-id=\"Nodule-Dataset-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Nodule Dataset</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Description\" data-toc-modified-id=\"Description-3.0.1\"><span class=\"toc-item-num\">3.0.1&nbsp;&nbsp;</span>Description</a></span></li><li><span><a href=\"#Generate-Negative-Examples\" data-toc-modified-id=\"Generate-Negative-Examples-3.0.2\"><span class=\"toc-item-num\">3.0.2&nbsp;&nbsp;</span>Generate Negative Examples</a></span></li><li><span><a href=\"#Read-Negative-Examples\" data-toc-modified-id=\"Read-Negative-Examples-3.0.3\"><span class=\"toc-item-num\">3.0.3&nbsp;&nbsp;</span>Read Negative Examples</a></span></li><li><span><a href=\"#Generate-Positive-Examples\" data-toc-modified-id=\"Generate-Positive-Examples-3.0.4\"><span class=\"toc-item-num\">3.0.4&nbsp;&nbsp;</span>Generate Positive Examples</a></span></li><li><span><a href=\"#Read-Positive-Examples\" data-toc-modified-id=\"Read-Positive-Examples-3.0.5\"><span class=\"toc-item-num\">3.0.5&nbsp;&nbsp;</span>Read Positive Examples</a></span></li><li><span><a href=\"#Combine-both-into-a-distribution-of-n:m-(pos:neg)\" data-toc-modified-id=\"Combine-both-into-a-distribution-of-n:m-(pos:neg)-3.0.6\"><span class=\"toc-item-num\">3.0.6&nbsp;&nbsp;</span>Combine both into a distribution of n:m (pos:neg)</a></span></li><li><span><a href=\"#Data-Augmentation\" data-toc-modified-id=\"Data-Augmentation-3.0.7\"><span class=\"toc-item-num\">3.0.7&nbsp;&nbsp;</span>Data Augmentation</a></span></li><li><span><a href=\"#Convert-into-rec-file\" data-toc-modified-id=\"Convert-into-rec-file-3.0.8\"><span class=\"toc-item-num\">3.0.8&nbsp;&nbsp;</span>Convert into rec file</a></span></li><li><span><a href=\"#Repeat-for-validate\" data-toc-modified-id=\"Repeat-for-validate-3.0.9\"><span class=\"toc-item-num\">3.0.9&nbsp;&nbsp;</span>Repeat for validate</a></span></li><li><span><a href=\"#Repeat-for-test\" data-toc-modified-id=\"Repeat-for-test-3.0.10\"><span class=\"toc-item-num\">3.0.10&nbsp;&nbsp;</span>Repeat for test</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib nbagg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,\"./src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylidc as pl #pip install -Iv scikit-image==0.13\n",
    "from random import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import scipy\n",
    "import random\n",
    "import os\n",
    "from imports import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIDC Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm -r {root_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_all_data = True\n",
    "\n",
    "dataset_path = '/media/mas/Untitled/LIDC/DOI/'\n",
    "\n",
    "root_path = '/home/mas/x110/Datasets/Dataset5/'\n",
    "tmp_path = root_path + \"tmp/\"\n",
    "processed_path = root_path +\"processed/\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir {root_path}\n",
    "!mkdir {tmp_path}\n",
    "!mkdir {tmp_path+\"train/\"}\n",
    "!mkdir {tmp_path+\"valid/\"}\n",
    "!mkdir {tmp_path+\"test/\"}\n",
    "!mkdir {tmp_path+\"train/pos/\"}\n",
    "!mkdir {tmp_path+\"train/neg/\"}\n",
    "!mkdir {tmp_path+\"valid/pos/\"}\n",
    "!mkdir {tmp_path+\"valid/neg/\"}\n",
    "!mkdir {tmp_path+\"test/pos/\"}\n",
    "!mkdir {tmp_path+\"test/neg/\"}\n",
    "!mkdir {processed_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split_path =   tmp_path + \"scan_id_split.idx\"#save the scan ids that will be considered\n",
    "\n",
    "dftrain_path =  tmp_path + \"dftrain.csv\"\n",
    "dfvalid_path =  tmp_path + \"dfvalid.csv\"\n",
    "dftest_path  =  tmp_path + \"dftest.csv\"\n",
    "\n",
    "ds_train = 'train'\n",
    "ds_test = 'test'\n",
    "ds_valid = 'valid'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The Lung Image Database Consortium image collection (LIDC-IDRI) consists of diagnostic and lung cancer screening thoracic computed tomography (CT) scans with marked-up annotated lesions. It is a web-accessible international resource for development, training, and evaluation of computer-assisted diagnostic (CAD) methods for lung cancer detection and diagnosis. Initiated by the National Cancer Institute (NCI), further advanced by the Foundation for the National Institutes of Health (FNIH), and accompanied by the Food and Drug Administration (FDA) through active participation, this public-private partnership demonstrates the success of a consortium founded on a consensus-based process.\n",
    "\n",
    "Seven academic centers and eight medical imaging companies collaborated to create this data set which contains **1018 cases**.  Each subject includes images from a clinical thoracic CT scan and an associated XML file that records the results of a two-phase image annotation process performed by four experienced thoracic radiologists. In the initial blinded-read phase, each radiologist independently reviewed each CT scan and marked lesions belonging to one of three categories (\"nodule > or =3 mm,\" \"nodule <3 mm,\" and \"non-nodule > or =3 mm\"). In the subsequent unblinded-read phase, each radiologist independently reviewed their own marks along with the anonymized marks of the three other radiologists to render a final opinion. The goal of this process was to identify as completely as possible all lung nodules in each CT scan without requiring forced consensus.\" [[1]](https://wiki.cancerimagingarchive.net/display/Public/LIDC-IDRI)\n",
    "\n",
    " [Download dataset here](https://wiki.cancerimagingarchive.net/display/Public/LIDC-IDRI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment to download the data\n",
    "#!apt-get install icedtea-netx\n",
    "#!javaws TCIA_LIDC-IDRI_06-22-2015.jnlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = !ls {dataset_path}\n",
    "files = [f for f in files if not f.endswith(\".zip\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_all_data:\n",
    "    files = ['LIDC-IDRI-'+str(i+1).zfill(4) for i in range(12)]\n",
    "#files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "qu = pl.query(pl.Scan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Scan(id=1,patient_id=LIDC-IDRI-0078),\n",
       " Scan(id=2,patient_id=LIDC-IDRI-0069),\n",
       " Scan(id=3,patient_id=LIDC-IDRI-0079),\n",
       " Scan(id=4,patient_id=LIDC-IDRI-0101),\n",
       " Scan(id=5,patient_id=LIDC-IDRI-0110)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scans=qu.all() #all scans in the original LIDC dataset\n",
    "scans[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = [i for i,s in enumerate(scans) if s.patient_id in files]\n",
    "#ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_scans=[scans[i] for i in ind];  #mini_scans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scan=mini_scans[0]\n",
    "#scan.annotations\n",
    "#scan.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1012 1 1013\n"
     ]
    }
   ],
   "source": [
    "#check all scans are ok to read\n",
    "scans_ok=[]\n",
    "scans_error=[]\n",
    "\n",
    "for q in mini_scans:\n",
    "    try:\n",
    "        q.get_path_to_dicom_files()\n",
    "        scans_ok.append(q)\n",
    "    except:\n",
    "        scans_error.append(q)\n",
    "\n",
    "print(len(scans_ok),len(scans_error),len(mini_scans))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split scans to train, validate, and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split scans into 60% for training, 20% for validation, and 20% for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "607 202 203\n"
     ]
    }
   ],
   "source": [
    "#shuffle data, then split to train, valid, test\n",
    "random.seed(313)\n",
    "random.shuffle(scans_ok)\n",
    "L=len(scans_ok)\n",
    "j=np.int(.6*L)\n",
    "jj=np.int(.2*L)\n",
    "scans_train=scans_ok[0:j]\n",
    "scans_valid=scans_ok[j:j+jj]\n",
    "scans_test=scans_ok[j+jj:]\n",
    "print(len(scans_train),len(scans_valid),len(scans_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scans_train,scans_valid,scans_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter annotation according to nodule diameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will consider only annonations of diameter < 30mm  and greater than 6mm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3208\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ann_id</th>\n",
       "      <th>scan_id</th>\n",
       "      <th>nodule_diameter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5606</td>\n",
       "      <td>820</td>\n",
       "      <td>6.557978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5607</td>\n",
       "      <td>820</td>\n",
       "      <td>7.848868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5608</td>\n",
       "      <td>820</td>\n",
       "      <td>7.191991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5609</td>\n",
       "      <td>820</td>\n",
       "      <td>10.411026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5610</td>\n",
       "      <td>820</td>\n",
       "      <td>7.028515</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ann_id  scan_id  nodule_diameter\n",
       "0    5606      820         6.557978\n",
       "1    5607      820         7.848868\n",
       "2    5608      820         7.191991\n",
       "3    5609      820        10.411026\n",
       "4    5610      820         7.028515"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scans0=scans_train\n",
    "filename = dftrain_path\n",
    "\n",
    "#consider only annonations of diameter < 30mm  and greater than 6mm \n",
    "\n",
    "if True:\n",
    "\n",
    "    l=[q.annotations for q in scans0]\n",
    "    anns = [item for sublist in l for item in sublist]\n",
    "\n",
    "    columns=['ann_id','scan_id','nodule_diameter']\n",
    "\n",
    "    df=[]\n",
    "    for scan in scans0:\n",
    "        for a in scan.annotations:\n",
    "            row = [a.id,a.scan_id,a.diameter]\n",
    "            df.append(row)\n",
    "\n",
    "    df1=pd.DataFrame(df,columns=columns)\n",
    "    #keep nodules between 6mm and 30 mm\n",
    "    df2=df1[(df1.nodule_diameter<=30) & (df1.nodule_diameter>=6)]\n",
    "    df2.reset_index(inplace=True,drop=True)\n",
    "    df2.to_csv(filename)\n",
    "    df_train=df2\n",
    "else: \n",
    "    df_train=pd.read_csv(filename,index_col=0)\n",
    "print(df_train.shape[0])\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1006\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ann_id</th>\n",
       "      <th>scan_id</th>\n",
       "      <th>nodule_diameter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3990</td>\n",
       "      <td>566</td>\n",
       "      <td>7.787898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1290</td>\n",
       "      <td>152</td>\n",
       "      <td>7.370298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1291</td>\n",
       "      <td>152</td>\n",
       "      <td>11.103649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1292</td>\n",
       "      <td>152</td>\n",
       "      <td>6.720567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1293</td>\n",
       "      <td>152</td>\n",
       "      <td>7.202769</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ann_id  scan_id  nodule_diameter\n",
       "0    3990      566         7.787898\n",
       "1    1290      152         7.370298\n",
       "2    1291      152        11.103649\n",
       "3    1292      152         6.720567\n",
       "4    1293      152         7.202769"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Repeat for validation set\n",
    "scans0=scans_valid\n",
    "filename = dfvalid_path\n",
    "#consider only annonations of diameter < 30mm  and greater than 6mm \n",
    "\n",
    "if True:\n",
    "\n",
    "    l=[q.annotations for q in scans0]\n",
    "    anns = [item for sublist in l for item in sublist]\n",
    "\n",
    "    columns=['ann_id','scan_id','nodule_diameter']\n",
    "\n",
    "    df=[]\n",
    "    for scan in scans0:\n",
    "        for a in scan.annotations:\n",
    "            row = [a.id,a.scan_id,a.diameter]\n",
    "            df.append(row)\n",
    "\n",
    "    df1=pd.DataFrame(df,columns=columns)\n",
    "    #keep nodules between 6mm and 30 mm\n",
    "    df2=df1[(df1.nodule_diameter<=30) & (df1.nodule_diameter>=6)]\n",
    "    df2.reset_index(inplace=True,drop=True)\n",
    "    df2.to_csv(filename)\n",
    "    df_valid=df2\n",
    "    #print(f\"df1.shape[0],df2.shape[0]:{df1.shape[0]},{df2.shape[0]}\")\n",
    "else: \n",
    "    df_valid=pd.read_csv(filename,index_col=0)\n",
    "print(df_valid.shape[0])\n",
    "df_valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1209\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ann_id</th>\n",
       "      <th>scan_id</th>\n",
       "      <th>nodule_diameter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>500</td>\n",
       "      <td>60</td>\n",
       "      <td>6.971669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>501</td>\n",
       "      <td>60</td>\n",
       "      <td>6.941811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>502</td>\n",
       "      <td>60</td>\n",
       "      <td>10.088506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>503</td>\n",
       "      <td>60</td>\n",
       "      <td>8.599116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>504</td>\n",
       "      <td>60</td>\n",
       "      <td>9.228280</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ann_id  scan_id  nodule_diameter\n",
       "0     500       60         6.971669\n",
       "1     501       60         6.941811\n",
       "2     502       60        10.088506\n",
       "3     503       60         8.599116\n",
       "4     504       60         9.228280"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Repeat for Testing set\n",
    "scans0=scans_test\n",
    "filename = dftest_path\n",
    "#consider only annonations of diameter < 30mm  and greater than 6mm \n",
    "\n",
    "if True:\n",
    "\n",
    "    l=[q.annotations for q in scans0]\n",
    "    anns = [item for sublist in l for item in sublist]\n",
    "\n",
    "    columns=['ann_id','scan_id','nodule_diameter']\n",
    "\n",
    "    df=[]\n",
    "    for scan in scans0:\n",
    "        for a in scan.annotations:\n",
    "            row = [a.id,a.scan_id,a.diameter]\n",
    "            df.append(row)\n",
    "\n",
    "    df1=pd.DataFrame(df,columns=columns)\n",
    "    #keep nodules between 6mm and 30 mm\n",
    "    df2=df1[(df1.nodule_diameter<=30) & (df1.nodule_diameter>=6)]\n",
    "    df2.reset_index(inplace=True,drop=True)\n",
    "    df2.to_csv(filename)\n",
    "    df_test=df2\n",
    "    #print(f\"df1.shape[0],df2.shape[0]:{df1.shape[0]},{df2.shape[0]}\")\n",
    "else: \n",
    "    df_test=pd.read_csv(filename,index_col=0)\n",
    "print(df_test.shape[0])\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot distribution of nodule diameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train.hist(column='nodule_diameter')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAGPNJREFUeJzt3X+QXWWd5/H3x/BzaCaBDdMTk4zNSpxZIDNRbiGWzuxtKDHA7ganlILKaKK4rVVYiztxN8HaHdGRnThrZLV0cdoNQxSkifxYsgEGMdCFTA1gGiNJiK6tNgM9IRlICFzE7CZ894/79M4ldvc9t/ve3Pa5n1dVqs95znPOeb6c5NOHc885VxGBmZnl6w3tHoCZmbWWg97MLHMOejOzzDnozcwy56A3M8ucg97MLHMOejOzzDnobcaTNCjpI83uO5X9S1ou6TvN3L5ZqznozRoQEbdExIVHe7+SVkp65Gjv1/LgoDfrAJKOafcYrH0c9NYykkYkfVLSk5IOSLpN0glp2b+VNCxpn6RNkt5Ys967Jf0orfMVQDXLrpV0c818j6SYKMgkfVjSLkn7Jd0v6U0Fxj3Z/l93Zi3pS5KekfSSpCFJf3jEWL8t6WZJL0vaLuktkq6RtDetd2FN/9mS1kvaLWlU0uckzZL0L4CvAe+QVJH0Yup/vKQvSPp7SXskfU3SiWlZWdKzklZLeg7463p1W74c9NZqlwFLgdOB3wdWSjof+Iu0bB7wNDAAIGkucCfwn4C5wE+Bd05lx5KWAZ8C/hg4DfgecGuddRrd//eBJcCpwLeAb4/9Mkv+NfBN4BTgB8D9VP/dzQc+C/xVTd+bgEPAGcBbgQuBj0TELuBjwN9FRFdEzEn91wJvSfs/I23zz2q299tpXG8C+iar2/LmoLdW+3JE/ENE7AP+F9VQWg7cGBFPRMRB4BqqZ6s9wMXAzoi4PSL+L/DfgOemuO+PAX8REbsi4hDwX4Aldc7qG9p/RNwcES9ExKGIWAccD/xuTZfvRcT9af/fpvoLZ23a9gDQI2mOpO60709ExCsRsRe4Hrh8vP1KEtXw/vcRsS8iXk711fZ/Dfh0RByMiFcnqdky5+t21mq1IfkL4I3APwOeGGuMiIqkF6iekb4ReKZmWUh6hql5E/AlSetq2pT28/QE6zS0f0mfBK5M6wXwm1T/T2DMnprpV4HnI+JwzTxAV1r/WGB3NcOB6onYRPs+DfgNYKimv4BZNX3+MSJ+OdHYrXM46K0d/oFqCAMg6SSq4T8K7AYW1ixT7TzwCtWAG/Pbk+znGeC6iLilgbHV2z81y/4Q+I/ABVT/L+A1SfupuabfgGeAg8DcdPZ/pCPfJ/481V8UZ0XE6ATb9DvIDfClG2uPW4EPSVoi6Xiqlxwei4gR4B7gLEl/nD5g/Xe8Psy3AX8k6XckzaZ62WciXwOukXQW/P8PO99fZ2z19l/rZKrX1P8ROEbSn1E9o29YROwGvgOsk/Sbkt4g6c2S/mXqsgdYIOm41P814OvA9ZJ+K9U3X9J7prJ/y5uD3o66iPgu8J+BO6ieQb+ZdG05Ip4H3k/1g8YXgEXA39as+wBwG/AkMARsnmQ/dwGfBwYkvQTsAC6qM7ZJ93+E+4G/Af431UtBv2TiSy1FfBA4DngK2A/cTvXDaoAHgZ3Ac5KeT22rgWHg0VTfd3n95wNmAMjfMGVmljef0ZuZZc4fxlrHSR+i3jfesojoOsrDMWs5X7oxM8vcjDijnzt3bvT09ADwyiuvcNJJJ7V3QG3SybVDZ9ffybWD659q/UNDQ89HxGn1+s2IoO/p6WHr1q0ADA4OUi6X2zugNunk2qGz6+/k2sH1T7V+SRM9+Pc6/jDWzCxzDnozs8wVDvr0utQfSNqc5k+X9Fh61extY0/spVen3pbaH0svqjIzszZp5Iz+amBXzfzngesj4gyqT/FdmdqvBPan9utTPzMza5NCQS9pAXAJ8D/SvIDzqT6iDbABuDRNL0vzpOUXqOb1emZmdnQVuo9e0u1UvyjiZOCTwErg0XTWjqSFwH0RcbakHcDSiHg2Lfsp8Pb0DpHabfaRvgyhu7v7nIGBAQAqlQpdXZ35zEon1w6dXX8n1w6uf6r19/b2DkVEqV6/urdXSvpXwN6IGJJUbngkE4iIfqAfoFQqxditRZ18m1Un1w6dXX8n1w6uv9X1F7mP/p3Av5F0MXAC1dewfgmYI+mY9O7sBVTfJU76uRB4Nr3mdTbVtwCamVkb1L1GHxHXRMSCiOih+irZByNiOfAQ8L7UbQVwd5relOZJyx8Mv2fBzKxtpvNk7Gqq7/n+HNUvPV6f2tcD35Q0DOxjgu+8bJaeNfe0cvOTGll7Sdv2bWZWVENBHxGDwGCa/hlw7jh9fkn1ixvMzGwG8JOxZmaZc9CbmWXOQW9mljkHvZlZ5hz0ZmaZc9CbmWXOQW9mljkHvZlZ5hz0ZmaZc9CbmWXOQW9mljkHvZlZ5hz0ZmaZc9CbmWXOQW9mljkHvZlZ5hz0ZmaZqxv0kk6Q9LikH0raKekzqf0mST+XtC39WZLaJenLkoYlPSnpba0uwszMJlbkqwQPAudHREXSscAjku5Ly/5DRNx+RP+LgEXpz9uBG9JPMzNrg7pn9FFVSbPHpj8xySrLgG+k9R4F5kiaN/2hmpnZVChissxOnaRZwBBwBvDViFgt6SbgHVTP+LcAayLioKTNwNqIeCStuwVYHRFbj9hmH9AH0N3dfc7AwAAAlUqFrq6uwgVsHz1QuG+zLZ4/u6nba7T23HRy/Z1cO7j+qdbf29s7FBGlev2KXLohIg4DSyTNAe6SdDZwDfAccBzQD6wGPlt0gBHRn9ajVCpFuVwGYHBwkLHpIlauuadw32YbWV5u6vYarT03nVx/J9cOrr/V9Td0101EvAg8BCyNiN3p8sxB4K+Bc1O3UWBhzWoLUpuZmbVBkbtuTktn8kg6EXg38KOx6+6SBFwK7EirbAI+mO6+OQ84EBG7WzJ6MzOrq8ilm3nAhnSd/g3AxojYLOlBSacBArYBH0v97wUuBoaBXwAfav6wzcysqLpBHxFPAm8dp/38CfoHcNX0h2ZmZs3gJ2PNzDLnoDczy5yD3swscw56M7PMOejNzDLnoDczy5yD3swscw56M7PMOejNzDLnoDczy5yD3swscw56M7PMOejNzDLnoDczy5yD3swscw56M7PMOejNzDLnoDczy1yRLwc/QdLjkn4oaaekz6T20yU9JmlY0m2Sjkvtx6f54bS8p7UlmJnZZIqc0R8Ezo+IPwCWAEslnQd8Hrg+Is4A9gNXpv5XAvtT+/Wpn5mZtUndoI+qSpo9Nv0J4Hzg9tS+Abg0TS9L86TlF0hS00ZsZmYNUUTU7yTNAoaAM4CvAv8VeDSdtSNpIXBfRJwtaQewNCKeTct+Crw9Ip4/Ypt9QB9Ad3f3OQMDAwBUKhW6uroKF7B99EDhvs22eP7spm6v0dpz08n1d3Lt4PqnWn9vb+9QRJTq9TumyMYi4jCwRNIc4C7g9xoe0a9usx/oByiVSlEulwEYHBxkbLqIlWvume5Qpmxkebmp22u09tx0cv2dXDu4/lbX39BdNxHxIvAQ8A5gjqSxXxQLgNE0PQosBEjLZwMvNGW0ZmbWsCJ33ZyWzuSRdCLwbmAX1cB/X+q2Arg7TW9K86TlD0aR60NmZtYSRS7dzAM2pOv0bwA2RsRmSU8BA5I+B/wAWJ/6rwe+KWkY2Adc3oJxm5lZQXWDPiKeBN46TvvPgHPHaf8l8P6mjM7MzKbNT8aamWXOQW9mljkHvZlZ5hz0ZmaZc9CbmWXOQW9mljkHvZlZ5hz0ZmaZc9CbmWXOQW9mljkHvZlZ5hz0ZmaZc9CbmWXOQW9mljkHvZlZ5hz0ZmaZc9CbmWWuyHfGLpT0kKSnJO2UdHVqv1bSqKRt6c/FNetcI2lY0o8lvaeVBZiZ2eSKfGfsIWBVRDwh6WRgSNIDadn1EfGF2s6SzqT6PbFnAW8EvivpLRFxuJkDNzOzYuqe0UfE7oh4Ik2/DOwC5k+yyjJgICIORsTPgWHG+W5ZMzM7OhQRxTtLPcDDwNnAnwIrgZeArVTP+vdL+grwaETcnNZZD9wXEbcfsa0+oA+gu7v7nIGBAQAqlQpdXV2Fx7R99EDhvs22eP7spm6v0dpz08n1d3Lt4PqnWn9vb+9QRJTq9Sty6QYASV3AHcAnIuIlSTcAfw5E+rkO+HDR7UVEP9APUCqVolwuAzA4OMjYdBEr19xTuG+zjSwvN3V7jdaem06uv5NrB9ff6voL3XUj6ViqIX9LRNwJEBF7IuJwRLwGfJ1/ujwzCiysWX1BajMzszYocteNgPXAroj4Yk37vJpu7wV2pOlNwOWSjpd0OrAIeLx5QzYzs0YUuXTzTuADwHZJ21Lbp4ArJC2heulmBPgoQETslLQReIrqHTtX+Y4bM7P2qRv0EfEIoHEW3TvJOtcB101jXGZm1iR+MtbMLHOF77qxX9XT5Dt+Vi0+VOguopG1lzR1v2aWN5/Rm5llzkFvZpY5B72ZWeYc9GZmmXPQm5llzkFvZpY5B72ZWeYc9GZmmXPQm5llzkFvZpY5B72ZWeYc9GZmmXPQm5llzkFvZpY5B72ZWeaKfGfsQkkPSXpK0k5JV6f2UyU9IOkn6ecpqV2SvixpWNKTkt7W6iLMzGxiRc7oDwGrIuJM4DzgKklnAmuALRGxCNiS5gEuovqF4IuAPuCGpo/azMwKqxv0EbE7Ip5I0y8Du4D5wDJgQ+q2Abg0TS8DvhFVjwJzJM1r+sjNzKwQRUTxzlIP8DBwNvD3ETEntQvYHxFzJG0G1qYvFUfSFmB1RGw9Ylt9VM/46e7uPmdgYACASqVCV1dX4TFtHz1QuO9M130i7Hm1fr/F82e3fjBt0Oixz0kn1w6uf6r19/b2DkVEqV6/wt8ZK6kLuAP4RES8VM32qogIScV/Y1TX6Qf6AUqlUpTLZQAGBwcZmy6iyHes/rpYtfgQ67bXPyQjy8utH0wbNHrsc9LJtYPrb3X9he66kXQs1ZC/JSLuTM17xi7JpJ97U/sosLBm9QWpzczM2qDIXTcC1gO7IuKLNYs2ASvS9Arg7pr2D6a7b84DDkTE7iaO2czMGlDk0s07gQ8A2yVtS22fAtYCGyVdCTwNXJaW3QtcDAwDvwA+1NQRm5lZQ+oGffpQVRMsvmCc/gFcNc1xmZlZk/jJWDOzzDnozcwy56A3M8ucg97MLHMOejOzzDnozcwy56A3M8ucg97MLHMOejOzzDnozcwy56A3M8ucg97MLHMOejOzzDnozcwy56A3M8ucg97MLHMOejOzzDnozcwyV+TLwW+UtFfSjpq2ayWNStqW/lxcs+waScOSfizpPa0auJmZFVPkjP4mYOk47ddHxJL0514ASWcClwNnpXX+u6RZzRqsmZk1rm7QR8TDwL6C21sGDETEwYj4OTAMnDuN8ZmZ2TQpIup3knqAzRFxdpq/FlgJvARsBVZFxH5JXwEejYibU7/1wH0Rcfs42+wD+gC6u7vPGRgYAKBSqdDV1VW4gO2jBwr3nem6T4Q9r9bvt3j+7NYPpg0aPfY56eTawfVPtf7e3t6hiCjV63fMlEYFNwB/DkT6uQ74cCMbiIh+oB+gVCpFuVwGYHBwkLHpIlauuaeR3c5oqxYfYt32+odkZHm59YNpg0aPfU46uXZw/a2uf0p33UTEnog4HBGvAV/nny7PjAILa7ouSG1mZtYmUwp6SfNqZt8LjN2Rswm4XNLxkk4HFgGPT2+IZmY2HXWvE0i6FSgDcyU9C3waKEtaQvXSzQjwUYCI2ClpI/AUcAi4KiIOt2boZmZWRN2gj4grxmleP0n/64DrpjMoMzNrHj8Za2aWOQe9mVnmHPRmZplz0JuZZc5Bb2aWOQe9mVnmHPRmZplz0JuZZc5Bb2aWOQe9mVnmHPRmZpmb6vvorY162vgO/pG1l7Rt32Y2NT6jNzPLnIPezCxzDnozs8w56M3MMuegNzPLXN2gl3SjpL2SdtS0nSrpAUk/ST9PSe2S9GVJw5KelPS2Vg7ezMzqK3JGfxOw9Ii2NcCWiFgEbEnzABdR/ULwRUAfcENzhmlmZlNVN+gj4mFg3xHNy4ANaXoDcGlN+zei6lFgjqR5zRqsmZk1ThFRv5PUA2yOiLPT/IsRMSdNC9gfEXMkbQbWRsQjadkWYHVEbB1nm31Uz/rp7u4+Z2BgAIBKpUJXV1fhAraPHijcd6brPhH2vNruUUxu8fzZLdt2o8c+J51cO7j+qdbf29s7FBGlev2m/WRsRISk+r8tfnW9fqAfoFQqRblcBmBwcJCx6SJWtvEp0WZbtfgQ67bP7IeVR5aXW7btRo99Tjq5dnD9ra5/qnfd7Bm7JJN+7k3to8DCmn4LUpuZmbXJVIN+E7AiTa8A7q5p/2C6++Y84EBE7J7mGM3MbBrqXieQdCtQBuZKehb4NLAW2CjpSuBp4LLU/V7gYmAY+AXwoRaM2czMGlA36CPiigkWXTBO3wCumu6gzMysefxkrJlZ5hz0ZmaZc9CbmWXOQW9mljkHvZlZ5hz0ZmaZc9CbmWXOQW9mljkHvZlZ5hz0ZmaZc9CbmWXOQW9mljkHvZlZ5mb21xnZjNPTwm/0WrX40ITfGDay9pKW7dcsdz6jNzPLnIPezCxzDnozs8xN6xq9pBHgZeAwcCgiSpJOBW4DeoAR4LKI2D+9YZqZ2VQ148PY3oh4vmZ+DbAlItZKWpPmVzdhP9bBWvkhcD3+INh+3bXi0s0yYEOa3gBc2oJ9mJlZQap+n/cUV5Z+DuwHAviriOiX9GJEzEnLBewfmz9i3T6gD6C7u/ucgYEBACqVCl1dXYXHsH30wJTHP9N0nwh7Xm33KNpnpta/eP7slu+j0b/3uXH9U6u/t7d3KCJK9fpN99LNuyJiVNJvAQ9I+lHtwogISeP+JomIfqAfoFQqRblcBmBwcJCx6SImuu/619GqxYdYt71zH22YqfWPLC+3fB+N/r3Pjetvbf3T+lcVEaPp515JdwHnAnskzYuI3ZLmAXubME6ztjkanw+M97CYPxuwZpnyNXpJJ0k6eWwauBDYAWwCVqRuK4C7pztIMzObuumc0XcDd1Uvw3MM8K2I+BtJ3wc2SroSeBq4bPrDNDOzqZpy0EfEz4A/GKf9BeCC6QzKzMyax0/Gmpllbubd4mBmQPseEvOHwPnxGb2ZWeYc9GZmmXPQm5llzkFvZpY5B72ZWeYc9GZmmXPQm5llzkFvZpY5PzBlZq/Tjge1xt7e6Ye1WsNBb2Yzhp8Gbg0HvZl1vNy/k9jX6M3MMuegNzPLnIPezCxzDnozs8w56M3MMteyoJe0VNKPJQ1LWtOq/ZiZ2eRaEvSSZgFfBS4CzgSukHRmK/ZlZmaTa9UZ/bnAcET8LCL+DzAALGvRvszMbBKKiOZvVHofsDQiPpLmPwC8PSI+XtOnD+hLs78L/DhNzwWeb/qgfj10cu3Q2fV3cu3g+qda/5si4rR6ndr2ZGxE9AP9R7ZL2hoRpTYMqe06uXbo7Po7uXZw/a2uv1WXbkaBhTXzC1KbmZkdZa0K+u8DiySdLuk44HJgU4v2ZWZmk2jJpZuIOCTp48D9wCzgxojYWXD1X7mc00E6uXbo7Po7uXZw/S2tvyUfxpqZ2czhJ2PNzDLnoDczy9yMCXpJI5K2S9omaWu7x9Nqkm6UtFfSjpq2UyU9IOkn6ecp7RxjK01Q/7WSRtPfgW2SLm7nGFtF0kJJD0l6StJOSVen9uyP/yS1d8qxP0HS45J+mOr/TGo/XdJj6ZUxt6WbWJq335lyjV7SCFCKiI54aELSHwEV4BsRcXZq+0tgX0SsTe8HOiUiVrdznK0yQf3XApWI+EI7x9ZqkuYB8yLiCUknA0PApcBKMj/+k9R+GZ1x7AWcFBEVSccCjwBXA38K3BkRA5K+BvwwIm5o1n5nzBl9p4mIh4F9RzQvAzak6Q1U/wFkaYL6O0JE7I6IJ9L0y8AuYD4dcPwnqb0jRFUlzR6b/gRwPnB7am/6sZ9JQR/AdyQNpdcjdKLuiNidpp8Duts5mDb5uKQn06Wd7C5dHElSD/BW4DE67PgfUTt0yLGXNEvSNmAv8ADwU+DFiDiUujxLk3/5zaSgf1dEvI3qGy+vSv9r37Giek1tZlxXO3puAN4MLAF2A+vaO5zWktQF3AF8IiJeql2W+/Efp/aOOfYRcTgillB9Y8C5wO+1ep8zJugjYjT93AvcRfU/QKfZk65hjl3L3Nvm8RxVEbEn/SN4Dfg6Gf8dSNdn7wBuiYg7U3NHHP/xau+kYz8mIl4EHgLeAcyRNPYAa9NfGTMjgl7SSemDGSSdBFwI7Jh8rSxtAlak6RXA3W0cy1E3FnLJe8n070D6QG49sCsivlizKPvjP1HtHXTsT5M0J02fCLyb6ucUDwHvS92afuxnxF03kv451bN4qL6W4VsRcV0bh9Rykm4FylRfT7oH+DTwP4GNwO8ATwOXRUSWH1hOUH+Z6v+6BzACfLTmmnU2JL0L+B6wHXgtNX+K6rXqrI//JLVfQWcc+9+n+mHrLKon2hsj4rMpAweAU4EfAH8SEQebtt+ZEPRmZtY6M+LSjZmZtY6D3swscw56M7PMOejNzDLnoDczy5yD3swscw56M7PM/T8KTYwLbG5jgAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#look at  the distribution of nodules after filtering\n",
    "df_valid.hist(column='nodule_diameter')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#look at  the distribution of nodules after filtering\n",
    "df_test.hist(column='nodule_diameter')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of the nodule diameter is higly unbalanced. The number of small nodules is greater than the number of large nodules. Hence,  if uniform sampling is used, the network will be biased towards small nodules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save train, validate, and test scan indices in a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    scan_id_train=list(set(df_train[\"scan_id\"]))\n",
    "    scan_id_valid=list(set(df_valid[\"scan_id\"]))\n",
    "    scan_id_test=list(set(df_test[\"scan_id\"]))\n",
    "    filename = train_test_split_path\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump([scan_id_train,scan_id_valid,scan_id_test], f)\n",
    "else:\n",
    "    filename=train_test_split_path\n",
    "    with open(filename, 'rb') as f:\n",
    "        scan_id_train,scan_id_valid,scan_id_test=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scan_id_train,scan_id_valid,scan_id_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan_id_set = {ds_train: scan_id_train,\n",
    "               ds_valid: scan_id_valid,\n",
    "               ds_test: scan_id_test}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nodule Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be taking small cubes from the ctscan volume. The size of this small cube is 52x52x52. I can serially decompose the say 512x512x300 ctscan volume into 52x52x52 cubes. But the problem with this approach is that I will have many \"unintresting\" cubes. like cubes that are all black. As an alternative, I will first create a lung mask. pick random points that resides inside the lung mask, and extract the 52x52x52 cube where the random point is the center of that cube. As a final check, I will make make sure that there does not exist a nodule in that cube, because remember we ar now generating negative examples. A summary of what I just described is:\n",
    "\n",
    "1. get a scan\n",
    "2. Apply the lung mask \n",
    "3. Find the range of zs where the lung occupies >2% of the total area. \n",
    "4. Select a random zc location.\n",
    "5. On that z slice, apply the lung mask.\n",
    "6. Select a random xc,yc point that resides inside the lung mask.\n",
    "7. extract a cube where xc,yc,zc is its center and its side is N=52.\n",
    "8. sum the mask of the newly generated cube to ensure that it does not include a nodule. \n",
    "9. The naming convention would be neg_scan_id_cx_xy_cz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Negative Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_type = ds_train\n",
    "interm_dir3 = tmp_path+ds_type+'/neg/'\n",
    "interm_dir2 = tmp_path+ds_type+'/pos/'\n",
    "fname_df = 'df_'+ds_type+'.csv'\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(313)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n"
     ]
    }
   ],
   "source": [
    "for scan_id in scan_id_set[ds_type][xx+1:]:\n",
    "    scan_1 = ctscan(scan_id) \n",
    "    S,B=get_segmented_lungs2(scan_1.image_resampled, plot=False)\n",
    "\n",
    "    T = B.shape[1]**2\n",
    "    Areas=[np.sum(b)/T for b in B]\n",
    "    ind2=[i for i,a in enumerate(Areas) if a>.02]\n",
    "    z1,z2=ind2[0],ind2[-1]\n",
    "\n",
    "    for k in range(epochs):\n",
    "        zz=np.random.randint(z1,z2)\n",
    "\n",
    "        Bf=B[zz].flatten()\n",
    "        #In that slice, find the elements that are true\n",
    "        Cs=[i for i,e in enumerate(Bf) if e]\n",
    "        #randomly select an element from Cs\n",
    "        i = random.choice(Cs)\n",
    "        #from i get the original row and column of that element in B\n",
    "        a=B.shape[1];a\n",
    "        r = i//a\n",
    "        c=i-a*r\n",
    "\n",
    "        #Thus, we have succssfully selected a random point that resides inside the lung area\n",
    "        #we would like to extract a 52x52x52 patch from the ctscan volume.\n",
    "        #The patch is centered at the conditioned random point we have generated\n",
    "        m = 52\n",
    "        cz,cy,cx =[zz,r,c]\n",
    "        #grap the volume\n",
    "        image=scan_1.image_normalized #zxy\n",
    "        cube_img,corner0 = get_cube_from_img(image, cx, cy, cz, m)\n",
    "        cube_label,corner1 = get_cube_from_img(scan_1.Z2, cx, cy, cz, m)\n",
    "        if np.sum(cube_label)==0:\n",
    "            #save file\n",
    "            filename=interm_dir3+'data_N_'+str(scan_id)+\"_\"+str(cx)+\"_\"+str(cy)+\"_\"+str(cz)+\".pkl\"\n",
    "            with open(filename, 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "                pickle.dump([cube_img,cube_label.astype(np.bool)], f)\n",
    "        else:\n",
    "            k=k-1\n",
    "\n",
    "#x=[i for i,j in enumerate(scan_id_train) if j==scan_id]\n",
    "#xx=x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "388"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=[i for i,j in enumerate(scan_id_set[ds_type]) if j==scan_id]\n",
    "xx=x[0]\n",
    "xx#53,59,63,70,346,347,351,361,366,373,388"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1017"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scan_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Negative Examples\n",
    "it is handy to create a csv file that contains a list of the file names and its class and some other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4391, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scan_id</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>label</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>71</td>\n",
       "      <td>280</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>data_N_2_71_280_61.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>91</td>\n",
       "      <td>156</td>\n",
       "      <td>180</td>\n",
       "      <td>0</td>\n",
       "      <td>data_N_2_91_156_180.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>135</td>\n",
       "      <td>256</td>\n",
       "      <td>192</td>\n",
       "      <td>0</td>\n",
       "      <td>data_N_2_135_256_192.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>227</td>\n",
       "      <td>255</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>data_N_2_227_255_84.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>221</td>\n",
       "      <td>255</td>\n",
       "      <td>235</td>\n",
       "      <td>0</td>\n",
       "      <td>data_N_2_221_255_235.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>331</td>\n",
       "      <td>172</td>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "      <td>data_N_2_331_172_82.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>272</td>\n",
       "      <td>170</td>\n",
       "      <td>193</td>\n",
       "      <td>0</td>\n",
       "      <td>data_N_2_272_170_193.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>228</td>\n",
       "      <td>219</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>data_N_2_228_219_236.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "      <td>200</td>\n",
       "      <td>81</td>\n",
       "      <td>0</td>\n",
       "      <td>data_N_2_45_200_81.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>247</td>\n",
       "      <td>242</td>\n",
       "      <td>242</td>\n",
       "      <td>0</td>\n",
       "      <td>data_N_2_247_242_242.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5</td>\n",
       "      <td>103</td>\n",
       "      <td>259</td>\n",
       "      <td>104</td>\n",
       "      <td>0</td>\n",
       "      <td>data_N_5_103_259_104.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5</td>\n",
       "      <td>224</td>\n",
       "      <td>66</td>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "      <td>data_N_5_224_66_77.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5</td>\n",
       "      <td>206</td>\n",
       "      <td>73</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>data_N_5_206_73_65.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5</td>\n",
       "      <td>296</td>\n",
       "      <td>204</td>\n",
       "      <td>226</td>\n",
       "      <td>0</td>\n",
       "      <td>data_N_5_296_204_226.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5</td>\n",
       "      <td>158</td>\n",
       "      <td>94</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>data_N_5_158_94_49.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>212</td>\n",
       "      <td>190</td>\n",
       "      <td>0</td>\n",
       "      <td>data_N_5_50_212_190.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5</td>\n",
       "      <td>173</td>\n",
       "      <td>231</td>\n",
       "      <td>239</td>\n",
       "      <td>0</td>\n",
       "      <td>data_N_5_173_231_239.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5</td>\n",
       "      <td>220</td>\n",
       "      <td>90</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>data_N_5_220_90_70.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5</td>\n",
       "      <td>141</td>\n",
       "      <td>146</td>\n",
       "      <td>191</td>\n",
       "      <td>0</td>\n",
       "      <td>data_N_5_141_146_191.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>6</td>\n",
       "      <td>212</td>\n",
       "      <td>187</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>data_N_6_212_187_74.pkl</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   scan_id    x    y    z  label                  filename\n",
       "0        2   71  280   61      0    data_N_2_71_280_61.pkl\n",
       "1        2   91  156  180      0   data_N_2_91_156_180.pkl\n",
       "2        2  135  256  192      0  data_N_2_135_256_192.pkl\n",
       "3        2  227  255   84      0   data_N_2_227_255_84.pkl\n",
       "4        2  221  255  235      0  data_N_2_221_255_235.pkl\n",
       "5        2  331  172   82      0   data_N_2_331_172_82.pkl\n",
       "6        2  272  170  193      0  data_N_2_272_170_193.pkl\n",
       "7        2  228  219  236      0  data_N_2_228_219_236.pkl\n",
       "8        2   45  200   81      0    data_N_2_45_200_81.pkl\n",
       "9        2  247  242  242      0  data_N_2_247_242_242.pkl\n",
       "10       5  103  259  104      0  data_N_5_103_259_104.pkl\n",
       "11       5  224   66   77      0    data_N_5_224_66_77.pkl\n",
       "12       5  206   73   65      0    data_N_5_206_73_65.pkl\n",
       "13       5  296  204  226      0  data_N_5_296_204_226.pkl\n",
       "14       5  158   94   49      0    data_N_5_158_94_49.pkl\n",
       "15       5   50  212  190      0   data_N_5_50_212_190.pkl\n",
       "16       5  173  231  239      0  data_N_5_173_231_239.pkl\n",
       "17       5  220   90   70      0    data_N_5_220_90_70.pkl\n",
       "18       5  141  146  191      0  data_N_5_141_146_191.pkl\n",
       "19       6  212  187   74      0   data_N_6_212_187_74.pkl"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if True:\n",
    "    temp=!ls {interm_dir3} -irlat #>> myfiles2.csv\n",
    "    #keep string that satisfy a condition\n",
    "    temp1=[t for t in temp if \"data\" in t]\n",
    "    temp1[0:5]\n",
    "\n",
    "    temp2=[t.split(\" \")[-1] for t in temp1]\n",
    "\n",
    "    df=pd.DataFrame([t.split(\".\")[0].split(\"_\")[-4:] for t in temp2],columns=['scan_id','x','y','z'])\n",
    "\n",
    "    df['label']=0\n",
    "    df['filename']=temp2\n",
    "    df.to_csv(tmp_path+ds_type+\"df_neg_scanid_centroid.csv\")\n",
    "else:\n",
    "    df=pd.read_csv(tmp_path+ds_type+\"_df_neg_scanid_centroid.csv\",index_col=0)\n",
    "print(df.shape)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Positive Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Strategy is to:\n",
    "- pick a nodule at random (repition not allowed)\n",
    "- extract the 52x52x52 cube\n",
    "- The naming convention would be pos_scan_id_cx_xy_cz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n"
     ]
    }
   ],
   "source": [
    "m = 52\n",
    "for scan_id in scan_id_set[ds_type][xx+1:]:\n",
    "    #grap the volume\n",
    "    scan_1 = ctscan(scan_id)\n",
    "    image=scan_1.image_normalized #zxy\n",
    "    for c2 in scan_1.centroids2:\n",
    "        cx,cy,cz =c2\n",
    "        cube_img,corner0 = get_cube_from_img(image, cx, cy, cz, m)\n",
    "        cube_label,corner1 = get_cube_from_img(scan_1.Z2, cx, cy, cz, m)\n",
    "        filename=interm_dir2+'data_P_'+str(scan_id)+\"_\"+str(cx)+\"_\"+str(cy)+\"_\"+str(cz)+\".pkl\"\n",
    "        with open(filename, 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "            pickle.dump([cube_img,cube_label.astype(np.bool)], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[i for i,j in enumerate(scan_id_set[ds_type]) if j==scan_id]\n",
    "xx=x[0]\n",
    "xx#53,59,63,70,346,347,351,361,366,373,388"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Positive Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    temp=!ls {interm_dir2} -irlat #>> myfiles2.csv\n",
    "    #keep string that satisfy a condition\n",
    "    temp1=[t for t in temp if \"data\" in t]\n",
    "    temp1[0:5]\n",
    "\n",
    "    temp2=[t.split(\" \")[-1] for t in temp1]\n",
    "\n",
    "    df=pd.DataFrame([t.split(\".\")[0].split(\"_\")[-4:] for t in temp2],columns=['scan_id','x','y','z'])\n",
    "    df['label']=1\n",
    "    df['filename']=temp2\n",
    "    #let us compute some analytics\n",
    "    Area=[]\n",
    "    for file in df.filename:\n",
    "        filename=interm_dir2+'/'+file\n",
    "        with open(filename, 'rb') as f:  # Python 3: open(..., 'wb')\n",
    "            X,Y = pickle.load(f)\n",
    "        Area.append(np.sum(Y)/52./52./52.*100)\n",
    "    df[\"Area_percentage\"] = Area\n",
    "    \n",
    "    df.to_csv(tmp_path+ds_type+\"df_pos_scanid_centroid.csv\")\n",
    "else:\n",
    "    df=pd.read_csv(tmp_path+ds_type+\"df_pos_scanid_centroid.csv\",index_col=0)\n",
    "print(df.shape)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us double check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=interm_dir2+'/'+df.filename[0]\n",
    "print(filename)\n",
    "with open(filename, 'rb') as f:  # Python 3: open(..., 'wb')\n",
    "    X,Y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zs=52\n",
    "\n",
    "X2 = X.copy()\n",
    "Z2=Y.copy()\n",
    "Z2 = np.ma.masked_where(Z2 ==0 , Z2)\n",
    "\n",
    "num_rows=7\n",
    "num_cols=8\n",
    "\n",
    "f, plots = plt.subplots(num_rows, num_cols, sharex='col', sharey='row', figsize=(5,5))\n",
    "\n",
    "ind=np.arange(0,52)\n",
    "for i in range(zs):\n",
    "    ii=ind[i]\n",
    "    plots[i // num_cols, i % num_cols].axis('off')\n",
    "    plots[i // num_cols, i % num_cols].imshow(X2[ii],'gray',vmin=0,vmax=1)\n",
    "\n",
    "    plots[i // num_cols, i % num_cols].imshow(Z2[ii],alpha=0.7,vmin=0,vmax=1)\n",
    "    plots[i // num_cols, i % num_cols].set_title(str(ii))\n",
    "      \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine both into a distribution of n:m (pos:neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp=pd.read_csv(tmp_path+ds_type+\"df_pos_scanid_centroid.csv\",index_col=0)\n",
    "del dfp[\"Area_percentage\"]\n",
    "dfp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfn=pd.read_csv(tmp_path+ds_type+\"df_neg_scanid_centroid.csv\",index_col=0)\n",
    "dfn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp.shape[0],dfn.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=np.int(dfp.shape[0]*.2)\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2= dfn.sample(n=n,random_state=313).reset_index(drop=True)\n",
    "print(dfn.shape)\n",
    "print(df2.shape)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=dfp.append(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df.sample(frac=1,random_state=313).reset_index(drop=True)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(processed_path+fname_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I generated positive examples, I extracted the mini cube around the nodule. Hence, the nodule will always be at the center. Now, we need to change this by introducing some offset in all directions x,y,z.\n",
    "\n",
    "The strategy is to get a random portion of this mini cube with the condition that the center should belong to the new \"mini mini cube\". The center of the original cube must contain a nodule (by design). Hence, this way we gurantee than the new cube will contain a nodule. \n",
    "\n",
    "By visually looking at the different scenarios of extracting a 32x32x32 cube from a 52x52x52 cube, I can say that all will contain the center.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(313)\n",
    "def random_crop(img,label):\n",
    "    #compute the upper left corner of the new cube\n",
    "    x = random.randint(0, 20) #Assume input is 52x52x52\n",
    "    y = random.randint(0, 20) #Assume input is 52x52x52\n",
    "    z = random.randint(0, 20) #Assume input is 52x52x52\n",
    "    img2 = img[x:x+32,y:y+32,z:z+32]\n",
    "    label2 = label[x:x+32,y:y+32,z:z+32]\n",
    "\n",
    "    return img2,label2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    filename=interm_dir2+df.filename[0]\n",
    "    print(filename)\n",
    "    with open(filename, 'rb') as f:  # Python 3: open(..., 'wb')\n",
    "        X,Y = pickle.load(f)\n",
    "\n",
    "    zs=32\n",
    "\n",
    "    X2,Z2 = random_crop(X,Y)\n",
    "    Z2 = np.ma.masked_where(Z2 ==0 , Z2)\n",
    "\n",
    "    num_rows=6\n",
    "    num_cols=6\n",
    "\n",
    "    f, plots = plt.subplots(num_rows, num_cols, sharex='col', sharey='row', figsize=(5,5))\n",
    "\n",
    "    ind=np.arange(0,32)\n",
    "    for i in range(zs):\n",
    "        ii=ind[i]\n",
    "        plots[i // num_cols, i % num_cols].axis('off')\n",
    "        plots[i // num_cols, i % num_cols].imshow(X2[ii],'gray',vmin=0,vmax=1)\n",
    "\n",
    "        plots[i // num_cols, i % num_cols].imshow(Z2[ii],alpha=0.7,vmin=0,vmax=1)\n",
    "        plots[i // num_cols, i % num_cols].set_title(str(ii))\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert into rec file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime \n",
    "\n",
    "now = datetime.datetime.now()\n",
    "today = now.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = today.replace('-','_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     2,
     19
    ]
   },
   "outputs": [],
   "source": [
    "import mxnet as mx #pip install mxnet-cu80\n",
    "#write to .rec file\n",
    "if False:#No longer used\n",
    "    fname==processed_path+ds_type+today+'.rec'\n",
    "\n",
    "    record = mx.recordio.MXRecordIO(fname, 'w')\n",
    "\n",
    "    counter=0\n",
    "    sum_img=np.zeros((32,32,32))\n",
    "\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        if row['label']==1:\n",
    "            path=interm_dir2\n",
    "        else:\n",
    "            path=interm_dir3\n",
    "\n",
    "        filename=path+'/'+row.filename\n",
    "\n",
    "        with open(filename, 'rb') as f:  # Python 3: open(..., 'wb')\n",
    "            x,t=pickle.load(f)\n",
    "            x,t = random_crop(x,t)\n",
    "\n",
    "\n",
    "\n",
    "        sum_img+=x\n",
    "\n",
    "        t=1*t.flatten()#(255*t.flatten()).astype(np.uint8)\n",
    "        x=x.flatten()#(255*x.flatten()).astype(np.uint8)\n",
    "        header = mx.recordio.IRHeader(flag=0, label=x, id=int(counter), id2=0)\n",
    "        s = mx.recordio.pack_img(header, t, quality=100,img_fmt='.png')\n",
    "        record.write(s)\n",
    "        counter +=1\n",
    "    record.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the above method, I can read data sequentially. However, I need to create a .rec file that supports random access. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx #pip install mxnet-cu80\n",
    "#write to .rec file\n",
    "if True:    \n",
    "    fname=processed_path+ds_type+today+'.rec'\n",
    "    idx=processed_path+ds_type+today+'.idx'\n",
    "\n",
    "    record = mx.recordio.MXIndexedRecordIO(idx, fname, 'w')\n",
    "\n",
    "    counter=0\n",
    "    sum_img=np.zeros((32,32,32))\n",
    "\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        if row['label']==1:\n",
    "            path=interm_dir2\n",
    "        else:\n",
    "            path=interm_dir3\n",
    "\n",
    "        filename=path+'/'+row.filename\n",
    "\n",
    "        with open(filename, 'rb') as f:  # Python 3: open(..., 'wb')\n",
    "            x,t=pickle.load(f)\n",
    "            x,t = random_crop(x,t)\n",
    "\n",
    "\n",
    "\n",
    "        sum_img+=x\n",
    "\n",
    "        t=1*t.flatten()#(255*t.flatten()).astype(np.uint8)\n",
    "        x=x.flatten()#(255*x.flatten()).astype(np.uint8)\n",
    "        header = mx.recordio.IRHeader(flag=0, label=x, id=int(counter), id2=0)\n",
    "        s = mx.recordio.pack_img(header, t, quality=100,img_fmt='.png')\n",
    "        record.write_idx(counter,s)\n",
    "        counter +=1\n",
    "    record.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repeat for validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_type = ds_valid\n",
    "interm_dir3 = tmp_path+ds_type+'/neg/'\n",
    "interm_dir2 = tmp_path+ds_type+'/pos/'\n",
    "fname_df = 'df_'+ds_type+'.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dicom files ... This may take a moment.\n",
      "(10, 6)\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "(57, 7)\n",
      "(10, 6)\n",
      "(1, 6)\n",
      "(58, 6)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "random.seed(313)\n",
    "for scan_id in scan_id_set[ds_type]:#[xx+1:]:\n",
    "    scan_1 = ctscan(scan_id) \n",
    "    S,B=get_segmented_lungs2(scan_1.image_resampled, plot=False)\n",
    "\n",
    "    T = B.shape[1]**2\n",
    "    Areas=[np.sum(b)/T for b in B]\n",
    "    ind2=[i for i,a in enumerate(Areas) if a>.02]\n",
    "    z1,z2=ind2[0],ind2[-1]\n",
    "\n",
    "    for k in range(10):\n",
    "        zz=np.random.randint(z1,z2)\n",
    "\n",
    "        Bf=B[zz].flatten()\n",
    "        #In that slice, find the elements that are true\n",
    "        Cs=[i for i,e in enumerate(Bf) if e]\n",
    "        #randomly select an element from Cs\n",
    "        i = random.choice(Cs)\n",
    "        #from i get the original row and column of that element in B\n",
    "        a=B.shape[1];a\n",
    "        r = i//a\n",
    "        c=i-a*r\n",
    "\n",
    "        #Thus, we have succssfully selected a random point that resides inside the lung area\n",
    "        #we would like to extract a 52x52x52 patch from the ctscan volume.\n",
    "        #The patch is centered at the conditioned random point we have generated\n",
    "        m = 52\n",
    "        cz,cy,cx =[zz,r,c]\n",
    "        #grap the volume\n",
    "        image=scan_1.image_normalized #zxy\n",
    "        cube_img,corner0 = get_cube_from_img(image, cx, cy, cz, m)\n",
    "        cube_label,corner1 = get_cube_from_img(scan_1.Z2, cx, cy, cz, m)\n",
    "        if np.sum(cube_label)==0:\n",
    "            #save file\n",
    "            filename=interm_dir3+'data_N_'+str(scan_id)+\"_\"+str(cx)+\"_\"+str(cy)+\"_\"+str(cz)+\".pkl\"\n",
    "            with open(filename, 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "                pickle.dump([cube_img,cube_label.astype(np.bool)], f)\n",
    "        else:\n",
    "            k=k-1\n",
    "\n",
    "#x=[i for i,j in enumerate(scan_id_train) if j==scan_id]\n",
    "#xx=x[0]\n",
    "\n",
    "if True:\n",
    "    temp=!ls {interm_dir3} -irlat #>> myfiles2.csv\n",
    "    #keep string that satisfy a condition\n",
    "    temp1=[t for t in temp if \"data\" in t]\n",
    "    temp1[0:5]\n",
    "\n",
    "    temp2=[t.split(\" \")[-1] for t in temp1]\n",
    "\n",
    "    df=pd.DataFrame([t.split(\".\")[0].split(\"_\")[-4:] for t in temp2],columns=['scan_id','x','y','z'])\n",
    "\n",
    "    df['label']=0\n",
    "    df['filename']=temp2\n",
    "    df.to_csv(tmp_path+ds_type+\"df_neg_scanid_centroid.csv\")\n",
    "else:\n",
    "    df=pd.read_csv(tmp_path+ds_type+\"_df_neg_scanid_centroid.csv\",index_col=0)\n",
    "print(df.shape)\n",
    "df.head(20)\n",
    "\n",
    "m = 52\n",
    "for scan_id in scan_id_set[ds_type]:\n",
    "    #grap the volume\n",
    "    scan_1 = ctscan(scan_id)\n",
    "    image=scan_1.image_normalized #zxy\n",
    "    for c2 in scan_1.centroids2:\n",
    "        cx,cy,cz =c2\n",
    "        cube_img,corner0 = get_cube_from_img(image, cx, cy, cz, m)\n",
    "        cube_label,corner1 = get_cube_from_img(scan_1.Z2, cx, cy, cz, m)\n",
    "        filename=interm_dir2+'data_P_'+str(scan_id)+\"_\"+str(cx)+\"_\"+str(cy)+\"_\"+str(cz)+\".pkl\"\n",
    "        with open(filename, 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "            pickle.dump([cube_img,cube_label.astype(np.bool)], f)\n",
    "\n",
    "if True:\n",
    "    temp=!ls {interm_dir2} -irlat #>> myfiles2.csv\n",
    "    #keep string that satisfy a condition\n",
    "    temp1=[t for t in temp if \"data\" in t]\n",
    "    temp1[0:5]\n",
    "\n",
    "    temp2=[t.split(\" \")[-1] for t in temp1]\n",
    "\n",
    "    df=pd.DataFrame([t.split(\".\")[0].split(\"_\")[-4:] for t in temp2],columns=['scan_id','x','y','z'])\n",
    "    df['label']=1\n",
    "    df['filename']=temp2\n",
    "    #let us compute some analytics\n",
    "    Area=[]\n",
    "    for file in df.filename:\n",
    "        filename=interm_dir2+'/'+file\n",
    "        with open(filename, 'rb') as f:  # Python 3: open(..., 'wb')\n",
    "            X,Y = pickle.load(f)\n",
    "        Area.append(np.sum(Y)/52./52./52.*100)\n",
    "    df[\"Area_percentage\"] = Area\n",
    "    \n",
    "    df.to_csv(tmp_path+ds_type+\"df_pos_scanid_centroid.csv\")\n",
    "else:\n",
    "    df=pd.read_csv(tmp_path+ds_type+\"df_pos_scanid_centroid.csv\",index_col=0)\n",
    "print(df.shape)\n",
    "df.head(20)\n",
    "\n",
    "dfp=pd.read_csv(tmp_path+ds_type+\"df_pos_scanid_centroid.csv\",index_col=0)\n",
    "del dfp[\"Area_percentage\"]\n",
    "dfp.head()\n",
    "\n",
    "dfn=pd.read_csv(tmp_path+ds_type+\"df_neg_scanid_centroid.csv\",index_col=0)\n",
    "dfn.head()\n",
    "\n",
    "df2= dfn.sample(frac=.08,random_state=313).reset_index(drop=True)\n",
    "print(dfn.shape)\n",
    "print(df2.shape)\n",
    "df2.head()\n",
    "\n",
    "df=dfp.append(df2)\n",
    "\n",
    "df= df.sample(frac=1,random_state=313).reset_index(drop=True)\n",
    "print(df.shape)\n",
    "df.head()\n",
    "\n",
    "df.to_csv(processed_path+fname_df)\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "today = now.strftime('%Y-%m-%d')\n",
    "\n",
    "today = today.replace('-','_')\n",
    "\n",
    "#write to .rec file\n",
    "if True:    \n",
    "    fname=processed_path+ds_type+today+'.rec'\n",
    "    idx=processed_path+ds_type+today+'.idx'\n",
    "\n",
    "    record = mx.recordio.MXIndexedRecordIO(idx, fname, 'w')\n",
    "\n",
    "    counter=0\n",
    "    sum_img=np.zeros((32,32,32))\n",
    "\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        if row['label']==1:\n",
    "            path=interm_dir2\n",
    "        else:\n",
    "            path=interm_dir3\n",
    "\n",
    "        filename=path+'/'+row.filename\n",
    "\n",
    "        with open(filename, 'rb') as f:  # Python 3: open(..., 'wb')\n",
    "            x,t=pickle.load(f)\n",
    "            x,t = random_crop(x,t)\n",
    "\n",
    "\n",
    "\n",
    "        sum_img+=x\n",
    "\n",
    "        t=1*t.flatten()#(255*t.flatten()).astype(np.uint8)\n",
    "        x=x.flatten()#(255*x.flatten()).astype(np.uint8)\n",
    "        header = mx.recordio.IRHeader(flag=0, label=x, id=int(counter), id2=0)\n",
    "        s = mx.recordio.pack_img(header, t, quality=100,img_fmt='.png')\n",
    "        record.write_idx(counter,s)\n",
    "        counter +=1\n",
    "    record.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repeat for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_type = ds_test\n",
    "interm_dir3 = tmp_path+ds_type+'/neg/'\n",
    "interm_dir2 = tmp_path+ds_type+'/pos/'\n",
    "fname_df = 'df_'+ds_type+'.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "(28, 6)\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "(57, 7)\n",
      "(28, 6)\n",
      "(2, 6)\n",
      "(59, 6)\n"
     ]
    }
   ],
   "source": [
    "random.seed(313)\n",
    "for scan_id in scan_id_set[ds_type]:#[xx+1:]:\n",
    "    scan_1 = ctscan(scan_id) \n",
    "    S,B=get_segmented_lungs2(scan_1.image_resampled, plot=False)\n",
    "\n",
    "    T = B.shape[1]**2\n",
    "    Areas=[np.sum(b)/T for b in B]\n",
    "    ind2=[i for i,a in enumerate(Areas) if a>.02]\n",
    "    z1,z2=ind2[0],ind2[-1]\n",
    "\n",
    "    for k in range(10):\n",
    "        zz=np.random.randint(z1,z2)\n",
    "\n",
    "        Bf=B[zz].flatten()\n",
    "        #In that slice, find the elements that are true\n",
    "        Cs=[i for i,e in enumerate(Bf) if e]\n",
    "        #randomly select an element from Cs\n",
    "        i = random.choice(Cs)\n",
    "        #from i get the original row and column of that element in B\n",
    "        a=B.shape[1];a\n",
    "        r = i//a\n",
    "        c=i-a*r\n",
    "\n",
    "        #Thus, we have succssfully selected a random point that resides inside the lung area\n",
    "        #we would like to extract a 52x52x52 patch from the ctscan volume.\n",
    "        #The patch is centered at the conditioned random point we have generated\n",
    "        m = 52\n",
    "        cz,cy,cx =[zz,r,c]\n",
    "        #grap the volume\n",
    "        image=scan_1.image_normalized #zxy\n",
    "        cube_img,corner0 = get_cube_from_img(image, cx, cy, cz, m)\n",
    "        cube_label,corner1 = get_cube_from_img(scan_1.Z2, cx, cy, cz, m)\n",
    "        if np.sum(cube_label)==0:\n",
    "            #save file\n",
    "            filename=interm_dir3+'data_N_'+str(scan_id)+\"_\"+str(cx)+\"_\"+str(cy)+\"_\"+str(cz)+\".pkl\"\n",
    "            with open(filename, 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "                pickle.dump([cube_img,cube_label.astype(np.bool)], f)\n",
    "        else:\n",
    "            k=k-1\n",
    "\n",
    "#x=[i for i,j in enumerate(scan_id_train) if j==scan_id]\n",
    "#xx=x[0]\n",
    "\n",
    "if True:\n",
    "    temp=!ls {interm_dir3} -irlat #>> myfiles2.csv\n",
    "    #keep string that satisfy a condition\n",
    "    temp1=[t for t in temp if \"data\" in t]\n",
    "    temp1[0:5]\n",
    "\n",
    "    temp2=[t.split(\" \")[-1] for t in temp1]\n",
    "\n",
    "    df=pd.DataFrame([t.split(\".\")[0].split(\"_\")[-4:] for t in temp2],columns=['scan_id','x','y','z'])\n",
    "\n",
    "    df['label']=0\n",
    "    df['filename']=temp2\n",
    "    df.to_csv(tmp_path+ds_type+\"df_neg_scanid_centroid.csv\")\n",
    "else:\n",
    "    df=pd.read_csv(tmp_path+ds_type+\"_df_neg_scanid_centroid.csv\",index_col=0)\n",
    "print(df.shape)\n",
    "df.head(20)\n",
    "\n",
    "m = 52\n",
    "for scan_id in scan_id_set[ds_type]:\n",
    "    #grap the volume\n",
    "    scan_1 = ctscan(scan_id)\n",
    "    image=scan_1.image_normalized #zxy\n",
    "    for c2 in scan_1.centroids2:\n",
    "        cx,cy,cz =c2\n",
    "        cube_img,corner0 = get_cube_from_img(image, cx, cy, cz, m)\n",
    "        cube_label,corner1 = get_cube_from_img(scan_1.Z2, cx, cy, cz, m)\n",
    "        filename=interm_dir2+'data_P_'+str(scan_id)+\"_\"+str(cx)+\"_\"+str(cy)+\"_\"+str(cz)+\".pkl\"\n",
    "        with open(filename, 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "            pickle.dump([cube_img,cube_label.astype(np.bool)], f)\n",
    "\n",
    "if True:\n",
    "    temp=!ls {interm_dir2} -irlat #>> myfiles2.csv\n",
    "    #keep string that satisfy a condition\n",
    "    temp1=[t for t in temp if \"data\" in t]\n",
    "    temp1[0:5]\n",
    "\n",
    "    temp2=[t.split(\" \")[-1] for t in temp1]\n",
    "\n",
    "    df=pd.DataFrame([t.split(\".\")[0].split(\"_\")[-4:] for t in temp2],columns=['scan_id','x','y','z'])\n",
    "    df['label']=1\n",
    "    df['filename']=temp2\n",
    "    #let us compute some analytics\n",
    "    Area=[]\n",
    "    for file in df.filename:\n",
    "        filename=interm_dir2+'/'+file\n",
    "        with open(filename, 'rb') as f:  # Python 3: open(..., 'wb')\n",
    "            X,Y = pickle.load(f)\n",
    "        Area.append(np.sum(Y)/52./52./52.*100)\n",
    "    df[\"Area_percentage\"] = Area\n",
    "    \n",
    "    df.to_csv(tmp_path+ds_type+\"df_pos_scanid_centroid.csv\")\n",
    "else:\n",
    "    df=pd.read_csv(tmp_path+ds_type+\"df_pos_scanid_centroid.csv\",index_col=0)\n",
    "print(df.shape)\n",
    "df.head(20)\n",
    "\n",
    "dfp=pd.read_csv(tmp_path+ds_type+\"df_pos_scanid_centroid.csv\",index_col=0)\n",
    "del dfp[\"Area_percentage\"]\n",
    "dfp.head()\n",
    "\n",
    "dfn=pd.read_csv(tmp_path+ds_type+\"df_neg_scanid_centroid.csv\",index_col=0)\n",
    "dfn.head()\n",
    "\n",
    "df2= dfn.sample(frac=.08,random_state=313).reset_index(drop=True)\n",
    "print(dfn.shape)\n",
    "print(df2.shape)\n",
    "df2.head()\n",
    "\n",
    "df=dfp.append(df2)\n",
    "\n",
    "df= df.sample(frac=1,random_state=313).reset_index(drop=True)\n",
    "print(df.shape)\n",
    "df.head()\n",
    "\n",
    "df.to_csv(processed_path+fname_df)\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "today = now.strftime('%Y-%m-%d')\n",
    "\n",
    "today = today.replace('-','_')\n",
    "\n",
    "#write to .rec file\n",
    "if True:    \n",
    "    fname=processed_path+ds_type+today+'.rec'\n",
    "    idx=processed_path+ds_type+today+'.idx'\n",
    "\n",
    "    record = mx.recordio.MXIndexedRecordIO(idx, fname, 'w')\n",
    "\n",
    "    counter=0\n",
    "    sum_img=np.zeros((32,32,32))\n",
    "\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        if row['label']==1:\n",
    "            path=interm_dir2\n",
    "        else:\n",
    "            path=interm_dir3\n",
    "\n",
    "        filename=path+'/'+row.filename\n",
    "\n",
    "        with open(filename, 'rb') as f:  # Python 3: open(..., 'wb')\n",
    "            x,t=pickle.load(f)\n",
    "            x,t = random_crop(x,t)\n",
    "\n",
    "\n",
    "\n",
    "        sum_img+=x\n",
    "\n",
    "        t=1*t.flatten()#(255*t.flatten()).astype(np.uint8)\n",
    "        x=x.flatten()#(255*x.flatten()).astype(np.uint8)\n",
    "        header = mx.recordio.IRHeader(flag=0, label=x, id=int(counter), id2=0)\n",
    "        s = mx.recordio.pack_img(header, t, quality=100,img_fmt='.png')\n",
    "        record.write_idx(counter,s)\n",
    "        counter +=1\n",
    "    record.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "429.667px",
    "left": "855.667px",
    "top": "110.567px",
    "width": "277.667px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
