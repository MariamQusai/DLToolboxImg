{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nodule Segmentation: Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Configuration\" data-toc-modified-id=\"Configuration-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Configuration</a></span></li><li><span><a href=\"#Preprocessing\" data-toc-modified-id=\"Preprocessing-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Preprocessing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Find-mean-of-images\" data-toc-modified-id=\"Find-mean-of-images-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Find mean of images</a></span></li><li><span><a href=\"#Find-variance-of-images\" data-toc-modified-id=\"Find-variance-of-images-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Find variance of images</a></span></li></ul></li><li><span><a href=\"#Data-Iterator\" data-toc-modified-id=\"Data-Iterator-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Data Iterator</a></span></li><li><span><a href=\"#Evaluation-Metric\" data-toc-modified-id=\"Evaluation-Metric-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Evaluation Metric</a></span></li><li><span><a href=\"#Model-Architecture\" data-toc-modified-id=\"Model-Architecture-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Model Architecture</a></span></li><li><span><a href=\"#Optimizer\" data-toc-modified-id=\"Optimizer-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Optimizer</a></span><ul class=\"toc-item\"><li><span><a href=\"#Find--learning-rate\" data-toc-modified-id=\"Find--learning-rate-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Find  learning rate</a></span></li><li><span><a href=\"#Optimizer-Parameters\" data-toc-modified-id=\"Optimizer-Parameters-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Optimizer Parameters</a></span></li></ul></li><li><span><a href=\"#Train-Model\" data-toc-modified-id=\"Train-Model-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Train Model</a></span></li><li><span><a href=\"#Evaluate-Model\" data-toc-modified-id=\"Evaluate-Model-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Evaluate Model</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,\"./src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import mynnet7 as nn\n",
    "import pickle\n",
    "import mxnet as mx\n",
    "from imports import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "interm_dir4='/home/mas/x110/Datasets/Dataset5/' \n",
    "s = \"2018_10_31\"\n",
    "train_data_path=interm_dir4+'processed/train'+s+'.rec'\n",
    "train_idx_path=interm_dir4+'processed/train'+s+'.idx'\n",
    "valid_data_path=interm_dir4+'processed/valid'+s+'.rec'\n",
    "valid_idx_path=interm_dir4+'processed/valid'+s+'.idx'\n",
    "test_data_path=interm_dir4+'processed/test'+s+'.rec'\n",
    "test_idx_path=interm_dir4+'processed/test'+s+'.idx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "find_stats = True\n",
    "bs = 34\n",
    "load_model = False\n",
    "model_path = \"/home/mas/x110/model/oct22\"\n",
    "model_epoch=499\n",
    "prefix = \"/home/mas/x110/model/oct31\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find mean of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27012206321241655"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if find_stats:\n",
    "    BATCH_SIZE=1\n",
    "    train_iter=nn.FileIter(train_data_path,train_idx_path,batch_size=BATCH_SIZE,do_augment=True,mean_image=0,std_image=1)\n",
    "    train_iter.reset()\n",
    "    x_mean = np.zeros((32,32,32))\n",
    "    for i,batch in enumerate(train_iter):\n",
    "        X =  batch.data[0][0][0].asnumpy()\n",
    "        x_mean+=X\n",
    "    x_mean=np.mean(x_mean/i)\n",
    "    # Saving the objects:\n",
    "    with open(interm_dir4+'processed/x_mean.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "        pickle.dump([x_mean], f)\n",
    "\n",
    "else:\n",
    "    with open(interm_dir4+'processed/x_mean.pkl','rb') as f:  # Python 3: open(..., 'rb')\n",
    "        x_mean = pickle.load(f)\n",
    "x_mean#x_mean=.2815"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_mean=0.27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find variance of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.268007871005759"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if find_stats: \n",
    "    BATCH_SIZE=1\n",
    "    train_iter=nn.FileIter(train_data_path,train_idx_path,batch_size=BATCH_SIZE,do_augment=True,mean_image=0,std_image = 1)\n",
    "    train_iter.reset()\n",
    "    x_var = np.zeros((32,32,32))\n",
    "    for i,batch in enumerate(train_iter):\n",
    "        X =  (batch.data[0][0][0].asnumpy()-x_mean)**2\n",
    "        x_var+=X\n",
    "    #x_var=x_var/(i-1)\n",
    "    #x_var#x_mean=.2815\n",
    "    N = i*32*32*32\n",
    "    x_var = np.sum(x_var)/(N-1)\n",
    "    x_var#x_var = .07877\n",
    "    x_std = np.sqrt(x_var)#x_std=.2807\n",
    "    with open(interm_dir4+'processed/x_std.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "        pickle.dump([x_std], f)\n",
    "else:\n",
    "    with open(interm_dir4+'processed/x_std.pkl','rb') as f:  # Python 3: open(..., 'rb')\n",
    "        x_std = pickle.load(f)\n",
    "x_std#x_std=.2807"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_std = 0.27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': (34, 1, 32, 32, 32), 'softmax_label': (34, 32768)}\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE=bs\n",
    "train_iter=nn.FileIter(train_data_path,train_idx_path,batch_size=BATCH_SIZE,do_augment=False,mean_image=x_mean,std_image = x_std)\n",
    "input_shapes = dict(train_iter.provide_data+train_iter.provide_label)\n",
    "print(input_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': (34, 1, 32, 32, 32), 'softmax_label': (34, 32768)}\n"
     ]
    }
   ],
   "source": [
    "print(input_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE=bs\n",
    "valid_iter=nn.FileIter(valid_data_path,valid_idx_path,batch_size=BATCH_SIZE,do_augment=False,mean_image=x_mean,std_image = x_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_iter.reset()\n",
    "valid_iter.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def dice_coef2(label, y):\n",
    "    smooth = 1.\n",
    "    label=mx.nd.array(label).as_in_context(mx.gpu(0))\n",
    "    y=mx.nd.array(y).as_in_context(mx.gpu(0))\n",
    "    intersection = mx.nd.sum(label*y)\n",
    "    return ((2. * intersection + smooth) / (mx.nd.sum(label) +mx.nd.sum(mx.nd.abs(y)) + smooth))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ===============Evaluation metric(s)================= \n",
    "metric = mx.metric.CustomMetric(feval=nn.dice_coef2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mas/.virtualenvs/colab/lib/python3.5/site-packages/ipykernel_launcher.py:27: DeprecationWarning: \u001b[91mCalling initializer with init(str, NDArray) has been deprecated.please use init(mx.init.InitDesc(...), NDArray) instead.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if load_model:\n",
    "    network, arg_params, aux_params = mx.model.load_checkpoint(model_path , model_epoch)\n",
    "\n",
    "    # Binding\n",
    "    exe = network.simple_bind(ctx=mx.gpu(0), **input_shapes)\n",
    "\n",
    "\n",
    "    exe.copy_params_from(arg_params, aux_params)\n",
    "    \n",
    "    # get handle to input arrays\n",
    "    arg_arrays = dict(zip(network.list_arguments(), exe.arg_arrays))\n",
    "    data = arg_arrays[train_iter.provide_data[0][0]]\n",
    "    label = arg_arrays[train_iter.provide_label[0][0]]\n",
    "else:\n",
    "\n",
    "    network = nn.get_net_317()\n",
    "    init = mx.init.Normal(0.01) #note biases and gamma/beta are not affected\n",
    "\n",
    "    # Binding\n",
    "    exe = network.simple_bind(ctx=mx.gpu(), **input_shapes)\n",
    "    # get handle to input arrays\n",
    "    arg_arrays = dict(zip(network.list_arguments(), exe.arg_arrays))\n",
    "    data = arg_arrays[train_iter.provide_data[0][0]]\n",
    "    label = arg_arrays[train_iter.provide_label[0][0]]\n",
    "    for name, arr in arg_arrays.items():\n",
    "        if name not in input_shapes:\n",
    "            init(name, arr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find  learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nb=train_iter.num_data//train_iter.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sched=nn.lr_find(1e-4,nb,end_lr=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We also need to create an optimizer for updating weights\n",
    "opt = mx.optimizer.SGD(\n",
    "    learning_rate=.01,\n",
    "    momentum=0.9,\n",
    "    wd=0.00001,\n",
    "    lr_scheduler=sched)\n",
    "\n",
    "updater = mx.optimizer.get_updater(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 139.71043348312378 Epoch: 0 trainloss: -0.18696861 validloss: -0.18071817\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(0,1):\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    metric.reset()\n",
    "\n",
    "    train_iter.reset()\n",
    "    \n",
    "    valid_iter.reset()\n",
    "    \n",
    "    sched.reset()\n",
    "        \n",
    "    sched.on_train_begin()\n",
    "    \n",
    "\n",
    "    for batch in train_iter:\n",
    "        # Copy data to executor input. Note the [:].\n",
    "        data[:] = batch.data[0]\n",
    "        label[:] = batch.label[0]\n",
    "\n",
    "        # Forward\n",
    "        outputs=exe.forward(is_train=True)\n",
    "        # Backward\n",
    "        exe.backward()\n",
    "\n",
    "        # Update\n",
    "        for i, pair in enumerate(zip(exe.arg_arrays, exe.grad_arrays)):\n",
    "            weight, grad = pair\n",
    "            updater(i, grad, weight)   \n",
    "        metric.update(batch.label[0], exe.outputs[0])#metric.update(label,p)\n",
    "        \n",
    "        e=metric.get()\n",
    "        err_train=-e[1].asnumpy()[0]\n",
    "        sched.on_batch_end(err_train)\n",
    "    \n",
    "    if epoch % 100== 0:       \n",
    "        #print(\"do_checkpoint\")\n",
    "        arg={k:v for k, v in arg_arrays.items() if k not in input_shapes}\n",
    "        aux = dict(zip(network.list_auxiliary_states(), exe.aux_arrays))\n",
    "        mx.model.save_checkpoint(prefix, epoch, network, arg, aux)\n",
    "        \n",
    "        \n",
    "    #compute valid loss per epoch    \n",
    "    metric.reset()\n",
    "    for batch in valid_iter:        \n",
    "        data[:] = batch.data[0]       \n",
    "        label[:] = batch.label[0]\n",
    "        # predict\n",
    "        outputs = exe.forward(is_train=False)\n",
    "        metric.update(batch.label[0], exe.outputs[0])\n",
    "    e=metric.get()\n",
    "    err_valid=-e[1].asnumpy()[0]\n",
    "    end = time.time()\n",
    "    print('time:',end-start,'Epoch:',epoch,'trainloss:',err_train,'validloss:',err_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f31302cae80>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(sched.lrs, sched.losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We also need to create an optimizer for updating weights\n",
    "# ===============Optimizer=================                        \n",
    "opt = mx.optimizer.SGD(\n",
    "    learning_rate=.01,momentum=0.99,wd=0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "updater.optimizer.lr_scheduler=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "updater = mx.optimizer.get_updater(opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 99.13337731361389 Epoch: 0 trainloss: -0.21006662 validloss: -0.22243527\n",
      "time: 98.13234972953796 Epoch: 1 trainloss: -0.21784371 validloss: -0.2329826\n",
      "time: 98.28647017478943 Epoch: 2 trainloss: -0.22565451 validloss: -0.24015585\n",
      "time: 98.30458879470825 Epoch: 3 trainloss: -0.23762484 validloss: -0.26260078\n",
      "time: 98.54022288322449 Epoch: 4 trainloss: -0.2469695 validloss: -0.2995131\n",
      "time: 98.27189826965332 Epoch: 5 trainloss: -0.25938457 validloss: -0.28533262\n",
      "time: 98.3297986984253 Epoch: 6 trainloss: -0.2757792 validloss: -0.30065966\n",
      "time: 98.16549921035767 Epoch: 7 trainloss: -0.29839376 validloss: -0.3418324\n",
      "time: 98.27327394485474 Epoch: 8 trainloss: -0.32213333 validloss: -0.26442698\n",
      "time: 98.25467419624329 Epoch: 9 trainloss: -0.35070053 validloss: -0.26968226\n",
      "time: 98.2281584739685 Epoch: 10 trainloss: -0.36434758 validloss: -0.41894838\n",
      "time: 98.25940012931824 Epoch: 11 trainloss: -0.41212115 validloss: -0.41288796\n",
      "time: 100.67501735687256 Epoch: 12 trainloss: -0.44332725 validloss: -0.49534422\n",
      "time: 113.92312622070312 Epoch: 13 trainloss: -0.48908645 validloss: -0.37675467\n",
      "time: 109.4739978313446 Epoch: 14 trainloss: -0.5117554 validloss: -0.3960545\n",
      "time: 114.15375208854675 Epoch: 15 trainloss: -0.54173756 validloss: -0.38049504\n",
      "time: 114.17168951034546 Epoch: 16 trainloss: -0.5313222 validloss: -0.5388639\n",
      "time: 117.51226854324341 Epoch: 17 trainloss: -0.5770089 validloss: -0.5186944\n",
      "time: 99.6531629562378 Epoch: 18 trainloss: -0.5932314 validloss: -0.35549837\n",
      "time: 100.16243410110474 Epoch: 19 trainloss: -0.6234002 validloss: -0.41684029\n",
      "time: 100.33356428146362 Epoch: 20 trainloss: -0.5973676 validloss: -0.3844778\n",
      "time: 98.14392685890198 Epoch: 21 trainloss: -0.63331795 validloss: -0.43969178\n",
      "time: 101.51941323280334 Epoch: 22 trainloss: -0.63623214 validloss: -0.36663014\n",
      "time: 98.2800760269165 Epoch: 23 trainloss: -0.6575736 validloss: -0.6361992\n",
      "time: 98.26494336128235 Epoch: 24 trainloss: -0.67372555 validloss: -0.4891054\n",
      "time: 98.39475393295288 Epoch: 25 trainloss: -0.684345 validloss: -0.6310221\n",
      "time: 98.36770844459534 Epoch: 26 trainloss: -0.6772977 validloss: -0.6506237\n",
      "time: 106.10885071754456 Epoch: 27 trainloss: -0.65661055 validloss: -0.5346078\n",
      "time: 109.1261658668518 Epoch: 28 trainloss: -0.69542813 validloss: -0.5322326\n",
      "time: 107.093665599823 Epoch: 29 trainloss: -0.7133386 validloss: -0.59897035\n",
      "time: 101.70897459983826 Epoch: 30 trainloss: -0.7194737 validloss: -0.49111822\n",
      "time: 110.56725978851318 Epoch: 31 trainloss: -0.7083593 validloss: -0.6102535\n",
      "time: 105.24040126800537 Epoch: 32 trainloss: -0.6596787 validloss: -0.6780649\n",
      "time: 108.04086256027222 Epoch: 33 trainloss: -0.7231898 validloss: -0.6885094\n",
      "time: 102.2624523639679 Epoch: 34 trainloss: -0.7273612 validloss: -0.6891988\n",
      "time: 103.43218564987183 Epoch: 35 trainloss: -0.7233078 validloss: -0.67937744\n",
      "time: 102.78812408447266 Epoch: 36 trainloss: -0.72068065 validloss: -0.7105776\n",
      "time: 102.42433738708496 Epoch: 37 trainloss: -0.73708665 validloss: -0.6994739\n",
      "time: 101.87799072265625 Epoch: 38 trainloss: -0.72795683 validloss: -0.72727233\n",
      "time: 103.54151678085327 Epoch: 39 trainloss: -0.7521505 validloss: -0.72446626\n",
      "time: 100.91119408607483 Epoch: 40 trainloss: -0.75110877 validloss: -0.7515165\n",
      "time: 127.67717456817627 Epoch: 41 trainloss: -0.76274014 validloss: -0.7435487\n",
      "time: 102.51688647270203 Epoch: 42 trainloss: -0.74401546 validloss: -0.7410318\n",
      "time: 98.92440152168274 Epoch: 43 trainloss: -0.7685772 validloss: -0.7191836\n",
      "time: 107.73657464981079 Epoch: 44 trainloss: -0.77163965 validloss: -0.7518264\n",
      "time: 113.58714318275452 Epoch: 45 trainloss: -0.7783296 validloss: -0.73271227\n",
      "time: 115.05996227264404 Epoch: 46 trainloss: -0.7724601 validloss: -0.75822055\n",
      "time: 113.14483118057251 Epoch: 47 trainloss: -0.7403131 validloss: -0.75632477\n",
      "time: 115.11100149154663 Epoch: 48 trainloss: -0.7530223 validloss: -0.75447994\n",
      "time: 111.90988302230835 Epoch: 49 trainloss: -0.79004437 validloss: -0.7376433\n",
      "time: 115.90242671966553 Epoch: 50 trainloss: -0.7735759 validloss: -0.7590705\n",
      "time: 110.32131385803223 Epoch: 51 trainloss: -0.778393 validloss: -0.7523132\n",
      "time: 101.17197275161743 Epoch: 52 trainloss: -0.7876159 validloss: -0.7238682\n",
      "time: 115.17333102226257 Epoch: 53 trainloss: -0.7658246 validloss: -0.76701707\n",
      "time: 105.92204451560974 Epoch: 54 trainloss: -0.8009499 validloss: -0.77014786\n",
      "time: 101.47484970092773 Epoch: 55 trainloss: -0.79786956 validloss: -0.7684444\n",
      "time: 98.87162947654724 Epoch: 56 trainloss: -0.8045894 validloss: -0.7633161\n",
      "time: 98.67868828773499 Epoch: 57 trainloss: -0.7621254 validloss: -0.77630836\n",
      "time: 98.70830821990967 Epoch: 58 trainloss: -0.7946177 validloss: -0.77925354\n",
      "time: 98.74840188026428 Epoch: 59 trainloss: -0.81022733 validloss: -0.7606753\n",
      "time: 103.7239305973053 Epoch: 60 trainloss: -0.79897225 validloss: -0.73913544\n",
      "time: 124.1809470653534 Epoch: 61 trainloss: -0.800372 validloss: -0.7763875\n",
      "time: 100.49501013755798 Epoch: 62 trainloss: -0.80724055 validloss: -0.7796811\n",
      "time: 98.083172082901 Epoch: 63 trainloss: -0.80534244 validloss: -0.780281\n",
      "time: 98.02549719810486 Epoch: 64 trainloss: -0.8225649 validloss: -0.76179904\n",
      "time: 98.05191898345947 Epoch: 65 trainloss: -0.80706275 validloss: -0.7542982\n",
      "time: 98.07244539260864 Epoch: 66 trainloss: -0.8251945 validloss: -0.7788605\n",
      "time: 98.00951528549194 Epoch: 67 trainloss: -0.80546594 validloss: -0.7910003\n",
      "time: 98.45005416870117 Epoch: 68 trainloss: -0.7922862 validloss: -0.7861344\n",
      "time: 118.82299304008484 Epoch: 69 trainloss: -0.82772994 validloss: -0.78096473\n",
      "time: 117.45067381858826 Epoch: 70 trainloss: -0.827975 validloss: -0.7944217\n",
      "time: 113.9080548286438 Epoch: 71 trainloss: -0.8316813 validloss: -0.7894413\n",
      "time: 104.79974102973938 Epoch: 72 trainloss: -0.7980034 validloss: -0.7922432\n",
      "time: 109.77365827560425 Epoch: 73 trainloss: -0.8322884 validloss: -0.7857669\n",
      "time: 106.22043871879578 Epoch: 74 trainloss: -0.83324355 validloss: -0.7804537\n",
      "time: 102.63986134529114 Epoch: 75 trainloss: -0.8269956 validloss: -0.7934138\n",
      "time: 116.54667019844055 Epoch: 76 trainloss: -0.8271533 validloss: -0.7954136\n",
      "time: 101.41023969650269 Epoch: 77 trainloss: -0.8341439 validloss: -0.79409826\n",
      "time: 98.25946235656738 Epoch: 78 trainloss: -0.8283792 validloss: -0.7878543\n",
      "time: 98.23538756370544 Epoch: 79 trainloss: -0.80717295 validloss: -0.73559225\n",
      "time: 98.12808632850647 Epoch: 80 trainloss: -0.8161141 validloss: -0.7927634\n",
      "time: 98.26725316047668 Epoch: 81 trainloss: -0.838271 validloss: -0.8035959\n",
      "time: 98.31104969978333 Epoch: 82 trainloss: -0.81923 validloss: -0.80165756\n",
      "time: 100.08810758590698 Epoch: 83 trainloss: -0.83937335 validloss: -0.7719636\n",
      "time: 110.5805139541626 Epoch: 84 trainloss: -0.831789 validloss: -0.76312476\n",
      "time: 101.5668351650238 Epoch: 85 trainloss: -0.84707963 validloss: -0.77983624\n",
      "time: 101.66729760169983 Epoch: 86 trainloss: -0.8388921 validloss: -0.799336\n",
      "time: 103.84106945991516 Epoch: 87 trainloss: -0.8425012 validloss: -0.80567986\n",
      "time: 99.2466950416565 Epoch: 88 trainloss: -0.8100741 validloss: -0.80754143\n",
      "time: 99.0641074180603 Epoch: 89 trainloss: -0.83868897 validloss: -0.80047935\n",
      "time: 109.29019713401794 Epoch: 90 trainloss: -0.85685605 validloss: -0.7628419\n",
      "time: 128.4217438697815 Epoch: 91 trainloss: -0.8553722 validloss: -0.77884996\n",
      "time: 133.25796341896057 Epoch: 92 trainloss: -0.8584881 validloss: -0.8129347\n",
      "time: 147.23384308815002 Epoch: 93 trainloss: -0.8472173 validloss: -0.8020788\n",
      "time: 121.09214043617249 Epoch: 94 trainloss: -0.8634627 validloss: -0.8044911\n",
      "time: 107.64839339256287 Epoch: 95 trainloss: -0.8528837 validloss: -0.8022787\n",
      "time: 101.74554991722107 Epoch: 96 trainloss: -0.8222502 validloss: -0.78370667\n",
      "time: 110.49452066421509 Epoch: 97 trainloss: -0.8436007 validloss: -0.7809797\n",
      "time: 115.59905624389648 Epoch: 98 trainloss: -0.85363805 validloss: -0.78934014\n",
      "time: 117.38014483451843 Epoch: 99 trainloss: -0.8480855 validloss: -0.8053131\n",
      "time: 104.10500288009644 Epoch: 100 trainloss: -0.86615956 validloss: -0.7946801\n",
      "time: 100.66471934318542 Epoch: 101 trainloss: -0.85848165 validloss: -0.7956829\n",
      "time: 99.92255210876465 Epoch: 102 trainloss: -0.86102563 validloss: -0.81172806\n",
      "time: 100.00757145881653 Epoch: 103 trainloss: -0.8599482 validloss: -0.81778747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 100.03697514533997 Epoch: 104 trainloss: -0.86142844 validloss: -0.8189102\n",
      "time: 111.5131688117981 Epoch: 105 trainloss: -0.87197465 validloss: -0.81019485\n",
      "time: 102.14282488822937 Epoch: 106 trainloss: -0.8714419 validloss: -0.785817\n",
      "time: 107.58115720748901 Epoch: 107 trainloss: -0.85275906 validloss: -0.6779071\n",
      "time: 110.5771872997284 Epoch: 108 trainloss: -0.85043967 validloss: -0.7449916\n",
      "time: 108.27163982391357 Epoch: 109 trainloss: -0.8662728 validloss: -0.7930954\n",
      "time: 125.20734596252441 Epoch: 110 trainloss: -0.8633196 validloss: -0.82657224\n",
      "time: 125.38067531585693 Epoch: 111 trainloss: -0.87527233 validloss: -0.8255826\n",
      "time: 126.43857717514038 Epoch: 112 trainloss: -0.85854983 validloss: -0.8267223\n",
      "time: 101.52550458908081 Epoch: 113 trainloss: -0.8525556 validloss: -0.79445165\n",
      "time: 116.09350967407227 Epoch: 114 trainloss: -0.8528991 validloss: -0.7426984\n",
      "time: 141.10627675056458 Epoch: 115 trainloss: -0.87358564 validloss: -0.7812447\n",
      "time: 134.84520030021667 Epoch: 116 trainloss: -0.8658353 validloss: -0.81866735\n",
      "time: 130.08804512023926 Epoch: 117 trainloss: -0.88036793 validloss: -0.8152234\n",
      "time: 129.18168663978577 Epoch: 118 trainloss: -0.870715 validloss: -0.81595755\n",
      "time: 141.07404899597168 Epoch: 119 trainloss: -0.88211715 validloss: -0.8305816\n",
      "time: 147.31387305259705 Epoch: 120 trainloss: -0.8804734 validloss: -0.8162632\n",
      "time: 138.86818838119507 Epoch: 121 trainloss: -0.87024426 validloss: -0.80982393\n",
      "time: 177.1329963207245 Epoch: 122 trainloss: -0.8432328 validloss: -0.82685226\n",
      "time: 174.62696361541748 Epoch: 123 trainloss: -0.8786864 validloss: -0.83145\n",
      "time: 208.3502492904663 Epoch: 124 trainloss: -0.8890174 validloss: -0.82557136\n",
      "time: 207.63444995880127 Epoch: 125 trainloss: -0.8799391 validloss: -0.82042646\n",
      "time: 206.51440119743347 Epoch: 126 trainloss: -0.88343596 validloss: -0.8260373\n",
      "time: 199.38621520996094 Epoch: 127 trainloss: -0.8827306 validloss: -0.8161915\n",
      "time: 191.613299369812 Epoch: 128 trainloss: -0.8854288 validloss: -0.8164791\n",
      "time: 196.582848072052 Epoch: 129 trainloss: -0.869311 validloss: -0.81947047\n",
      "time: 204.95621013641357 Epoch: 130 trainloss: -0.8932101 validloss: -0.82732767\n",
      "time: 184.6135323047638 Epoch: 131 trainloss: -0.8785214 validloss: -0.8284896\n",
      "time: 156.1862711906433 Epoch: 132 trainloss: -0.8697709 validloss: -0.8294774\n",
      "time: 153.5883424282074 Epoch: 133 trainloss: -0.88838166 validloss: -0.83926815\n",
      "time: 153.67316722869873 Epoch: 134 trainloss: -0.89457774 validloss: -0.8410363\n",
      "time: 153.80082297325134 Epoch: 135 trainloss: -0.8825028 validloss: -0.8336886\n",
      "time: 153.48204970359802 Epoch: 136 trainloss: -0.8975128 validloss: -0.8331253\n",
      "time: 153.20296120643616 Epoch: 137 trainloss: -0.8933584 validloss: -0.826145\n",
      "time: 153.6341052055359 Epoch: 138 trainloss: -0.89149415 validloss: -0.82782096\n",
      "time: 153.4725992679596 Epoch: 139 trainloss: -0.89413255 validloss: -0.83200854\n",
      "time: 153.52050280570984 Epoch: 140 trainloss: -0.8841811 validloss: -0.8452271\n",
      "time: 153.65350246429443 Epoch: 141 trainloss: -0.8936586 validloss: -0.83933365\n",
      "time: 153.3740313053131 Epoch: 142 trainloss: -0.8999089 validloss: -0.8338474\n",
      "time: 153.88770580291748 Epoch: 143 trainloss: -0.896717 validloss: -0.8323771\n",
      "time: 153.7219181060791 Epoch: 144 trainloss: -0.90433204 validloss: -0.82741195\n",
      "time: 153.91398572921753 Epoch: 145 trainloss: -0.90793014 validloss: -0.8299705\n",
      "time: 156.6500735282898 Epoch: 146 trainloss: -0.88098264 validloss: -0.84200156\n",
      "time: 156.85985898971558 Epoch: 147 trainloss: -0.9058742 validloss: -0.849288\n",
      "time: 155.6387906074524 Epoch: 148 trainloss: -0.9012979 validloss: -0.85268676\n",
      "time: 155.53298425674438 Epoch: 149 trainloss: -0.9023817 validloss: -0.8530246\n",
      "time: 159.43234062194824 Epoch: 150 trainloss: -0.90762323 validloss: -0.84953576\n",
      "time: 162.22261500358582 Epoch: 151 trainloss: -0.9104542 validloss: -0.843392\n",
      "time: 106.97477054595947 Epoch: 152 trainloss: -0.9120289 validloss: -0.8473655\n",
      "time: 114.41277194023132 Epoch: 153 trainloss: -0.90095997 validloss: -0.84491575\n",
      "time: 126.89788770675659 Epoch: 154 trainloss: -0.90239704 validloss: -0.79814756\n",
      "time: 150.2926847934723 Epoch: 155 trainloss: -0.8880587 validloss: -0.846541\n",
      "time: 156.67321753501892 Epoch: 156 trainloss: -0.8973136 validloss: -0.84529346\n",
      "time: 199.2178544998169 Epoch: 157 trainloss: -0.91127574 validloss: -0.84048176\n",
      "time: 195.72692584991455 Epoch: 158 trainloss: -0.9013403 validloss: -0.83667606\n",
      "time: 128.79878902435303 Epoch: 159 trainloss: -0.8976932 validloss: -0.85627323\n",
      "time: 128.88127946853638 Epoch: 160 trainloss: -0.8938824 validloss: -0.85482645\n",
      "time: 128.98307156562805 Epoch: 161 trainloss: -0.9022825 validloss: -0.83401054\n",
      "time: 134.07164883613586 Epoch: 162 trainloss: -0.875374 validloss: -0.828922\n",
      "time: 125.04460549354553 Epoch: 163 trainloss: -0.8987746 validloss: -0.84027666\n",
      "time: 125.11736226081848 Epoch: 164 trainloss: -0.8969677 validloss: -0.838297\n",
      "time: 124.96261239051819 Epoch: 165 trainloss: -0.89766866 validloss: -0.8398716\n",
      "time: 125.31445741653442 Epoch: 166 trainloss: -0.90140444 validloss: -0.83105314\n",
      "time: 125.12018370628357 Epoch: 167 trainloss: -0.89845425 validloss: -0.8417203\n",
      "time: 125.22899580001831 Epoch: 168 trainloss: -0.90829 validloss: -0.8488262\n",
      "time: 124.92480087280273 Epoch: 169 trainloss: -0.89368355 validloss: -0.83760446\n",
      "time: 125.43440532684326 Epoch: 170 trainloss: -0.9024625 validloss: -0.8335499\n",
      "time: 125.03322076797485 Epoch: 171 trainloss: -0.90521973 validloss: -0.85560834\n",
      "time: 125.18499255180359 Epoch: 172 trainloss: -0.9124579 validloss: -0.8500512\n",
      "time: 125.54837274551392 Epoch: 173 trainloss: -0.9058115 validloss: -0.8435926\n",
      "time: 141.42458176612854 Epoch: 174 trainloss: -0.9023173 validloss: -0.8458119\n",
      "time: 134.43056297302246 Epoch: 175 trainloss: -0.91522545 validloss: -0.84721196\n",
      "time: 130.8730947971344 Epoch: 176 trainloss: -0.90945214 validloss: -0.84712994\n",
      "time: 129.5955274105072 Epoch: 177 trainloss: -0.9130193 validloss: -0.8286141\n",
      "time: 129.6221399307251 Epoch: 178 trainloss: -0.9155437 validloss: -0.83613366\n",
      "time: 128.4865448474884 Epoch: 179 trainloss: -0.89708495 validloss: -0.85497147\n",
      "time: 129.12161922454834 Epoch: 180 trainloss: -0.90704656 validloss: -0.84270525\n",
      "time: 125.61301374435425 Epoch: 181 trainloss: -0.8950058 validloss: -0.8479699\n",
      "time: 133.6120285987854 Epoch: 182 trainloss: -0.89444745 validloss: -0.83745086\n",
      "time: 125.34920525550842 Epoch: 183 trainloss: -0.90139073 validloss: -0.8224359\n",
      "time: 125.57713484764099 Epoch: 184 trainloss: -0.8977707 validloss: -0.8457266\n",
      "time: 125.53542184829712 Epoch: 185 trainloss: -0.9122508 validloss: -0.8508516\n",
      "time: 124.90905261039734 Epoch: 186 trainloss: -0.9166438 validloss: -0.8380924\n",
      "time: 125.25776886940002 Epoch: 187 trainloss: -0.9013502 validloss: -0.8537547\n",
      "time: 125.04285478591919 Epoch: 188 trainloss: -0.9123701 validloss: -0.8502807\n",
      "time: 125.18860149383545 Epoch: 189 trainloss: -0.91800964 validloss: -0.8508367\n",
      "time: 125.18928742408752 Epoch: 190 trainloss: -0.902158 validloss: -0.853511\n",
      "time: 125.25492668151855 Epoch: 191 trainloss: -0.9196518 validloss: -0.8417656\n",
      "time: 124.9746208190918 Epoch: 192 trainloss: -0.9249863 validloss: -0.85384023\n",
      "time: 125.26999521255493 Epoch: 193 trainloss: -0.9184143 validloss: -0.8617181\n",
      "time: 125.2290244102478 Epoch: 194 trainloss: -0.9244335 validloss: -0.857924\n",
      "time: 124.94671297073364 Epoch: 195 trainloss: -0.92612076 validloss: -0.84862715\n",
      "time: 125.1410174369812 Epoch: 196 trainloss: -0.91924363 validloss: -0.8570666\n",
      "time: 125.41791415214539 Epoch: 197 trainloss: -0.9237536 validloss: -0.8640219\n",
      "time: 124.89681029319763 Epoch: 198 trainloss: -0.89700013 validloss: -0.83583516\n",
      "time: 125.08309960365295 Epoch: 199 trainloss: -0.91972667 validloss: -0.86245865\n",
      "time: 125.47221755981445 Epoch: 200 trainloss: -0.92068434 validloss: -0.85808563\n",
      "time: 125.18145442008972 Epoch: 201 trainloss: -0.9065079 validloss: -0.8337353\n",
      "time: 125.00224423408508 Epoch: 202 trainloss: -0.91957533 validloss: -0.82941145\n",
      "time: 125.30027079582214 Epoch: 203 trainloss: -0.92368054 validloss: -0.8458021\n",
      "time: 125.36578392982483 Epoch: 204 trainloss: -0.9112154 validloss: -0.8520228\n",
      "time: 124.9403064250946 Epoch: 205 trainloss: -0.9216793 validloss: -0.8527576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 125.2642023563385 Epoch: 206 trainloss: -0.9142902 validloss: -0.85737926\n",
      "time: 125.2261745929718 Epoch: 207 trainloss: -0.89594054 validloss: -0.8504585\n",
      "time: 131.04328155517578 Epoch: 208 trainloss: -0.92365485 validloss: -0.8342425\n",
      "time: 134.79119277000427 Epoch: 209 trainloss: -0.9185366 validloss: -0.84746\n",
      "time: 130.9949655532837 Epoch: 210 trainloss: -0.923588 validloss: -0.85706466\n",
      "time: 138.69810390472412 Epoch: 211 trainloss: -0.9128103 validloss: -0.84623253\n",
      "time: 138.85643005371094 Epoch: 212 trainloss: -0.9265204 validloss: -0.859681\n",
      "time: 136.35539650917053 Epoch: 213 trainloss: -0.9224039 validloss: -0.8664108\n",
      "time: 138.59972047805786 Epoch: 214 trainloss: -0.9226113 validloss: -0.8545657\n",
      "time: 131.91803693771362 Epoch: 215 trainloss: -0.9206498 validloss: -0.8476559\n",
      "time: 135.93557858467102 Epoch: 216 trainloss: -0.92353743 validloss: -0.83761364\n",
      "time: 131.63702130317688 Epoch: 217 trainloss: -0.91727406 validloss: -0.852518\n",
      "time: 127.35333561897278 Epoch: 218 trainloss: -0.9257367 validloss: -0.8602584\n",
      "time: 127.42610788345337 Epoch: 219 trainloss: -0.9266952 validloss: -0.8715517\n",
      "time: 127.57719159126282 Epoch: 220 trainloss: -0.9147084 validloss: -0.86544514\n",
      "time: 126.89210534095764 Epoch: 221 trainloss: -0.9348012 validloss: -0.8534438\n",
      "time: 127.64099407196045 Epoch: 222 trainloss: -0.9336371 validloss: -0.8608943\n",
      "time: 127.29822778701782 Epoch: 223 trainloss: -0.932975 validloss: -0.8666698\n",
      "time: 134.2649645805359 Epoch: 224 trainloss: -0.9298638 validloss: -0.8676943\n",
      "time: 129.67594623565674 Epoch: 225 trainloss: -0.9222784 validloss: -0.84970653\n",
      "time: 126.31066727638245 Epoch: 226 trainloss: -0.93713915 validloss: -0.8651701\n",
      "time: 126.5382571220398 Epoch: 227 trainloss: -0.91921395 validloss: -0.8544927\n",
      "time: 126.7890305519104 Epoch: 228 trainloss: -0.9247878 validloss: -0.8634153\n",
      "time: 127.15815448760986 Epoch: 229 trainloss: -0.9351332 validloss: -0.8695597\n",
      "time: 127.03415727615356 Epoch: 230 trainloss: -0.93589205 validloss: -0.85941935\n",
      "time: 126.62650561332703 Epoch: 231 trainloss: -0.9116447 validloss: -0.85180455\n",
      "time: 127.37243390083313 Epoch: 232 trainloss: -0.93188727 validloss: -0.8410763\n",
      "time: 127.11995887756348 Epoch: 233 trainloss: -0.93287617 validloss: -0.8622141\n",
      "time: 126.57650518417358 Epoch: 234 trainloss: -0.9099723 validloss: -0.8585495\n",
      "time: 127.18317317962646 Epoch: 235 trainloss: -0.92683905 validloss: -0.81883615\n",
      "time: 127.1666693687439 Epoch: 236 trainloss: -0.92560256 validloss: -0.8603077\n",
      "time: 126.62272691726685 Epoch: 237 trainloss: -0.93761843 validloss: -0.85916585\n",
      "time: 127.38800692558289 Epoch: 238 trainloss: -0.9225826 validloss: -0.86897033\n",
      "time: 127.06871795654297 Epoch: 239 trainloss: -0.93975365 validloss: -0.84749097\n",
      "time: 126.86552357673645 Epoch: 240 trainloss: -0.9286922 validloss: -0.85294384\n",
      "time: 126.80406999588013 Epoch: 241 trainloss: -0.9238184 validloss: -0.8714333\n",
      "time: 127.38426423072815 Epoch: 242 trainloss: -0.93699443 validloss: -0.8677172\n",
      "time: 127.48805785179138 Epoch: 243 trainloss: -0.9346278 validloss: -0.8726265\n",
      "time: 126.74712872505188 Epoch: 244 trainloss: -0.9361833 validloss: -0.8618966\n",
      "time: 127.61028861999512 Epoch: 245 trainloss: -0.9386514 validloss: -0.87321025\n",
      "time: 127.74136018753052 Epoch: 246 trainloss: -0.9341022 validloss: -0.8680366\n",
      "time: 127.02346062660217 Epoch: 247 trainloss: -0.93961054 validloss: -0.8798514\n",
      "time: 128.1227068901062 Epoch: 248 trainloss: -0.9370728 validloss: -0.8698228\n",
      "time: 128.1827895641327 Epoch: 249 trainloss: -0.94508564 validloss: -0.8747066\n",
      "time: 127.77829480171204 Epoch: 250 trainloss: -0.9463771 validloss: -0.86954886\n",
      "time: 127.50873947143555 Epoch: 251 trainloss: -0.93538934 validloss: -0.86764055\n",
      "time: 127.49704480171204 Epoch: 252 trainloss: -0.94019514 validloss: -0.853939\n",
      "time: 135.11776518821716 Epoch: 253 trainloss: -0.93811506 validloss: -0.86733097\n",
      "time: 132.24864554405212 Epoch: 254 trainloss: -0.9443193 validloss: -0.8772855\n",
      "time: 132.96037316322327 Epoch: 255 trainloss: -0.9332729 validloss: -0.8694994\n",
      "time: 132.57136034965515 Epoch: 256 trainloss: -0.9381066 validloss: -0.8663787\n",
      "time: 132.04940676689148 Epoch: 257 trainloss: -0.9481704 validloss: -0.860951\n",
      "time: 132.38190126419067 Epoch: 258 trainloss: -0.9473924 validloss: -0.86828035\n",
      "time: 132.76309442520142 Epoch: 259 trainloss: -0.9427856 validloss: -0.8689334\n",
      "time: 132.09554195404053 Epoch: 260 trainloss: -0.9478789 validloss: -0.8656381\n",
      "time: 133.38426661491394 Epoch: 261 trainloss: -0.9365588 validloss: -0.87018585\n",
      "time: 133.03766345977783 Epoch: 262 trainloss: -0.943977 validloss: -0.873424\n",
      "time: 132.00381207466125 Epoch: 263 trainloss: -0.94511265 validloss: -0.8826994\n",
      "time: 132.79305362701416 Epoch: 264 trainloss: -0.9445408 validloss: -0.87821114\n",
      "time: 132.9201204776764 Epoch: 265 trainloss: -0.9486026 validloss: -0.876758\n",
      "time: 132.8370189666748 Epoch: 266 trainloss: -0.94999176 validloss: -0.8750333\n",
      "time: 132.52769660949707 Epoch: 267 trainloss: -0.9490246 validloss: -0.87096107\n",
      "time: 132.65048575401306 Epoch: 268 trainloss: -0.951004 validloss: -0.86501104\n",
      "time: 131.74800300598145 Epoch: 269 trainloss: -0.9315231 validloss: -0.8891913\n",
      "time: 132.3531517982483 Epoch: 270 trainloss: -0.9376004 validloss: -0.87040806\n",
      "time: 132.12599062919617 Epoch: 271 trainloss: -0.94006366 validloss: -0.8340149\n",
      "time: 132.09902715682983 Epoch: 272 trainloss: -0.94396806 validloss: -0.8667716\n",
      "time: 132.14056992530823 Epoch: 273 trainloss: -0.9448093 validloss: -0.87490475\n",
      "time: 132.498393535614 Epoch: 274 trainloss: -0.93902993 validloss: -0.86156094\n",
      "time: 131.96680283546448 Epoch: 275 trainloss: -0.9459544 validloss: -0.8771411\n",
      "time: 131.81755208969116 Epoch: 276 trainloss: -0.93778497 validloss: -0.8786138\n",
      "time: 132.08195638656616 Epoch: 277 trainloss: -0.9486842 validloss: -0.8725919\n",
      "time: 131.65018439292908 Epoch: 278 trainloss: -0.94169694 validloss: -0.87744254\n",
      "time: 131.53210473060608 Epoch: 279 trainloss: -0.94815105 validloss: -0.8854458\n",
      "time: 131.8751540184021 Epoch: 280 trainloss: -0.94761103 validloss: -0.86992323\n",
      "time: 132.28983283042908 Epoch: 281 trainloss: -0.9503154 validloss: -0.8561329\n",
      "time: 131.88125014305115 Epoch: 282 trainloss: -0.95006436 validloss: -0.8662186\n",
      "time: 132.21420073509216 Epoch: 283 trainloss: -0.94579935 validloss: -0.87560266\n",
      "time: 132.99535608291626 Epoch: 284 trainloss: -0.94760156 validloss: -0.88098204\n",
      "time: 131.76874351501465 Epoch: 285 trainloss: -0.95149076 validloss: -0.8656576\n",
      "time: 132.58511996269226 Epoch: 286 trainloss: -0.9489613 validloss: -0.8748625\n",
      "time: 132.54691576957703 Epoch: 287 trainloss: -0.9581808 validloss: -0.8771595\n",
      "time: 131.81426429748535 Epoch: 288 trainloss: -0.95088863 validloss: -0.88757694\n",
      "time: 131.82081532478333 Epoch: 289 trainloss: -0.95484555 validloss: -0.8577541\n",
      "time: 131.62005019187927 Epoch: 290 trainloss: -0.9473841 validloss: -0.8804919\n",
      "time: 132.11636328697205 Epoch: 291 trainloss: -0.95698726 validloss: -0.888774\n",
      "time: 132.02529335021973 Epoch: 292 trainloss: -0.9544668 validloss: -0.8806463\n",
      "time: 132.44057035446167 Epoch: 293 trainloss: -0.92683387 validloss: -0.84732026\n",
      "time: 133.0528004169464 Epoch: 294 trainloss: -0.94923615 validloss: -0.85683936\n",
      "time: 131.8718798160553 Epoch: 295 trainloss: -0.94818974 validloss: -0.87185234\n",
      "time: 132.32817554473877 Epoch: 296 trainloss: -0.9479844 validloss: -0.86278856\n",
      "time: 132.74574542045593 Epoch: 297 trainloss: -0.95080525 validloss: -0.8701448\n",
      "time: 131.88908314704895 Epoch: 298 trainloss: -0.94525826 validloss: -0.8834547\n",
      "time: 132.61686754226685 Epoch: 299 trainloss: -0.9385218 validloss: -0.88291305\n",
      "time: 132.42487907409668 Epoch: 300 trainloss: -0.9506853 validloss: -0.86883885\n",
      "time: 131.8784577846527 Epoch: 301 trainloss: -0.9474588 validloss: -0.85807455\n",
      "time: 132.37057828903198 Epoch: 302 trainloss: -0.95029277 validloss: -0.8777147\n",
      "time: 131.99479246139526 Epoch: 303 trainloss: -0.9513181 validloss: -0.8839687\n",
      "time: 131.98759865760803 Epoch: 304 trainloss: -0.94726497 validloss: -0.8629312\n",
      "time: 132.30369806289673 Epoch: 305 trainloss: -0.9487181 validloss: -0.87168485\n",
      "time: 132.6794376373291 Epoch: 306 trainloss: -0.94686675 validloss: -0.87200785\n",
      "time: 132.0556881427765 Epoch: 307 trainloss: -0.94171345 validloss: -0.87546307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 132.89599347114563 Epoch: 308 trainloss: -0.95323795 validloss: -0.86507624\n",
      "time: 132.4319896697998 Epoch: 309 trainloss: -0.9451836 validloss: -0.87597287\n",
      "time: 132.5144500732422 Epoch: 310 trainloss: -0.95260394 validloss: -0.8751674\n",
      "time: 102.46601223945618 Epoch: 311 trainloss: -0.938946 validloss: -0.85783774\n",
      "time: 97.8660159111023 Epoch: 312 trainloss: -0.94653267 validloss: -0.8659707\n",
      "time: 98.08803176879883 Epoch: 313 trainloss: -0.9556851 validloss: -0.8752208\n",
      "time: 98.05097699165344 Epoch: 314 trainloss: -0.94721603 validloss: -0.88424623\n",
      "time: 98.08797335624695 Epoch: 315 trainloss: -0.9483086 validloss: -0.86599785\n",
      "time: 97.92254638671875 Epoch: 316 trainloss: -0.95057356 validloss: -0.8722948\n",
      "time: 98.12565660476685 Epoch: 317 trainloss: -0.9556811 validloss: -0.87754196\n",
      "time: 98.12435722351074 Epoch: 318 trainloss: -0.9549952 validloss: -0.8736828\n",
      "time: 97.89565920829773 Epoch: 319 trainloss: -0.95301247 validloss: -0.88487107\n",
      "time: 97.97349858283997 Epoch: 320 trainloss: -0.9526013 validloss: -0.8605771\n",
      "time: 98.1026623249054 Epoch: 321 trainloss: -0.94336724 validloss: -0.88020176\n",
      "time: 98.06236100196838 Epoch: 322 trainloss: -0.9607738 validloss: -0.88348866\n",
      "time: 98.06734991073608 Epoch: 323 trainloss: -0.95390356 validloss: -0.87000746\n",
      "time: 98.11510300636292 Epoch: 324 trainloss: -0.9524817 validloss: -0.8690725\n",
      "time: 98.08351945877075 Epoch: 325 trainloss: -0.95759183 validloss: -0.8686992\n",
      "time: 98.09439301490784 Epoch: 326 trainloss: -0.95448035 validloss: -0.87689036\n",
      "time: 98.08999943733215 Epoch: 327 trainloss: -0.96038467 validloss: -0.8776075\n",
      "time: 98.05904388427734 Epoch: 328 trainloss: -0.9560999 validloss: -0.8770995\n",
      "time: 98.10718035697937 Epoch: 329 trainloss: -0.9559877 validloss: -0.8725877\n",
      "time: 98.09084963798523 Epoch: 330 trainloss: -0.9542288 validloss: -0.8696179\n",
      "time: 98.05985808372498 Epoch: 331 trainloss: -0.9499801 validloss: -0.88780177\n",
      "time: 98.27036738395691 Epoch: 332 trainloss: -0.9544542 validloss: -0.87797105\n",
      "time: 98.19179081916809 Epoch: 333 trainloss: -0.95572263 validloss: -0.8907661\n",
      "time: 97.95800924301147 Epoch: 334 trainloss: -0.9502827 validloss: -0.88447297\n",
      "time: 97.76266670227051 Epoch: 335 trainloss: -0.95671535 validloss: -0.86371815\n",
      "time: 98.09736394882202 Epoch: 336 trainloss: -0.95947075 validloss: -0.8613262\n",
      "time: 98.04773736000061 Epoch: 337 trainloss: -0.9577463 validloss: -0.86492497\n",
      "time: 98.00694131851196 Epoch: 338 trainloss: -0.95881915 validloss: -0.8699861\n",
      "time: 98.10210704803467 Epoch: 339 trainloss: -0.9559576 validloss: -0.8786732\n",
      "time: 98.1537766456604 Epoch: 340 trainloss: -0.9604143 validloss: -0.87675774\n",
      "time: 98.086350440979 Epoch: 341 trainloss: -0.96354836 validloss: -0.8863109\n",
      "time: 98.07398557662964 Epoch: 342 trainloss: -0.96469796 validloss: -0.88550484\n",
      "time: 98.06347942352295 Epoch: 343 trainloss: -0.9586868 validloss: -0.8800851\n",
      "time: 98.06574296951294 Epoch: 344 trainloss: -0.9651489 validloss: -0.8682093\n",
      "time: 97.88732981681824 Epoch: 345 trainloss: -0.9645226 validloss: -0.87741053\n",
      "time: 98.10268712043762 Epoch: 346 trainloss: -0.9643608 validloss: -0.8922394\n",
      "time: 98.65199208259583 Epoch: 347 trainloss: -0.9659752 validloss: -0.8881915\n",
      "time: 98.12212610244751 Epoch: 348 trainloss: -0.96386373 validloss: -0.8827724\n",
      "time: 98.0423367023468 Epoch: 349 trainloss: -0.9624397 validloss: -0.8802796\n",
      "time: 98.07839155197144 Epoch: 350 trainloss: -0.9669086 validloss: -0.87761974\n",
      "time: 98.10157990455627 Epoch: 351 trainloss: -0.96352035 validloss: -0.8760253\n",
      "time: 97.8886890411377 Epoch: 352 trainloss: -0.96552837 validloss: -0.8824542\n",
      "time: 98.07697582244873 Epoch: 353 trainloss: -0.9658853 validloss: -0.88414633\n",
      "time: 98.11398816108704 Epoch: 354 trainloss: -0.96715593 validloss: -0.8887306\n",
      "time: 98.11656022071838 Epoch: 355 trainloss: -0.97042507 validloss: -0.8809412\n",
      "time: 98.13848757743835 Epoch: 356 trainloss: -0.97004265 validloss: -0.88883084\n",
      "time: 98.14609980583191 Epoch: 357 trainloss: -0.9683747 validloss: -0.8758691\n",
      "time: 98.079256772995 Epoch: 358 trainloss: -0.97091675 validloss: -0.88451165\n",
      "time: 98.03162670135498 Epoch: 359 trainloss: -0.96785885 validloss: -0.89391613\n",
      "time: 98.01685452461243 Epoch: 360 trainloss: -0.96560955 validloss: -0.8900046\n",
      "time: 98.08027839660645 Epoch: 361 trainloss: -0.96411014 validloss: -0.88661355\n",
      "time: 98.110915184021 Epoch: 362 trainloss: -0.96510446 validloss: -0.89252406\n",
      "time: 98.10422325134277 Epoch: 363 trainloss: -0.9714911 validloss: -0.8823436\n",
      "time: 98.1075975894928 Epoch: 364 trainloss: -0.9686723 validloss: -0.88306665\n",
      "time: 98.07165384292603 Epoch: 365 trainloss: -0.97122145 validloss: -0.8801276\n",
      "time: 97.84144806861877 Epoch: 366 trainloss: -0.9680318 validloss: -0.8952444\n",
      "time: 97.87158155441284 Epoch: 367 trainloss: -0.9685127 validloss: -0.88938385\n",
      "time: 97.91637396812439 Epoch: 368 trainloss: -0.9715968 validloss: -0.8821823\n",
      "time: 104.31094431877136 Epoch: 369 trainloss: -0.9732717 validloss: -0.8860306\n",
      "time: 111.60430145263672 Epoch: 370 trainloss: -0.9726284 validloss: -0.8912358\n",
      "time: 100.69645047187805 Epoch: 371 trainloss: -0.97256905 validloss: -0.887096\n",
      "time: 99.76446151733398 Epoch: 372 trainloss: -0.9731276 validloss: -0.8841394\n",
      "time: 98.38386392593384 Epoch: 373 trainloss: -0.96925616 validloss: -0.8892964\n",
      "time: 98.33650231361389 Epoch: 374 trainloss: -0.9687629 validloss: -0.89440566\n",
      "time: 98.29425096511841 Epoch: 375 trainloss: -0.97353023 validloss: -0.8912949\n",
      "time: 98.3971221446991 Epoch: 376 trainloss: -0.97229683 validloss: -0.8820877\n",
      "time: 98.35838341712952 Epoch: 377 trainloss: -0.9743465 validloss: -0.8795325\n",
      "time: 98.20791435241699 Epoch: 378 trainloss: -0.9714471 validloss: -0.8955144\n",
      "time: 98.37887024879456 Epoch: 379 trainloss: -0.9762558 validloss: -0.89284104\n",
      "time: 98.46726417541504 Epoch: 380 trainloss: -0.97330916 validloss: -0.8882947\n",
      "time: 98.39017462730408 Epoch: 381 trainloss: -0.976662 validloss: -0.89228743\n",
      "time: 98.39832878112793 Epoch: 382 trainloss: -0.9700319 validloss: -0.88922495\n",
      "time: 98.01682376861572 Epoch: 383 trainloss: -0.9743434 validloss: -0.885612\n",
      "time: 98.30922818183899 Epoch: 384 trainloss: -0.974162 validloss: -0.8877698\n",
      "time: 98.39663553237915 Epoch: 385 trainloss: -0.9735575 validloss: -0.8982457\n",
      "time: 98.39447259902954 Epoch: 386 trainloss: -0.9767862 validloss: -0.89109534\n",
      "time: 98.18337512016296 Epoch: 387 trainloss: -0.9728517 validloss: -0.89014655\n",
      "time: 98.35593914985657 Epoch: 388 trainloss: -0.9733651 validloss: -0.8976224\n",
      "time: 98.20227146148682 Epoch: 389 trainloss: -0.97745764 validloss: -0.8837618\n",
      "time: 98.37850141525269 Epoch: 390 trainloss: -0.9781085 validloss: -0.88331497\n",
      "time: 98.4138457775116 Epoch: 391 trainloss: -0.9766106 validloss: -0.8935932\n",
      "time: 98.3386299610138 Epoch: 392 trainloss: -0.9727901 validloss: -0.88808775\n",
      "time: 98.27944469451904 Epoch: 393 trainloss: -0.97535485 validloss: -0.8886725\n",
      "time: 98.18344259262085 Epoch: 394 trainloss: -0.9758445 validloss: -0.8876663\n",
      "time: 98.37172555923462 Epoch: 395 trainloss: -0.9769905 validloss: -0.8843455\n",
      "time: 98.37339329719543 Epoch: 396 trainloss: -0.9789643 validloss: -0.8902677\n",
      "time: 98.14753341674805 Epoch: 397 trainloss: -0.9769627 validloss: -0.878765\n",
      "time: 98.37457990646362 Epoch: 398 trainloss: -0.9778969 validloss: -0.897927\n",
      "time: 98.12945175170898 Epoch: 399 trainloss: -0.9790161 validloss: -0.8949427\n",
      "time: 98.01718020439148 Epoch: 400 trainloss: -0.98056155 validloss: -0.89328194\n",
      "time: 98.35678029060364 Epoch: 401 trainloss: -0.97837895 validloss: -0.8859096\n",
      "time: 98.18017959594727 Epoch: 402 trainloss: -0.9795137 validloss: -0.89278233\n",
      "time: 98.3275134563446 Epoch: 403 trainloss: -0.9788855 validloss: -0.8986064\n",
      "time: 98.31540656089783 Epoch: 404 trainloss: -0.9795457 validloss: -0.8876725\n",
      "time: 98.18581128120422 Epoch: 405 trainloss: -0.9777167 validloss: -0.89633846\n",
      "time: 98.37731838226318 Epoch: 406 trainloss: -0.9795884 validloss: -0.90211856\n",
      "time: 106.06208109855652 Epoch: 407 trainloss: -0.97416437 validloss: -0.8924344\n",
      "time: 98.33775615692139 Epoch: 408 trainloss: -0.97404563 validloss: -0.87991863\n",
      "time: 98.42206168174744 Epoch: 409 trainloss: -0.978155 validloss: -0.90549093\n",
      "time: 98.34811544418335 Epoch: 410 trainloss: -0.9729406 validloss: -0.90187836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 98.21736311912537 Epoch: 411 trainloss: -0.9761472 validloss: -0.8700983\n",
      "time: 98.34607982635498 Epoch: 412 trainloss: -0.97623277 validloss: -0.89815503\n",
      "time: 98.35931515693665 Epoch: 413 trainloss: -0.97029537 validloss: -0.89780986\n",
      "time: 98.37481713294983 Epoch: 414 trainloss: -0.9772424 validloss: -0.8905025\n",
      "time: 98.34135246276855 Epoch: 415 trainloss: -0.97546995 validloss: -0.8798534\n",
      "time: 98.3924469947815 Epoch: 416 trainloss: -0.9785279 validloss: -0.8798948\n",
      "time: 98.18431115150452 Epoch: 417 trainloss: -0.9764185 validloss: -0.8917431\n",
      "time: 98.38339948654175 Epoch: 418 trainloss: -0.97782487 validloss: -0.8862625\n",
      "time: 98.40623068809509 Epoch: 419 trainloss: -0.9785322 validloss: -0.8914248\n",
      "time: 98.351637840271 Epoch: 420 trainloss: -0.97537595 validloss: -0.9002274\n",
      "time: 98.3689775466919 Epoch: 421 trainloss: -0.97838604 validloss: -0.88567096\n",
      "time: 98.24748182296753 Epoch: 422 trainloss: -0.9779405 validloss: -0.8920606\n",
      "time: 98.39670753479004 Epoch: 423 trainloss: -0.9796375 validloss: -0.8933324\n",
      "time: 98.38512992858887 Epoch: 424 trainloss: -0.97949547 validloss: -0.8967007\n",
      "time: 98.35741782188416 Epoch: 425 trainloss: -0.98000485 validloss: -0.89568245\n",
      "time: 98.37676405906677 Epoch: 426 trainloss: -0.97646457 validloss: -0.87792176\n",
      "time: 98.3515145778656 Epoch: 427 trainloss: -0.9790291 validloss: -0.89059865\n",
      "time: 98.19571089744568 Epoch: 428 trainloss: -0.9767452 validloss: -0.89903325\n",
      "time: 98.38596272468567 Epoch: 429 trainloss: -0.9766253 validloss: -0.88034856\n",
      "time: 98.40284037590027 Epoch: 430 trainloss: -0.9788248 validloss: -0.8852368\n",
      "time: 98.3612904548645 Epoch: 431 trainloss: -0.9793034 validloss: -0.90024805\n",
      "time: 98.35515117645264 Epoch: 432 trainloss: -0.9779021 validloss: -0.88908565\n",
      "time: 98.18802881240845 Epoch: 433 trainloss: -0.9816789 validloss: -0.8793096\n",
      "time: 98.33021688461304 Epoch: 434 trainloss: -0.9800112 validloss: -0.8959288\n",
      "time: 98.38911628723145 Epoch: 435 trainloss: -0.9809778 validloss: -0.899571\n",
      "time: 98.26361060142517 Epoch: 436 trainloss: -0.98152447 validloss: -0.89698225\n",
      "time: 98.35678815841675 Epoch: 437 trainloss: -0.9786511 validloss: -0.88380945\n",
      "time: 98.3902804851532 Epoch: 438 trainloss: -0.97959244 validloss: -0.9017779\n",
      "time: 98.19414234161377 Epoch: 439 trainloss: -0.97238153 validloss: -0.90050787\n",
      "time: 98.33526968955994 Epoch: 440 trainloss: -0.9833947 validloss: -0.8801135\n",
      "time: 98.3791720867157 Epoch: 441 trainloss: -0.98039407 validloss: -0.8956768\n",
      "time: 98.3041033744812 Epoch: 442 trainloss: -0.9833717 validloss: -0.8862222\n",
      "time: 98.3533616065979 Epoch: 443 trainloss: -0.9812283 validloss: -0.89677423\n",
      "time: 97.98720788955688 Epoch: 444 trainloss: -0.97903115 validloss: -0.87978476\n",
      "time: 98.35598230361938 Epoch: 445 trainloss: -0.98032665 validloss: -0.9038083\n",
      "time: 98.35122323036194 Epoch: 446 trainloss: -0.9812276 validloss: -0.8951646\n",
      "time: 98.18701171875 Epoch: 447 trainloss: -0.98128074 validloss: -0.89842147\n",
      "time: 98.37831687927246 Epoch: 448 trainloss: -0.98146045 validloss: -0.8774424\n",
      "time: 98.17974495887756 Epoch: 449 trainloss: -0.9786783 validloss: -0.8968902\n",
      "time: 98.18938994407654 Epoch: 450 trainloss: -0.98299664 validloss: -0.9024766\n",
      "time: 98.3685393333435 Epoch: 451 trainloss: -0.9841548 validloss: -0.8975686\n",
      "time: 98.11021566390991 Epoch: 452 trainloss: -0.9813665 validloss: -0.8872442\n",
      "time: 98.2959794998169 Epoch: 453 trainloss: -0.9829722 validloss: -0.8995808\n",
      "time: 98.35390877723694 Epoch: 454 trainloss: -0.9798691 validloss: -0.8959175\n",
      "time: 98.14799308776855 Epoch: 455 trainloss: -0.977703 validloss: -0.8903647\n",
      "time: 98.35557985305786 Epoch: 456 trainloss: -0.9782799 validloss: -0.89664847\n",
      "time: 98.35119128227234 Epoch: 457 trainloss: -0.98105353 validloss: -0.89736503\n",
      "time: 98.17716431617737 Epoch: 458 trainloss: -0.98096704 validloss: -0.88782257\n",
      "time: 98.37116742134094 Epoch: 459 trainloss: -0.98341334 validloss: -0.8749562\n",
      "time: 98.36715245246887 Epoch: 460 trainloss: -0.98083144 validloss: -0.90130824\n",
      "time: 98.21040868759155 Epoch: 461 trainloss: -0.9812686 validloss: -0.890957\n",
      "time: 98.38056421279907 Epoch: 462 trainloss: -0.98154336 validloss: -0.89499885\n",
      "time: 98.39617419242859 Epoch: 463 trainloss: -0.98266226 validloss: -0.892321\n",
      "time: 98.42854189872742 Epoch: 464 trainloss: -0.98392975 validloss: -0.8954288\n",
      "time: 98.42282390594482 Epoch: 465 trainloss: -0.98277545 validloss: -0.90041965\n",
      "time: 98.68262004852295 Epoch: 466 trainloss: -0.98286676 validloss: -0.90079343\n",
      "time: 98.24140214920044 Epoch: 467 trainloss: -0.9809447 validloss: -0.8873745\n",
      "time: 98.37044930458069 Epoch: 468 trainloss: -0.981151 validloss: -0.8991199\n",
      "time: 98.40175580978394 Epoch: 469 trainloss: -0.9834138 validloss: -0.9003738\n",
      "time: 98.32838463783264 Epoch: 470 trainloss: -0.9826377 validloss: -0.89074725\n",
      "time: 98.38665342330933 Epoch: 471 trainloss: -0.9818324 validloss: -0.89104563\n",
      "time: 98.2075662612915 Epoch: 472 trainloss: -0.9801229 validloss: -0.8996441\n",
      "time: 98.39187335968018 Epoch: 473 trainloss: -0.97966033 validloss: -0.8974558\n",
      "time: 98.3901515007019 Epoch: 474 trainloss: -0.96647805 validloss: -0.89599186\n",
      "time: 98.161776304245 Epoch: 475 trainloss: -0.97348636 validloss: -0.8895441\n",
      "time: 98.35618162155151 Epoch: 476 trainloss: -0.96904296 validloss: -0.8873329\n",
      "time: 98.34804391860962 Epoch: 477 trainloss: -0.9610413 validloss: -0.87910694\n",
      "time: 98.19830369949341 Epoch: 478 trainloss: -0.9708327 validloss: -0.86543626\n",
      "time: 98.35295987129211 Epoch: 479 trainloss: -0.96963286 validloss: -0.8780989\n",
      "time: 98.25641012191772 Epoch: 480 trainloss: -0.9713836 validloss: -0.8875589\n",
      "time: 98.37468481063843 Epoch: 481 trainloss: -0.9619774 validloss: -0.8865644\n",
      "time: 98.41445517539978 Epoch: 482 trainloss: -0.97445136 validloss: -0.8757331\n",
      "time: 98.19369649887085 Epoch: 483 trainloss: -0.9730037 validloss: -0.8870472\n",
      "time: 98.35089659690857 Epoch: 484 trainloss: -0.9759965 validloss: -0.8896894\n",
      "time: 98.38825631141663 Epoch: 485 trainloss: -0.9679147 validloss: -0.8745257\n",
      "time: 98.40038561820984 Epoch: 486 trainloss: -0.97092843 validloss: -0.8827987\n",
      "time: 98.3786997795105 Epoch: 487 trainloss: -0.9747294 validloss: -0.8943358\n",
      "time: 98.31147742271423 Epoch: 488 trainloss: -0.97419906 validloss: -0.883942\n",
      "time: 98.2059555053711 Epoch: 489 trainloss: -0.97670543 validloss: -0.88942003\n",
      "time: 98.31055855751038 Epoch: 490 trainloss: -0.97861975 validloss: -0.885968\n",
      "time: 98.41012501716614 Epoch: 491 trainloss: -0.9786819 validloss: -0.88734895\n",
      "time: 98.38435935974121 Epoch: 492 trainloss: -0.9809947 validloss: -0.9016937\n",
      "time: 98.37579226493835 Epoch: 493 trainloss: -0.9791237 validloss: -0.89075756\n",
      "time: 98.20666885375977 Epoch: 494 trainloss: -0.9791415 validloss: -0.88893807\n",
      "time: 98.37989497184753 Epoch: 495 trainloss: -0.9801738 validloss: -0.9012784\n",
      "time: 98.39381456375122 Epoch: 496 trainloss: -0.9758139 validloss: -0.8954211\n",
      "time: 98.3825798034668 Epoch: 497 trainloss: -0.98169225 validloss: -0.88379794\n",
      "time: 98.35213136672974 Epoch: 498 trainloss: -0.978239 validloss: -0.8873011\n",
      "time: 98.36736798286438 Epoch: 499 trainloss: -0.9821825 validloss: -0.89759886\n",
      "time: 98.18706345558167 Epoch: 500 trainloss: -0.9794577 validloss: -0.8952555\n"
     ]
    }
   ],
   "source": [
    "Es_train=[]\n",
    "for epoch in range(0,epochs+1):\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    metric.reset()\n",
    "\n",
    "    train_iter.reset()\n",
    "    \n",
    "    valid_iter.reset()\n",
    "    \n",
    "\n",
    "    for batch in train_iter:\n",
    "        # Copy data to executor input. Note the [:].\n",
    "        data[:] = batch.data[0]\n",
    "        label[:] = batch.label[0]\n",
    "\n",
    "        # Forward\n",
    "        outputs=exe.forward(is_train=True)\n",
    "        Es_train.append(outputs[1].asnumpy()[0])\n",
    "        # Backward\n",
    "        exe.backward()\n",
    "\n",
    "        # Update\n",
    "        for i, pair in enumerate(zip(exe.arg_arrays, exe.grad_arrays)):\n",
    "            weight, grad = pair\n",
    "            updater(i, grad, weight)   \n",
    "        metric.update(batch.label[0], exe.outputs[0])#metric.update(label,p)\n",
    "        \n",
    "    e=metric.get()\n",
    "    err_train=-e[1].asnumpy()[0]\n",
    "    \n",
    "    if epoch % 100== 0:       \n",
    "        #print(\"do_checkpoint\")\n",
    "        arg={k:v for k, v in arg_arrays.items() if k not in input_shapes}\n",
    "        aux = dict(zip(network.list_auxiliary_states(), exe.aux_arrays))\n",
    "        mx.model.save_checkpoint(prefix, epoch, network, arg, aux)\n",
    "        \n",
    "        \n",
    "    #compute valid loss per epoch    \n",
    "    metric.reset()\n",
    "    for batch in valid_iter:        \n",
    "        data[:] = batch.data[0]       \n",
    "        label[:] = batch.label[0]\n",
    "        # predict\n",
    "        outputs = exe.forward(is_train=False)\n",
    "        metric.update(batch.label[0], exe.outputs[0])\n",
    "    e=metric.get()\n",
    "    err_valid=-e[1].asnumpy()[0]\n",
    "    end = time.time()\n",
    "    print('time:',end-start,'Epoch:',epoch,'trainloss:',err_train,'validloss:',err_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Es_train2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-0c60021ad6b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEs_train2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'Es_train2' is not defined"
     ]
    }
   ],
   "source": [
    "np.mean(Es_train2,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Es_train[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Es_train = np.array(Es_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Es_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Es_train2 = Es_train.reshape((-1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.mean(Es_train2,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_iter.num_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "68/34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(Es_train,'.-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE=1\n",
    "test_iter=nn.FileIter(test_data_path,test_idx_path,batch_size=BATCH_SIZE,do_shuffle=False,mean_image=x_mean,std_image = x_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_sample_at(valid_iter,n):\n",
    "    valid_iter.ind2=[n]\n",
    "    return valid_iter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "K = valid_iter.num_data\n",
    "n = random.randint(0,K)\n",
    "print('index = '+ str(n))\n",
    "batch = get_sample_at(valid_iter,n)\n",
    "\n",
    "\n",
    "data[:] = batch.data[0]       \n",
    "label[:] = batch.label[0]\n",
    "# predict\n",
    "outputs = exe.forward(is_train=False)\n",
    "\n",
    "\n",
    "p = outputs[0][0].asnumpy().reshape(32,32,32)\n",
    "\n",
    "\n",
    "\n",
    "X = batch.data[0][0][0].asnumpy()\n",
    "Y = batch.label[0][0].asnumpy().reshape((32,32,32))\n",
    "                              \n",
    "img = X*x_std+x_mean\n",
    "msk1 = Y\n",
    "msk2 = p>.5#.001\n",
    "msk2=msk2*1\n",
    "msk1= np.ma.masked_where(msk1 == 0, msk1)\n",
    "msk2= np.ma.masked_where(msk2 == 0, msk2)\n",
    "\n",
    "zs=32\n",
    "num_rows=np.ceil(zs/8).astype(int)\n",
    "f, plots = plt.subplots(num_rows, 8, sharex='col', sharey='row', figsize=(10, 8))\n",
    "for i in range(zs):\n",
    "    plots[i // 8, i % 8].axis('off')\n",
    "    plots[i // 8, i % 8].imshow(img[i], 'gray',vmin=0,vmax=1)\n",
    "    plots[i // 8, i % 8].imshow(msk1[i],interpolation='none', cmap=plt.cm.Reds, alpha=.7, vmin=0, vmax=1)\n",
    "    plots[i // 8, i % 8].imshow(msk2[i],interpolation='none',  alpha=0.4, vmin=0, vmax=1)\n",
    "\n",
    "\n",
    "      \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": "1",
   "nav_menu": {
    "height": "290px",
    "width": "353.333px"
   },
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "284.95px",
    "left": "954px",
    "top": "110.567px",
    "width": "216.333px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
