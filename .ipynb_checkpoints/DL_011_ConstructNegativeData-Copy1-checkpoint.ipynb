{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DLToolboxImg: Part 2\n",
    "A set of helper functions that one repeatedly need to construct a dataset from raw images, visualise the performance of a neural network while it is getting trained, evaluate the performance of a model after training is completed. \n",
    "\n",
    "As a running example, I will apply the functinos on the LIDC dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "- [Generate Dataset](#generatedata)\n",
    "    - [Generate Negative Examples](#neg)\n",
    "    - [Generate Positive Examples](#pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib nbagg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imports import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"generatedata\"></a>\n",
    "## Generate Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the indices for train, valid, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir=\"drive/\"\n",
    "interm_dir=root_dir+\"interm5/\"\n",
    "filename=interm_dir+\"scan_id_split\"\n",
    "with open(filename, 'rb') as f:  # Python 3: open(..., 'wb')\n",
    "    scan_id_train,scan_id_valid,scan_id_test=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([15, 16, 17, 20, 22, 23], [19], [18, 21, 14])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scan_id_train,scan_id_valid,scan_id_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Negative Examples\n",
    "<a id=\"neg\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/home/mas/x110/data/’: File exists\n",
      "mkdir: cannot create directory ‘/home/mas/x110/data/pos’: File exists\n",
      "mkdir: cannot create directory ‘/home/mas/x110/data/neg’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir /home/mas/x110/data/\n",
    "!mkdir /home/mas/x110/data/pos\n",
    "!mkdir /home/mas/x110/data/neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "interm_dir2='/home/mas/x110/data/pos'\n",
    "interm_dir3='/home/mas/x110/data/neg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be taking small cubes from the ctscan volume. the size of this small cube is 52x52x52. I can serially decompose the say 512x512x300 ctscan volume into 52x52x52 cubes. But the problem with this approach is that I will have many \"unintresting\" cubes. like cubes that are all black. As an alternative, I will first create a lung mask. pick random points that resides inside the lung mask, and extract the 52x52x52 cube where the random point is the center of that cube. As a final check, I will make make sure that there does not exist a nodule in that cube, because remember we ar now generating negative examples. A summary of what I just described is:\n",
    "\n",
    "1. get a scan\n",
    "2. Apply the lung mask \n",
    "3. Find the range of zs where the lung occupies >2% of the total area. \n",
    "4. Select a random zc location.\n",
    "5. On that z slice, apply the lung mask.\n",
    "6. Select a random xc,yc point that resides inside the lung mask.\n",
    "7. extract a cube where xc,yc,zc is its center and its side is N=52.\n",
    "8. sum the mask of the newly generated cube to ensure that it does not include a nodule. \n",
    "9. The naming convention would be neg_scan_id_cx_xy_cz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dicom files ... This may take a moment.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mas/.virtualenvs/colab/lib/python3.5/site-packages/skimage/segmentation/_clear_border.py:58: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  borders[slicedim] = True\n",
      "/home/mas/.virtualenvs/colab/lib/python3.5/site-packages/skimage/segmentation/_clear_border.py:60: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  borders[slicedim] = True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n"
     ]
    }
   ],
   "source": [
    "random.seed(313)\n",
    "for scan_id in scan_id_train:#[xx+1:]:\n",
    "    scan_1 = ctscan(scan_id) \n",
    "    S,B=get_segmented_lungs2(scan_1.image_resampled, plot=False)\n",
    "\n",
    "    T = B.shape[1]**2\n",
    "    Areas=[np.sum(b)/T for b in B]\n",
    "    ind2=[i for i,a in enumerate(Areas) if a>.02]\n",
    "    z1,z2=ind2[0],ind2[-1]\n",
    "\n",
    "    for k in range(10):\n",
    "        zz=np.random.randint(z1,z2)\n",
    "\n",
    "        Bf=B[zz].flatten()\n",
    "        #In that slice, find the elements that are true\n",
    "        Cs=[i for i,e in enumerate(Bf) if e]\n",
    "        #randomly select an element from Cs\n",
    "        i = random.choice(Cs)\n",
    "        #from i get the original row and column of that element in B\n",
    "        a=B.shape[1];a\n",
    "        r = i//a\n",
    "        c=i-a*r\n",
    "\n",
    "        #Thus, we have succssfully selected a random point that resides inside the lung area\n",
    "        #we would like to extract a 52x52x52 patch from the ctscan volume.\n",
    "        #The patch is centered at the conditioned random point we have generated\n",
    "        m = 32\n",
    "        cz,cy,cx =[zz,r,c]\n",
    "        #grap the volume\n",
    "        image=scan_1.image_normalized #zxy\n",
    "        cube_img,corner0 = get_cube_from_img(image, cx, cy, cz, m)\n",
    "        cube_label,corner1 = get_cube_from_img(scan_1.Z2, cx, cy, cz, m)\n",
    "        if np.sum(cube_label)==0:\n",
    "            #save file\n",
    "            filename=interm_dir3+'/data_N_'+str(scan_id)+\"_\"+str(cx)+\"_\"+str(cy)+\"_\"+str(cz)+\".pkl\"\n",
    "            with open(filename, 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "                pickle.dump([cube_img,cube_label.astype(np.bool)], f)\n",
    "        else:\n",
    "            k=k-1\n",
    "\n",
    "#x=[i for i,j in enumerate(scan_id_train) if j==scan_id]\n",
    "#xx=x[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Negative Examples\n",
    "it is handy to create a csv file that contains a list of the file names and its class and some other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(58, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scan_id</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>label</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15</td>\n",
       "      <td>181</td>\n",
       "      <td>261</td>\n",
       "      <td>164</td>\n",
       "      <td>0</td>\n",
       "      <td>data_N_15_181_261_164.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>178</td>\n",
       "      <td>183</td>\n",
       "      <td>233</td>\n",
       "      <td>0</td>\n",
       "      <td>data_N_15_178_183_233.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>262</td>\n",
       "      <td>276</td>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "      <td>data_N_15_262_276_77.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>102</td>\n",
       "      <td>246</td>\n",
       "      <td>130</td>\n",
       "      <td>0</td>\n",
       "      <td>data_N_15_102_246_130.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>282</td>\n",
       "      <td>226</td>\n",
       "      <td>237</td>\n",
       "      <td>0</td>\n",
       "      <td>data_N_15_282_226_237.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>15</td>\n",
       "      <td>183</td>\n",
       "      <td>182</td>\n",
       "      <td>131</td>\n",
       "      <td>0</td>\n",
       "      <td>data_N_15_183_182_131.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>15</td>\n",
       "      <td>160</td>\n",
       "      <td>188</td>\n",
       "      <td>205</td>\n",
       "      <td>0</td>\n",
       "      <td>data_N_15_160_188_205.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>15</td>\n",
       "      <td>296</td>\n",
       "      <td>223</td>\n",
       "      <td>117</td>\n",
       "      <td>0</td>\n",
       "      <td>data_N_15_296_223_117.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>15</td>\n",
       "      <td>297</td>\n",
       "      <td>202</td>\n",
       "      <td>95</td>\n",
       "      <td>0</td>\n",
       "      <td>data_N_15_297_202_95.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15</td>\n",
       "      <td>324</td>\n",
       "      <td>220</td>\n",
       "      <td>158</td>\n",
       "      <td>0</td>\n",
       "      <td>data_N_15_324_220_158.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>16</td>\n",
       "      <td>116</td>\n",
       "      <td>211</td>\n",
       "      <td>296</td>\n",
       "      <td>0</td>\n",
       "      <td>data_N_16_116_211_296.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>16</td>\n",
       "      <td>222</td>\n",
       "      <td>164</td>\n",
       "      <td>270</td>\n",
       "      <td>0</td>\n",
       "      <td>data_N_16_222_164_270.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>16</td>\n",
       "      <td>264</td>\n",
       "      <td>150</td>\n",
       "      <td>157</td>\n",
       "      <td>0</td>\n",
       "      <td>data_N_16_264_150_157.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>16</td>\n",
       "      <td>215</td>\n",
       "      <td>253</td>\n",
       "      <td>91</td>\n",
       "      <td>0</td>\n",
       "      <td>data_N_16_215_253_91.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>16</td>\n",
       "      <td>125</td>\n",
       "      <td>252</td>\n",
       "      <td>103</td>\n",
       "      <td>0</td>\n",
       "      <td>data_N_16_125_252_103.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>284</td>\n",
       "      <td>235</td>\n",
       "      <td>124</td>\n",
       "      <td>0</td>\n",
       "      <td>data_N_16_284_235_124.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>167</td>\n",
       "      <td>200</td>\n",
       "      <td>153</td>\n",
       "      <td>0</td>\n",
       "      <td>data_N_16_167_200_153.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>16</td>\n",
       "      <td>113</td>\n",
       "      <td>165</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>data_N_16_113_165_250.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>16</td>\n",
       "      <td>119</td>\n",
       "      <td>193</td>\n",
       "      <td>218</td>\n",
       "      <td>0</td>\n",
       "      <td>data_N_16_119_193_218.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>17</td>\n",
       "      <td>266</td>\n",
       "      <td>135</td>\n",
       "      <td>182</td>\n",
       "      <td>0</td>\n",
       "      <td>data_N_17_266_135_182.pkl</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   scan_id    x    y    z  label                   filename\n",
       "0       15  181  261  164      0  data_N_15_181_261_164.pkl\n",
       "1       15  178  183  233      0  data_N_15_178_183_233.pkl\n",
       "2       15  262  276   77      0   data_N_15_262_276_77.pkl\n",
       "3       15  102  246  130      0  data_N_15_102_246_130.pkl\n",
       "4       15  282  226  237      0  data_N_15_282_226_237.pkl\n",
       "5       15  183  182  131      0  data_N_15_183_182_131.pkl\n",
       "6       15  160  188  205      0  data_N_15_160_188_205.pkl\n",
       "7       15  296  223  117      0  data_N_15_296_223_117.pkl\n",
       "8       15  297  202   95      0   data_N_15_297_202_95.pkl\n",
       "9       15  324  220  158      0  data_N_15_324_220_158.pkl\n",
       "10      16  116  211  296      0  data_N_16_116_211_296.pkl\n",
       "11      16  222  164  270      0  data_N_16_222_164_270.pkl\n",
       "12      16  264  150  157      0  data_N_16_264_150_157.pkl\n",
       "13      16  215  253   91      0   data_N_16_215_253_91.pkl\n",
       "14      16  125  252  103      0  data_N_16_125_252_103.pkl\n",
       "15      16  284  235  124      0  data_N_16_284_235_124.pkl\n",
       "16      16  167  200  153      0  data_N_16_167_200_153.pkl\n",
       "17      16  113  165  250      0  data_N_16_113_165_250.pkl\n",
       "18      16  119  193  218      0  data_N_16_119_193_218.pkl\n",
       "19      17  266  135  182      0  data_N_17_266_135_182.pkl"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if True:\n",
    "    temp=!ls {interm_dir3} -irlat #>> myfiles2.csv\n",
    "    #keep string that satisfy a condition\n",
    "    temp1=[t for t in temp if \"data\" in t]\n",
    "    temp1[0:5]\n",
    "\n",
    "    temp2=[t.split(\" \")[-1] for t in temp1]\n",
    "\n",
    "    df=pd.DataFrame([t.split(\".\")[0].split(\"_\")[-4:] for t in temp2],columns=['scan_id','x','y','z'])\n",
    "\n",
    "    df['label']=0\n",
    "    df['filename']=temp2\n",
    "    df.to_csv(interm_dir3+\"df_neg_scanid_centroid.csv\")\n",
    "else:\n",
    "    df=pd.read_csv(interm_dir3+\"df_neg_scanid_centroid.csv\",index_col=0)\n",
    "print(df.shape)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
