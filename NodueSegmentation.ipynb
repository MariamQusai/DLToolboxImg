{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nodule Segmentation in Lung Ct-scans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Configuration\" data-toc-modified-id=\"Configuration-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Configuration</a></span></li><li><span><a href=\"#LIDC-Dataset\" data-toc-modified-id=\"LIDC-Dataset-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>LIDC Dataset</a></span><ul class=\"toc-item\"><li><span><a href=\"#Description\" data-toc-modified-id=\"Description-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Description</a></span></li><li><span><a href=\"#Load-Dataset\" data-toc-modified-id=\"Load-Dataset-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Load Dataset</a></span></li><li><span><a href=\"#Split-scans-to-train,-validate,-and-test\" data-toc-modified-id=\"Split-scans-to-train,-validate,-and-test-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Split scans to train, validate, and test</a></span></li><li><span><a href=\"#Filter-annotation-according-to-nodule-diameter\" data-toc-modified-id=\"Filter-annotation-according-to-nodule-diameter-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Filter annotation according to nodule diameter</a></span></li><li><span><a href=\"#Plot-distribution-of-nodule-diameter\" data-toc-modified-id=\"Plot-distribution-of-nodule-diameter-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Plot distribution of nodule diameter</a></span></li><li><span><a href=\"#Save-train,-validate,-and-test-scan-indices-in-a-file\" data-toc-modified-id=\"Save-train,-validate,-and-test-scan-indices-in-a-file-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>Save train, validate, and test scan indices in a file</a></span></li></ul></li><li><span><a href=\"#Nodule-Dataset\" data-toc-modified-id=\"Nodule-Dataset-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Nodule Dataset</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Description\" data-toc-modified-id=\"Description-3.0.1\"><span class=\"toc-item-num\">3.0.1&nbsp;&nbsp;</span>Description</a></span></li><li><span><a href=\"#Generate-Negative-Examples\" data-toc-modified-id=\"Generate-Negative-Examples-3.0.2\"><span class=\"toc-item-num\">3.0.2&nbsp;&nbsp;</span>Generate Negative Examples</a></span></li><li><span><a href=\"#Read-Negative-Examples\" data-toc-modified-id=\"Read-Negative-Examples-3.0.3\"><span class=\"toc-item-num\">3.0.3&nbsp;&nbsp;</span>Read Negative Examples</a></span></li><li><span><a href=\"#Generate-Positive-Examples\" data-toc-modified-id=\"Generate-Positive-Examples-3.0.4\"><span class=\"toc-item-num\">3.0.4&nbsp;&nbsp;</span>Generate Positive Examples</a></span></li><li><span><a href=\"#Read-Positive-Examples\" data-toc-modified-id=\"Read-Positive-Examples-3.0.5\"><span class=\"toc-item-num\">3.0.5&nbsp;&nbsp;</span>Read Positive Examples</a></span></li><li><span><a href=\"#Combine-both-into-a-distribution-of-n:m-(pos:neg)\" data-toc-modified-id=\"Combine-both-into-a-distribution-of-n:m-(pos:neg)-3.0.6\"><span class=\"toc-item-num\">3.0.6&nbsp;&nbsp;</span>Combine both into a distribution of n:m (pos:neg)</a></span></li><li><span><a href=\"#Data-Augmentation\" data-toc-modified-id=\"Data-Augmentation-3.0.7\"><span class=\"toc-item-num\">3.0.7&nbsp;&nbsp;</span>Data Augmentation</a></span></li><li><span><a href=\"#Convert-into-rec-file\" data-toc-modified-id=\"Convert-into-rec-file-3.0.8\"><span class=\"toc-item-num\">3.0.8&nbsp;&nbsp;</span>Convert into rec file</a></span></li><li><span><a href=\"#Repeat-for-validate\" data-toc-modified-id=\"Repeat-for-validate-3.0.9\"><span class=\"toc-item-num\">3.0.9&nbsp;&nbsp;</span>Repeat for validate</a></span></li><li><span><a href=\"#Repeat-for-test\" data-toc-modified-id=\"Repeat-for-test-3.0.10\"><span class=\"toc-item-num\">3.0.10&nbsp;&nbsp;</span>Repeat for test</a></span></li></ul></li></ul></li><li><span><a href=\"#Model\" data-toc-modified-id=\"Model-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Configuration\" data-toc-modified-id=\"Configuration-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Configuration</a></span></li><li><span><a href=\"#Compute-Dataset-Statistics\" data-toc-modified-id=\"Compute-Dataset-Statistics-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Compute Dataset Statistics</a></span><ul class=\"toc-item\"><li><span><a href=\"#Find-Mean\" data-toc-modified-id=\"Find-Mean-4.2.1\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;</span>Find Mean</a></span></li><li><span><a href=\"#Find-Variance\" data-toc-modified-id=\"Find-Variance-4.2.2\"><span class=\"toc-item-num\">4.2.2&nbsp;&nbsp;</span>Find Variance</a></span></li></ul></li><li><span><a href=\"#Define-Data-Iterator\" data-toc-modified-id=\"Define-Data-Iterator-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Define Data Iterator</a></span></li><li><span><a href=\"#Define-Optimizer\" data-toc-modified-id=\"Define-Optimizer-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Define Optimizer</a></span></li><li><span><a href=\"#Define-Metric\" data-toc-modified-id=\"Define-Metric-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>Define Metric</a></span></li><li><span><a href=\"#Define-Model-Architecture\" data-toc-modified-id=\"Define-Model-Architecture-4.6\"><span class=\"toc-item-num\">4.6&nbsp;&nbsp;</span>Define Model Architecture</a></span></li><li><span><a href=\"#Initialize-Network-Weights\" data-toc-modified-id=\"Initialize-Network-Weights-4.7\"><span class=\"toc-item-num\">4.7&nbsp;&nbsp;</span>Initialize Network Weights</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-Pretrained-Neural-Network\" data-toc-modified-id=\"Load-Pretrained-Neural-Network-4.7.1\"><span class=\"toc-item-num\">4.7.1&nbsp;&nbsp;</span>Load Pretrained Neural Network</a></span></li><li><span><a href=\"#Initialize-network-to-random-Weights\" data-toc-modified-id=\"Initialize-network-to-random-Weights-4.7.2\"><span class=\"toc-item-num\">4.7.2&nbsp;&nbsp;</span>Initialize network to random Weights</a></span></li></ul></li><li><span><a href=\"#Train-Network\" data-toc-modified-id=\"Train-Network-4.8\"><span class=\"toc-item-num\">4.8&nbsp;&nbsp;</span>Train Network</a></span><ul class=\"toc-item\"><li><span><a href=\"#Find-a-good-learning-rate\" data-toc-modified-id=\"Find-a-good-learning-rate-4.8.1\"><span class=\"toc-item-num\">4.8.1&nbsp;&nbsp;</span>Find a good learning rate</a></span></li><li><span><a href=\"#Start-Training\" data-toc-modified-id=\"Start-Training-4.8.2\"><span class=\"toc-item-num\">4.8.2&nbsp;&nbsp;</span>Start Training</a></span></li></ul></li></ul></li><li><span><a href=\"#Evaluate-Model\" data-toc-modified-id=\"Evaluate-Model-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Evaluate Model</a></span></li><li><span><a href=\"#Book-Keeping\" data-toc-modified-id=\"Book-Keeping-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Book Keeping</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib nbagg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylidc as pl #pip install -Iv scikit-image==0.13\n",
    "from random import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import scipy\n",
    "import random\n",
    "import os\n",
    "from imports import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIDC Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm -r {root_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_all_data = False\n",
    "\n",
    "dataset_path = '/media/mas/Untitled/LIDC/DOI/'\n",
    "\n",
    "root_path = '/home/mas/x110/Datasets/Dataset2/'\n",
    "tmp_path = root_path + \"tmp/\"\n",
    "processed_path = root_path +\"processed/\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir {root_path}\n",
    "!mkdir {tmp_path}\n",
    "!mkdir {tmp_path+\"train/\"}\n",
    "!mkdir {tmp_path+\"valid/\"}\n",
    "!mkdir {tmp_path+\"test/\"}\n",
    "!mkdir {tmp_path+\"train/pos/\"}\n",
    "!mkdir {tmp_path+\"train/neg/\"}\n",
    "!mkdir {tmp_path+\"valid/pos/\"}\n",
    "!mkdir {tmp_path+\"valid/neg/\"}\n",
    "!mkdir {tmp_path+\"test/pos/\"}\n",
    "!mkdir {tmp_path+\"test/neg/\"}\n",
    "!mkdir {processed_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split_path =   tmp_path + \"scan_id_split.idx\"#save the scan ids that will be considered\n",
    "\n",
    "dftrain_path =  tmp_path + \"dftrain.csv\"\n",
    "dfvalid_path =  tmp_path + \"dfvalid.csv\"\n",
    "dftest_path  =  tmp_path + \"dftest.csv\"\n",
    "\n",
    "ds_train = 'train'\n",
    "ds_test = 'test'\n",
    "ds_valid = 'valid'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The Lung Image Database Consortium image collection (LIDC-IDRI) consists of diagnostic and lung cancer screening thoracic computed tomography (CT) scans with marked-up annotated lesions. It is a web-accessible international resource for development, training, and evaluation of computer-assisted diagnostic (CAD) methods for lung cancer detection and diagnosis. Initiated by the National Cancer Institute (NCI), further advanced by the Foundation for the National Institutes of Health (FNIH), and accompanied by the Food and Drug Administration (FDA) through active participation, this public-private partnership demonstrates the success of a consortium founded on a consensus-based process.\n",
    "\n",
    "Seven academic centers and eight medical imaging companies collaborated to create this data set which contains **1018 cases**.  Each subject includes images from a clinical thoracic CT scan and an associated XML file that records the results of a two-phase image annotation process performed by four experienced thoracic radiologists. In the initial blinded-read phase, each radiologist independently reviewed each CT scan and marked lesions belonging to one of three categories (\"nodule > or =3 mm,\" \"nodule <3 mm,\" and \"non-nodule > or =3 mm\"). In the subsequent unblinded-read phase, each radiologist independently reviewed their own marks along with the anonymized marks of the three other radiologists to render a final opinion. The goal of this process was to identify as completely as possible all lung nodules in each CT scan without requiring forced consensus.\" [[1]](https://wiki.cancerimagingarchive.net/display/Public/LIDC-IDRI)\n",
    "\n",
    " [Download dataset here](https://wiki.cancerimagingarchive.net/display/Public/LIDC-IDRI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment to download the data\n",
    "#!apt-get install icedtea-netx\n",
    "#!javaws TCIA_LIDC-IDRI_06-22-2015.jnlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = !ls {dataset_path}\n",
    "files = [f for f in files if not f.endswith(\".zip\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_all_data:\n",
    "    files = ['LIDC-IDRI-'+str(i+1).zfill(4) for i in range(12)]\n",
    "#files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "qu = pl.query(pl.Scan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Scan(id=1,patient_id=LIDC-IDRI-0078),\n",
       " Scan(id=2,patient_id=LIDC-IDRI-0069),\n",
       " Scan(id=3,patient_id=LIDC-IDRI-0079),\n",
       " Scan(id=4,patient_id=LIDC-IDRI-0101),\n",
       " Scan(id=5,patient_id=LIDC-IDRI-0110)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scans=qu.all() #all scans in the original LIDC dataset\n",
    "scans[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = [i for i,s in enumerate(scans) if s.patient_id in files]\n",
    "#ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_scans=[scans[i] for i in ind];  #mini_scans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scan=mini_scans[0]\n",
    "#scan.annotations\n",
    "#scan.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 0 12\n"
     ]
    }
   ],
   "source": [
    "#check all scans are ok to read\n",
    "scans_ok=[]\n",
    "scans_error=[]\n",
    "\n",
    "for q in mini_scans:\n",
    "    try:\n",
    "        q.get_path_to_dicom_files()\n",
    "        scans_ok.append(q)\n",
    "    except:\n",
    "        scans_error.append(q)\n",
    "\n",
    "print(len(scans_ok),len(scans_error),len(mini_scans))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split scans to train, validate, and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split scans into 60% for training, 20% for validation, and 20% for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 2 3\n"
     ]
    }
   ],
   "source": [
    "#shuffle data, then split to train, valid, test\n",
    "random.seed(313)\n",
    "random.shuffle(scans_ok)\n",
    "L=len(scans_ok)\n",
    "j=np.int(.6*L)\n",
    "jj=np.int(.2*L)\n",
    "scans_train=scans_ok[0:j]\n",
    "scans_valid=scans_ok[j:j+jj]\n",
    "scans_test=scans_ok[j+jj:]\n",
    "print(len(scans_train),len(scans_valid),len(scans_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scans_train,scans_valid,scans_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter annotation according to nodule diameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will consider only annonations of diameter < 30mm  and greater than 6mm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ann_id</th>\n",
       "      <th>scan_id</th>\n",
       "      <th>nodule_diameter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103</td>\n",
       "      <td>15</td>\n",
       "      <td>6.262191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>105</td>\n",
       "      <td>15</td>\n",
       "      <td>7.025444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>106</td>\n",
       "      <td>15</td>\n",
       "      <td>8.465747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>107</td>\n",
       "      <td>16</td>\n",
       "      <td>6.264748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>108</td>\n",
       "      <td>16</td>\n",
       "      <td>8.658309</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ann_id  scan_id  nodule_diameter\n",
       "0     103       15         6.262191\n",
       "1     105       15         7.025444\n",
       "2     106       15         8.465747\n",
       "3     107       16         6.264748\n",
       "4     108       16         8.658309"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scans0=scans_train\n",
    "filename = dftrain_path\n",
    "\n",
    "#consider only annonations of diameter < 30mm  and greater than 6mm \n",
    "\n",
    "if True:\n",
    "\n",
    "    l=[q.annotations for q in scans0]\n",
    "    anns = [item for sublist in l for item in sublist]\n",
    "\n",
    "    columns=['ann_id','scan_id','nodule_diameter']\n",
    "\n",
    "    df=[]\n",
    "    for scan in scans0:\n",
    "        for a in scan.annotations:\n",
    "            row = [a.id,a.scan_id,a.diameter]\n",
    "            df.append(row)\n",
    "\n",
    "    df1=pd.DataFrame(df,columns=columns)\n",
    "    #keep nodules between 6mm and 30 mm\n",
    "    df2=df1[(df1.nodule_diameter<=30) & (df1.nodule_diameter>=6)]\n",
    "    df2.reset_index(inplace=True,drop=True)\n",
    "    df2.to_csv(filename)\n",
    "    df_train=df2\n",
    "else: \n",
    "    df_train=pd.read_csv(filename,index_col=0)\n",
    "print(df_train.shape[0])\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ann_id</th>\n",
       "      <th>scan_id</th>\n",
       "      <th>nodule_diameter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>129</td>\n",
       "      <td>19</td>\n",
       "      <td>8.156489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>130</td>\n",
       "      <td>19</td>\n",
       "      <td>6.987712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>131</td>\n",
       "      <td>19</td>\n",
       "      <td>11.718750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>132</td>\n",
       "      <td>19</td>\n",
       "      <td>8.907621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>133</td>\n",
       "      <td>19</td>\n",
       "      <td>8.156489</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ann_id  scan_id  nodule_diameter\n",
       "0     129       19         8.156489\n",
       "1     130       19         6.987712\n",
       "2     131       19        11.718750\n",
       "3     132       19         8.907621\n",
       "4     133       19         8.156489"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Repeat for validation set\n",
    "scans0=scans_valid\n",
    "filename = dfvalid_path\n",
    "#consider only annonations of diameter < 30mm  and greater than 6mm \n",
    "\n",
    "if True:\n",
    "\n",
    "    l=[q.annotations for q in scans0]\n",
    "    anns = [item for sublist in l for item in sublist]\n",
    "\n",
    "    columns=['ann_id','scan_id','nodule_diameter']\n",
    "\n",
    "    df=[]\n",
    "    for scan in scans0:\n",
    "        for a in scan.annotations:\n",
    "            row = [a.id,a.scan_id,a.diameter]\n",
    "            df.append(row)\n",
    "\n",
    "    df1=pd.DataFrame(df,columns=columns)\n",
    "    #keep nodules between 6mm and 30 mm\n",
    "    df2=df1[(df1.nodule_diameter<=30) & (df1.nodule_diameter>=6)]\n",
    "    df2.reset_index(inplace=True,drop=True)\n",
    "    df2.to_csv(filename)\n",
    "    df_valid=df2\n",
    "    #print(f\"df1.shape[0],df2.shape[0]:{df1.shape[0]},{df2.shape[0]}\")\n",
    "else: \n",
    "    df_valid=pd.read_csv(filename,index_col=0)\n",
    "print(df_valid.shape[0])\n",
    "df_valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ann_id</th>\n",
       "      <th>scan_id</th>\n",
       "      <th>nodule_diameter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>125</td>\n",
       "      <td>18</td>\n",
       "      <td>20.684691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>91</td>\n",
       "      <td>14</td>\n",
       "      <td>13.353712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>92</td>\n",
       "      <td>14</td>\n",
       "      <td>14.674187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>94</td>\n",
       "      <td>14</td>\n",
       "      <td>14.326133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>95</td>\n",
       "      <td>14</td>\n",
       "      <td>11.943923</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ann_id  scan_id  nodule_diameter\n",
       "0     125       18        20.684691\n",
       "1      91       14        13.353712\n",
       "2      92       14        14.674187\n",
       "3      94       14        14.326133\n",
       "4      95       14        11.943923"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Repeat for Testing set\n",
    "scans0=scans_test\n",
    "filename = dftest_path\n",
    "#consider only annonations of diameter < 30mm  and greater than 6mm \n",
    "\n",
    "if True:\n",
    "\n",
    "    l=[q.annotations for q in scans0]\n",
    "    anns = [item for sublist in l for item in sublist]\n",
    "\n",
    "    columns=['ann_id','scan_id','nodule_diameter']\n",
    "\n",
    "    df=[]\n",
    "    for scan in scans0:\n",
    "        for a in scan.annotations:\n",
    "            row = [a.id,a.scan_id,a.diameter]\n",
    "            df.append(row)\n",
    "\n",
    "    df1=pd.DataFrame(df,columns=columns)\n",
    "    #keep nodules between 6mm and 30 mm\n",
    "    df2=df1[(df1.nodule_diameter<=30) & (df1.nodule_diameter>=6)]\n",
    "    df2.reset_index(inplace=True,drop=True)\n",
    "    df2.to_csv(filename)\n",
    "    df_test=df2\n",
    "    #print(f\"df1.shape[0],df2.shape[0]:{df1.shape[0]},{df2.shape[0]}\")\n",
    "else: \n",
    "    df_test=pd.read_csv(filename,index_col=0)\n",
    "print(df_test.shape[0])\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot distribution of nodule diameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train.hist(column='nodule_diameter')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#look at  the distribution of nodules after filtering\n",
    "df_valid.hist(column='nodule_diameter')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#look at  the distribution of nodules after filtering\n",
    "df_test.hist(column='nodule_diameter')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of the nodule diameter is higly unbalanced. The number of small nodules is greater than the number of large nodules. Hence,  if uniform sampling is used, the network will be biased towards small nodules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save train, validate, and test scan indices in a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    scan_id_train=list(set(df_train[\"scan_id\"]))\n",
    "    scan_id_valid=list(set(df_valid[\"scan_id\"]))\n",
    "    scan_id_test=list(set(df_test[\"scan_id\"]))\n",
    "    filename = train_test_split_path\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump([scan_id_train,scan_id_valid,scan_id_test], f)\n",
    "else:\n",
    "    filename=train_test_split_path\n",
    "    with open(filename, 'rb') as f:\n",
    "        scan_id_train,scan_id_valid,scan_id_test=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scan_id_train,scan_id_valid,scan_id_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan_id_set = {ds_train: scan_id_train,\n",
    "               ds_valid: scan_id_valid,\n",
    "               ds_test: scan_id_test}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nodule Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be taking small cubes from the ctscan volume. The size of this small cube is 52x52x52. I can serially decompose the say 512x512x300 ctscan volume into 52x52x52 cubes. But the problem with this approach is that I will have many \"unintresting\" cubes. like cubes that are all black. As an alternative, I will first create a lung mask. pick random points that resides inside the lung mask, and extract the 52x52x52 cube where the random point is the center of that cube. As a final check, I will make make sure that there does not exist a nodule in that cube, because remember we ar now generating negative examples. A summary of what I just described is:\n",
    "\n",
    "1. get a scan\n",
    "2. Apply the lung mask \n",
    "3. Find the range of zs where the lung occupies >2% of the total area. \n",
    "4. Select a random zc location.\n",
    "5. On that z slice, apply the lung mask.\n",
    "6. Select a random xc,yc point that resides inside the lung mask.\n",
    "7. extract a cube where xc,yc,zc is its center and its side is N=52.\n",
    "8. sum the mask of the newly generated cube to ensure that it does not include a nodule. \n",
    "9. The naming convention would be neg_scan_id_cx_xy_cz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Negative Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_type = ds_train\n",
    "interm_dir3 = tmp_path+ds_type+'/neg/'\n",
    "interm_dir2 = tmp_path+ds_type+'/pos/'\n",
    "fname_df = 'df_'+ds_type+'.csv'\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n",
      "Loading dicom files ... This may take a moment.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-7181fb964a60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m313\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mscan_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscan_id_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mds_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m#[xx+1:]:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mscan_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctscan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscan_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_segmented_lungs2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscan_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_resampled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/x110/DLToolboxImg/ctscan.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, scan_id)\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdesired_spacing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_resampled\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_spacing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_normalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_resampled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/x110/DLToolboxImg/ctscan.py\u001b[0m in \u001b[0;36mresample\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mnew_spacing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moriginal_spacing\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mreal_resize_factor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzoom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_resize_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'nearest'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mimage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswapaxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#zyx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/colab/lib/python3.5/site-packages/scipy/ndimage/interpolation.py\u001b[0m in \u001b[0;36mzoom\u001b[0;34m(input, zoom, output, order, mode, cval, prefilter)\u001b[0m\n\u001b[1;32m    593\u001b[0m                                                    shape=output_shape)\n\u001b[1;32m    594\u001b[0m     \u001b[0mzoom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mascontiguousarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzoom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m     \u001b[0m_nd_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzoom_shift\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzoom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    596\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "random.seed(313)\n",
    "for scan_id in scan_id_set[ds_type]:#[xx+1:]:\n",
    "    scan_1 = ctscan(scan_id) \n",
    "    S,B=get_segmented_lungs2(scan_1.image_resampled, plot=False)\n",
    "\n",
    "    T = B.shape[1]**2\n",
    "    Areas=[np.sum(b)/T for b in B]\n",
    "    ind2=[i for i,a in enumerate(Areas) if a>.02]\n",
    "    z1,z2=ind2[0],ind2[-1]\n",
    "\n",
    "    for k in range(epochs):\n",
    "        zz=np.random.randint(z1,z2)\n",
    "\n",
    "        Bf=B[zz].flatten()\n",
    "        #In that slice, find the elements that are true\n",
    "        Cs=[i for i,e in enumerate(Bf) if e]\n",
    "        #randomly select an element from Cs\n",
    "        i = random.choice(Cs)\n",
    "        #from i get the original row and column of that element in B\n",
    "        a=B.shape[1];a\n",
    "        r = i//a\n",
    "        c=i-a*r\n",
    "\n",
    "        #Thus, we have succssfully selected a random point that resides inside the lung area\n",
    "        #we would like to extract a 52x52x52 patch from the ctscan volume.\n",
    "        #The patch is centered at the conditioned random point we have generated\n",
    "        m = 52\n",
    "        cz,cy,cx =[zz,r,c]\n",
    "        #grap the volume\n",
    "        image=scan_1.image_normalized #zxy\n",
    "        cube_img,corner0 = get_cube_from_img(image, cx, cy, cz, m)\n",
    "        cube_label,corner1 = get_cube_from_img(scan_1.Z2, cx, cy, cz, m)\n",
    "        if np.sum(cube_label)==0:\n",
    "            #save file\n",
    "            filename=interm_dir3+'data_N_'+str(scan_id)+\"_\"+str(cx)+\"_\"+str(cy)+\"_\"+str(cz)+\".pkl\"\n",
    "            with open(filename, 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "                pickle.dump([cube_img,cube_label.astype(np.bool)], f)\n",
    "        else:\n",
    "            k=k-1\n",
    "\n",
    "#x=[i for i,j in enumerate(scan_id_train) if j==scan_id]\n",
    "#xx=x[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Negative Examples\n",
    "it is handy to create a csv file that contains a list of the file names and its class and some other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    temp=!ls {interm_dir3} -irlat #>> myfiles2.csv\n",
    "    #keep string that satisfy a condition\n",
    "    temp1=[t for t in temp if \"data\" in t]\n",
    "    temp1[0:5]\n",
    "\n",
    "    temp2=[t.split(\" \")[-1] for t in temp1]\n",
    "\n",
    "    df=pd.DataFrame([t.split(\".\")[0].split(\"_\")[-4:] for t in temp2],columns=['scan_id','x','y','z'])\n",
    "\n",
    "    df['label']=0\n",
    "    df['filename']=temp2\n",
    "    df.to_csv(tmp_path+ds_type+\"df_neg_scanid_centroid.csv\")\n",
    "else:\n",
    "    df=pd.read_csv(tmp_path+ds_type+\"_df_neg_scanid_centroid.csv\",index_col=0)\n",
    "print(df.shape)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Positive Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Strategy is to:\n",
    "- pick a nodule at random (repition not allowed)\n",
    "- extract the 52x52x52 cube\n",
    "- The naming convention would be pos_scan_id_cx_xy_cz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 52\n",
    "for scan_id in scan_id_train:\n",
    "    #grap the volume\n",
    "    scan_1 = ctscan(scan_id)\n",
    "    image=scan_1.image_normalized #zxy\n",
    "    for c2 in scan_1.centroids2:\n",
    "        cx,cy,cz =c2\n",
    "        cube_img,corner0 = get_cube_from_img(image, cx, cy, cz, m)\n",
    "        cube_label,corner1 = get_cube_from_img(scan_1.Z2, cx, cy, cz, m)\n",
    "        filename=interm_dir2+'data_P_'+str(scan_id)+\"_\"+str(cx)+\"_\"+str(cy)+\"_\"+str(cz)+\".pkl\"\n",
    "        with open(filename, 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "            pickle.dump([cube_img,cube_label.astype(np.bool)], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Positive Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    temp=!ls {interm_dir2} -irlat #>> myfiles2.csv\n",
    "    #keep string that satisfy a condition\n",
    "    temp1=[t for t in temp if \"data\" in t]\n",
    "    temp1[0:5]\n",
    "\n",
    "    temp2=[t.split(\" \")[-1] for t in temp1]\n",
    "\n",
    "    df=pd.DataFrame([t.split(\".\")[0].split(\"_\")[-4:] for t in temp2],columns=['scan_id','x','y','z'])\n",
    "    df['label']=1\n",
    "    df['filename']=temp2\n",
    "    #let us compute some analytics\n",
    "    Area=[]\n",
    "    for file in df.filename:\n",
    "        filename=interm_dir2+'/'+file\n",
    "        with open(filename, 'rb') as f:  # Python 3: open(..., 'wb')\n",
    "            X,Y = pickle.load(f)\n",
    "        Area.append(np.sum(Y)/52./52./52.*100)\n",
    "    df[\"Area_percentage\"] = Area\n",
    "    \n",
    "    df.to_csv(tmp_path+ds_type+\"df_pos_scanid_centroid.csv\")\n",
    "else:\n",
    "    df=pd.read_csv(tmp_path+ds_type+\"df_pos_scanid_centroid.csv\",index_col=0)\n",
    "print(df.shape)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us double check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=interm_dir2+'/'+df.filename[0]\n",
    "print(filename)\n",
    "with open(filename, 'rb') as f:  # Python 3: open(..., 'wb')\n",
    "    X,Y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zs=52\n",
    "\n",
    "X2 = X.copy()\n",
    "Z2=Y.copy()\n",
    "Z2 = np.ma.masked_where(Z2 ==0 , Z2)\n",
    "\n",
    "num_rows=7\n",
    "num_cols=8\n",
    "\n",
    "f, plots = plt.subplots(num_rows, num_cols, sharex='col', sharey='row', figsize=(5,5))\n",
    "\n",
    "ind=np.arange(0,52)\n",
    "for i in range(zs):\n",
    "    ii=ind[i]\n",
    "    plots[i // num_cols, i % num_cols].axis('off')\n",
    "    plots[i // num_cols, i % num_cols].imshow(X2[ii],'gray',vmin=0,vmax=1)\n",
    "\n",
    "    plots[i // num_cols, i % num_cols].imshow(Z2[ii],alpha=0.7,vmin=0,vmax=1)\n",
    "    plots[i // num_cols, i % num_cols].set_title(str(ii))\n",
    "      \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine both into a distribution of n:m (pos:neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp=pd.read_csv(tmp_path+ds_type+\"df_pos_scanid_centroid.csv\",index_col=0)\n",
    "del dfp[\"Area_percentage\"]\n",
    "dfp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfn=pd.read_csv(tmp_path+ds_type+\"df_neg_scanid_centroid.csv\",index_col=0)\n",
    "dfn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp.shape[0],dfn.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=np.int(dfp.shape[0]*.2)\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2= dfn.sample(n=n,random_state=313).reset_index(drop=True)\n",
    "print(dfn.shape)\n",
    "print(df2.shape)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=dfp.append(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df.sample(frac=1,random_state=313).reset_index(drop=True)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(processed_path+fname_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I generated positive examples, I extracted the mini cube around the nodule. Hence, the nodule will always be at the center. Now, we need to change this by introducing some offset in all directions x,y,z.\n",
    "\n",
    "The strategy is to get a random portion of this mini cube with the condition that the center should belong to the new \"mini mini cube\". The center of the original cube must contain a nodule (by design). Hence, this way we gurantee than the new cube will contain a nodule. \n",
    "\n",
    "By visually looking at the different scenarios of extracting a 32x32x32 cube from a 52x52x52 cube, I can say that all will contain the center.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(313)\n",
    "def random_crop(img,label):\n",
    "    #compute the upper left corner of the new cube\n",
    "    x = random.randint(0, 20) #Assume input is 52x52x52\n",
    "    y = random.randint(0, 20) #Assume input is 52x52x52\n",
    "    z = random.randint(0, 20) #Assume input is 52x52x52\n",
    "    img2 = img[x:x+32,y:y+32,z:z+32]\n",
    "    label2 = label[x:x+32,y:y+32,z:z+32]\n",
    "\n",
    "    return img2,label2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=interm_dir2+df.filename[12]\n",
    "print(filename)\n",
    "with open(filename, 'rb') as f:  # Python 3: open(..., 'wb')\n",
    "    X,Y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zs=32\n",
    "\n",
    "X2,Z2 = random_crop(X,Y)\n",
    "Z2 = np.ma.masked_where(Z2 ==0 , Z2)\n",
    "\n",
    "num_rows=6\n",
    "num_cols=6\n",
    "\n",
    "f, plots = plt.subplots(num_rows, num_cols, sharex='col', sharey='row', figsize=(5,5))\n",
    "\n",
    "ind=np.arange(0,32)\n",
    "for i in range(zs):\n",
    "    ii=ind[i]\n",
    "    plots[i // num_cols, i % num_cols].axis('off')\n",
    "    plots[i // num_cols, i % num_cols].imshow(X2[ii],'gray',vmin=0,vmax=1)\n",
    "\n",
    "    plots[i // num_cols, i % num_cols].imshow(Z2[ii],alpha=0.7,vmin=0,vmax=1)\n",
    "    plots[i // num_cols, i % num_cols].set_title(str(ii))\n",
    "      \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert into rec file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime \n",
    "\n",
    "now = datetime.datetime.now()\n",
    "today = now.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = today.replace('-','_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     2,
     19
    ]
   },
   "outputs": [],
   "source": [
    "import mxnet as mx #pip install mxnet-cu80\n",
    "#write to .rec file\n",
    "if False:#No longer used\n",
    "    fname==processed_path+ds_type+today+'.rec'\n",
    "\n",
    "    record = mx.recordio.MXRecordIO(fname, 'w')\n",
    "\n",
    "    counter=0\n",
    "    sum_img=np.zeros((32,32,32))\n",
    "\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        if row['label']==1:\n",
    "            path=interm_dir2\n",
    "        else:\n",
    "            path=interm_dir3\n",
    "\n",
    "        filename=path+'/'+row.filename\n",
    "\n",
    "        with open(filename, 'rb') as f:  # Python 3: open(..., 'wb')\n",
    "            x,t=pickle.load(f)\n",
    "            x,t = random_crop(x,t)\n",
    "\n",
    "\n",
    "\n",
    "        sum_img+=x\n",
    "\n",
    "        t=1*t.flatten()#(255*t.flatten()).astype(np.uint8)\n",
    "        x=x.flatten()#(255*x.flatten()).astype(np.uint8)\n",
    "        header = mx.recordio.IRHeader(flag=0, label=x, id=int(counter), id2=0)\n",
    "        s = mx.recordio.pack_img(header, t, quality=100,img_fmt='.png')\n",
    "        record.write(s)\n",
    "        counter +=1\n",
    "    record.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the above method, I can read data sequentially. However, I need to create a .rec file that supports random access. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx #pip install mxnet-cu80\n",
    "#write to .rec file\n",
    "if True:    \n",
    "    fname=processed_path+ds_type+today+'.rec'\n",
    "    idx=processed_path+ds_type+today+'.idx'\n",
    "\n",
    "    record = mx.recordio.MXIndexedRecordIO(idx, fname, 'w')\n",
    "\n",
    "    counter=0\n",
    "    sum_img=np.zeros((32,32,32))\n",
    "\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        if row['label']==1:\n",
    "            path=interm_dir2\n",
    "        else:\n",
    "            path=interm_dir3\n",
    "\n",
    "        filename=path+'/'+row.filename\n",
    "\n",
    "        with open(filename, 'rb') as f:  # Python 3: open(..., 'wb')\n",
    "            x,t=pickle.load(f)\n",
    "            x,t = random_crop(x,t)\n",
    "\n",
    "\n",
    "\n",
    "        sum_img+=x\n",
    "\n",
    "        t=1*t.flatten()#(255*t.flatten()).astype(np.uint8)\n",
    "        x=x.flatten()#(255*x.flatten()).astype(np.uint8)\n",
    "        header = mx.recordio.IRHeader(flag=0, label=x, id=int(counter), id2=0)\n",
    "        s = mx.recordio.pack_img(header, t, quality=100,img_fmt='.png')\n",
    "        record.write_idx(counter,s)\n",
    "        counter +=1\n",
    "    record.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repeat for validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_type = ds_valid\n",
    "interm_dir3 = tmp_path+ds_type+'/neg/'\n",
    "interm_dir2 = tmp_path+ds_type+'/pos/'\n",
    "fname_df = 'df_'+ds_type+'.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "random.seed(313)\n",
    "for scan_id in scan_id_set[ds_type]:#[xx+1:]:\n",
    "    scan_1 = ctscan(scan_id) \n",
    "    S,B=get_segmented_lungs2(scan_1.image_resampled, plot=False)\n",
    "\n",
    "    T = B.shape[1]**2\n",
    "    Areas=[np.sum(b)/T for b in B]\n",
    "    ind2=[i for i,a in enumerate(Areas) if a>.02]\n",
    "    z1,z2=ind2[0],ind2[-1]\n",
    "\n",
    "    for k in range(10):\n",
    "        zz=np.random.randint(z1,z2)\n",
    "\n",
    "        Bf=B[zz].flatten()\n",
    "        #In that slice, find the elements that are true\n",
    "        Cs=[i for i,e in enumerate(Bf) if e]\n",
    "        #randomly select an element from Cs\n",
    "        i = random.choice(Cs)\n",
    "        #from i get the original row and column of that element in B\n",
    "        a=B.shape[1];a\n",
    "        r = i//a\n",
    "        c=i-a*r\n",
    "\n",
    "        #Thus, we have succssfully selected a random point that resides inside the lung area\n",
    "        #we would like to extract a 52x52x52 patch from the ctscan volume.\n",
    "        #The patch is centered at the conditioned random point we have generated\n",
    "        m = 52\n",
    "        cz,cy,cx =[zz,r,c]\n",
    "        #grap the volume\n",
    "        image=scan_1.image_normalized #zxy\n",
    "        cube_img,corner0 = get_cube_from_img(image, cx, cy, cz, m)\n",
    "        cube_label,corner1 = get_cube_from_img(scan_1.Z2, cx, cy, cz, m)\n",
    "        if np.sum(cube_label)==0:\n",
    "            #save file\n",
    "            filename=interm_dir3+'data_N_'+str(scan_id)+\"_\"+str(cx)+\"_\"+str(cy)+\"_\"+str(cz)+\".pkl\"\n",
    "            with open(filename, 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "                pickle.dump([cube_img,cube_label.astype(np.bool)], f)\n",
    "        else:\n",
    "            k=k-1\n",
    "\n",
    "#x=[i for i,j in enumerate(scan_id_train) if j==scan_id]\n",
    "#xx=x[0]\n",
    "\n",
    "if True:\n",
    "    temp=!ls {interm_dir3} -irlat #>> myfiles2.csv\n",
    "    #keep string that satisfy a condition\n",
    "    temp1=[t for t in temp if \"data\" in t]\n",
    "    temp1[0:5]\n",
    "\n",
    "    temp2=[t.split(\" \")[-1] for t in temp1]\n",
    "\n",
    "    df=pd.DataFrame([t.split(\".\")[0].split(\"_\")[-4:] for t in temp2],columns=['scan_id','x','y','z'])\n",
    "\n",
    "    df['label']=0\n",
    "    df['filename']=temp2\n",
    "    df.to_csv(tmp_path+ds_type+\"df_neg_scanid_centroid.csv\")\n",
    "else:\n",
    "    df=pd.read_csv(tmp_path+ds_type+\"_df_neg_scanid_centroid.csv\",index_col=0)\n",
    "print(df.shape)\n",
    "df.head(20)\n",
    "\n",
    "m = 52\n",
    "for scan_id in scan_id_train:\n",
    "    #grap the volume\n",
    "    scan_1 = ctscan(scan_id)\n",
    "    image=scan_1.image_normalized #zxy\n",
    "    for c2 in scan_1.centroids2:\n",
    "        cx,cy,cz =c2\n",
    "        cube_img,corner0 = get_cube_from_img(image, cx, cy, cz, m)\n",
    "        cube_label,corner1 = get_cube_from_img(scan_1.Z2, cx, cy, cz, m)\n",
    "        filename=interm_dir2+'data_P_'+str(scan_id)+\"_\"+str(cx)+\"_\"+str(cy)+\"_\"+str(cz)+\".pkl\"\n",
    "        with open(filename, 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "            pickle.dump([cube_img,cube_label.astype(np.bool)], f)\n",
    "\n",
    "if True:\n",
    "    temp=!ls {interm_dir2} -irlat #>> myfiles2.csv\n",
    "    #keep string that satisfy a condition\n",
    "    temp1=[t for t in temp if \"data\" in t]\n",
    "    temp1[0:5]\n",
    "\n",
    "    temp2=[t.split(\" \")[-1] for t in temp1]\n",
    "\n",
    "    df=pd.DataFrame([t.split(\".\")[0].split(\"_\")[-4:] for t in temp2],columns=['scan_id','x','y','z'])\n",
    "    df['label']=1\n",
    "    df['filename']=temp2\n",
    "    #let us compute some analytics\n",
    "    Area=[]\n",
    "    for file in df.filename:\n",
    "        filename=interm_dir2+'/'+file\n",
    "        with open(filename, 'rb') as f:  # Python 3: open(..., 'wb')\n",
    "            X,Y = pickle.load(f)\n",
    "        Area.append(np.sum(Y)/52./52./52.*100)\n",
    "    df[\"Area_percentage\"] = Area\n",
    "    \n",
    "    df.to_csv(tmp_path+ds_type+\"df_pos_scanid_centroid.csv\")\n",
    "else:\n",
    "    df=pd.read_csv(tmp_path+ds_type+\"df_pos_scanid_centroid.csv\",index_col=0)\n",
    "print(df.shape)\n",
    "df.head(20)\n",
    "\n",
    "dfp=pd.read_csv(tmp_path+ds_type+\"df_pos_scanid_centroid.csv\",index_col=0)\n",
    "del dfp[\"Area_percentage\"]\n",
    "dfp.head()\n",
    "\n",
    "dfn=pd.read_csv(tmp_path+ds_type+\"df_neg_scanid_centroid.csv\",index_col=0)\n",
    "dfn.head()\n",
    "\n",
    "df2= dfn.sample(frac=.08,random_state=313).reset_index(drop=True)\n",
    "print(dfn.shape)\n",
    "print(df2.shape)\n",
    "df2.head()\n",
    "\n",
    "df=dfp.append(df2)\n",
    "\n",
    "df= df.sample(frac=1,random_state=313).reset_index(drop=True)\n",
    "print(df.shape)\n",
    "df.head()\n",
    "\n",
    "df.to_csv(processed_path+fname_df)\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "today = now.strftime('%Y-%m-%d')\n",
    "\n",
    "today = today.replace('-','_')\n",
    "\n",
    "#write to .rec file\n",
    "if True:    \n",
    "    fname=processed_path+ds_type+today+'.rec'\n",
    "    idx=processed_path+ds_type+today+'.idx'\n",
    "\n",
    "    record = mx.recordio.MXIndexedRecordIO(idx, fname, 'w')\n",
    "\n",
    "    counter=0\n",
    "    sum_img=np.zeros((32,32,32))\n",
    "\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        if row['label']==1:\n",
    "            path=interm_dir2\n",
    "        else:\n",
    "            path=interm_dir3\n",
    "\n",
    "        filename=path+'/'+row.filename\n",
    "\n",
    "        with open(filename, 'rb') as f:  # Python 3: open(..., 'wb')\n",
    "            x,t=pickle.load(f)\n",
    "            x,t = random_crop(x,t)\n",
    "\n",
    "\n",
    "\n",
    "        sum_img+=x\n",
    "\n",
    "        t=1*t.flatten()#(255*t.flatten()).astype(np.uint8)\n",
    "        x=x.flatten()#(255*x.flatten()).astype(np.uint8)\n",
    "        header = mx.recordio.IRHeader(flag=0, label=x, id=int(counter), id2=0)\n",
    "        s = mx.recordio.pack_img(header, t, quality=100,img_fmt='.png')\n",
    "        record.write_idx(counter,s)\n",
    "        counter +=1\n",
    "    record.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repeat for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_type = ds_test\n",
    "interm_dir3 = tmp_path+ds_type+'/neg/'\n",
    "interm_dir2 = tmp_path+ds_type+'/pos/'\n",
    "fname_df = 'df_'+ds_type+'.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(313)\n",
    "for scan_id in scan_id_set[ds_type]:#[xx+1:]:\n",
    "    scan_1 = ctscan(scan_id) \n",
    "    S,B=get_segmented_lungs2(scan_1.image_resampled, plot=False)\n",
    "\n",
    "    T = B.shape[1]**2\n",
    "    Areas=[np.sum(b)/T for b in B]\n",
    "    ind2=[i for i,a in enumerate(Areas) if a>.02]\n",
    "    z1,z2=ind2[0],ind2[-1]\n",
    "\n",
    "    for k in range(10):\n",
    "        zz=np.random.randint(z1,z2)\n",
    "\n",
    "        Bf=B[zz].flatten()\n",
    "        #In that slice, find the elements that are true\n",
    "        Cs=[i for i,e in enumerate(Bf) if e]\n",
    "        #randomly select an element from Cs\n",
    "        i = random.choice(Cs)\n",
    "        #from i get the original row and column of that element in B\n",
    "        a=B.shape[1];a\n",
    "        r = i//a\n",
    "        c=i-a*r\n",
    "\n",
    "        #Thus, we have succssfully selected a random point that resides inside the lung area\n",
    "        #we would like to extract a 52x52x52 patch from the ctscan volume.\n",
    "        #The patch is centered at the conditioned random point we have generated\n",
    "        m = 52\n",
    "        cz,cy,cx =[zz,r,c]\n",
    "        #grap the volume\n",
    "        image=scan_1.image_normalized #zxy\n",
    "        cube_img,corner0 = get_cube_from_img(image, cx, cy, cz, m)\n",
    "        cube_label,corner1 = get_cube_from_img(scan_1.Z2, cx, cy, cz, m)\n",
    "        if np.sum(cube_label)==0:\n",
    "            #save file\n",
    "            filename=interm_dir3+'data_N_'+str(scan_id)+\"_\"+str(cx)+\"_\"+str(cy)+\"_\"+str(cz)+\".pkl\"\n",
    "            with open(filename, 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "                pickle.dump([cube_img,cube_label.astype(np.bool)], f)\n",
    "        else:\n",
    "            k=k-1\n",
    "\n",
    "#x=[i for i,j in enumerate(scan_id_train) if j==scan_id]\n",
    "#xx=x[0]\n",
    "\n",
    "if True:\n",
    "    temp=!ls {interm_dir3} -irlat #>> myfiles2.csv\n",
    "    #keep string that satisfy a condition\n",
    "    temp1=[t for t in temp if \"data\" in t]\n",
    "    temp1[0:5]\n",
    "\n",
    "    temp2=[t.split(\" \")[-1] for t in temp1]\n",
    "\n",
    "    df=pd.DataFrame([t.split(\".\")[0].split(\"_\")[-4:] for t in temp2],columns=['scan_id','x','y','z'])\n",
    "\n",
    "    df['label']=0\n",
    "    df['filename']=temp2\n",
    "    df.to_csv(tmp_path+ds_type+\"df_neg_scanid_centroid.csv\")\n",
    "else:\n",
    "    df=pd.read_csv(tmp_path+ds_type+\"_df_neg_scanid_centroid.csv\",index_col=0)\n",
    "print(df.shape)\n",
    "df.head(20)\n",
    "\n",
    "m = 52\n",
    "for scan_id in scan_id_train:\n",
    "    #grap the volume\n",
    "    scan_1 = ctscan(scan_id)\n",
    "    image=scan_1.image_normalized #zxy\n",
    "    for c2 in scan_1.centroids2:\n",
    "        cx,cy,cz =c2\n",
    "        cube_img,corner0 = get_cube_from_img(image, cx, cy, cz, m)\n",
    "        cube_label,corner1 = get_cube_from_img(scan_1.Z2, cx, cy, cz, m)\n",
    "        filename=interm_dir2+'data_P_'+str(scan_id)+\"_\"+str(cx)+\"_\"+str(cy)+\"_\"+str(cz)+\".pkl\"\n",
    "        with open(filename, 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "            pickle.dump([cube_img,cube_label.astype(np.bool)], f)\n",
    "\n",
    "if True:\n",
    "    temp=!ls {interm_dir2} -irlat #>> myfiles2.csv\n",
    "    #keep string that satisfy a condition\n",
    "    temp1=[t for t in temp if \"data\" in t]\n",
    "    temp1[0:5]\n",
    "\n",
    "    temp2=[t.split(\" \")[-1] for t in temp1]\n",
    "\n",
    "    df=pd.DataFrame([t.split(\".\")[0].split(\"_\")[-4:] for t in temp2],columns=['scan_id','x','y','z'])\n",
    "    df['label']=1\n",
    "    df['filename']=temp2\n",
    "    #let us compute some analytics\n",
    "    Area=[]\n",
    "    for file in df.filename:\n",
    "        filename=interm_dir2+'/'+file\n",
    "        with open(filename, 'rb') as f:  # Python 3: open(..., 'wb')\n",
    "            X,Y = pickle.load(f)\n",
    "        Area.append(np.sum(Y)/52./52./52.*100)\n",
    "    df[\"Area_percentage\"] = Area\n",
    "    \n",
    "    df.to_csv(tmp_path+ds_type+\"df_pos_scanid_centroid.csv\")\n",
    "else:\n",
    "    df=pd.read_csv(tmp_path+ds_type+\"df_pos_scanid_centroid.csv\",index_col=0)\n",
    "print(df.shape)\n",
    "df.head(20)\n",
    "\n",
    "dfp=pd.read_csv(tmp_path+ds_type+\"df_pos_scanid_centroid.csv\",index_col=0)\n",
    "del dfp[\"Area_percentage\"]\n",
    "dfp.head()\n",
    "\n",
    "dfn=pd.read_csv(tmp_path+ds_type+\"df_neg_scanid_centroid.csv\",index_col=0)\n",
    "dfn.head()\n",
    "\n",
    "df2= dfn.sample(frac=.08,random_state=313).reset_index(drop=True)\n",
    "print(dfn.shape)\n",
    "print(df2.shape)\n",
    "df2.head()\n",
    "\n",
    "df=dfp.append(df2)\n",
    "\n",
    "df= df.sample(frac=1,random_state=313).reset_index(drop=True)\n",
    "print(df.shape)\n",
    "df.head()\n",
    "\n",
    "df.to_csv(processed_path+fname_df)\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "today = now.strftime('%Y-%m-%d')\n",
    "\n",
    "today = today.replace('-','_')\n",
    "\n",
    "#write to .rec file\n",
    "if True:    \n",
    "    fname=processed_path+ds_type+today+'.rec'\n",
    "    idx=processed_path+ds_type+today+'.idx'\n",
    "\n",
    "    record = mx.recordio.MXIndexedRecordIO(idx, fname, 'w')\n",
    "\n",
    "    counter=0\n",
    "    sum_img=np.zeros((32,32,32))\n",
    "\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        if row['label']==1:\n",
    "            path=interm_dir2\n",
    "        else:\n",
    "            path=interm_dir3\n",
    "\n",
    "        filename=path+'/'+row.filename\n",
    "\n",
    "        with open(filename, 'rb') as f:  # Python 3: open(..., 'wb')\n",
    "            x,t=pickle.load(f)\n",
    "            x,t = random_crop(x,t)\n",
    "\n",
    "\n",
    "\n",
    "        sum_img+=x\n",
    "\n",
    "        t=1*t.flatten()#(255*t.flatten()).astype(np.uint8)\n",
    "        x=x.flatten()#(255*x.flatten()).astype(np.uint8)\n",
    "        header = mx.recordio.IRHeader(flag=0, label=x, id=int(counter), id2=0)\n",
    "        s = mx.recordio.pack_img(header, t, quality=100,img_fmt='.png')\n",
    "        record.write_idx(counter,s)\n",
    "        counter +=1\n",
    "    record.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,\"./src\")\n",
    "import mynnet7 as nn\n",
    "import mxnet as mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interm_dir4='/home/mas/x110/data'\n",
    "train_data_path=interm_dir4+'/Train23Oct2018augment.rec'\n",
    "train_idx_path=interm_dir4+'/idx_Train23Oct2018augment.idx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir /home/mas/x110/model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"/home/mas/x110/model/oct26\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Dataset Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=1\n",
    "train_iter= nn.FileIter(train_data_path,train_idx_path,batch_size=BATCH_SIZE,do_augment=True,mean_image=0,std_image=1)\n",
    "train_iter.reset()\n",
    "x_mean = np.zeros((32,32,32))\n",
    "for i,batch in enumerate(train_iter):\n",
    "    X =  batch.data[0][0][0].asnumpy()\n",
    "    x_mean+=X\n",
    "x_mean=np.mean(x_mean/i)\n",
    "x_mean#x_mean=.2815"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=1\n",
    "train_iter=nn.FileIter(train_data_path,train_idx_path,batch_size=BATCH_SIZE,do_augment=True,mean_image=0,std_image = 1)\n",
    "train_iter.reset()\n",
    "x_var = np.zeros((32,32,32))\n",
    "for i,batch in enumerate(train_iter):\n",
    "    X =  (batch.data[0][0][0].asnumpy()-x_mean)**2\n",
    "    x_var+=X\n",
    "#x_var=x_var/(i-1)\n",
    "#x_var#x_mean=.2815\n",
    "N = i*32*32*32\n",
    "x_var = np.sum(x_var)/(N-1)\n",
    "x_var#x_var = .07877\n",
    "x_std = np.sqrt(x_var)#x_std=.2807\n",
    "x_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Data Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=5\n",
    "train_iter=nn.FileIter(train_data_path,train_idx_path,batch_size=BATCH_SIZE,do_augment=False,mean_image=x_mean,std_image = x_std)\n",
    "input_shapes = dict(train_iter.provide_data+train_iter.provide_label)\n",
    "print(input_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=5\n",
    "valid_iter=nn.FileIter(train_data_path,train_idx_path,batch_size=BATCH_SIZE,do_augment=False,mean_image=x_mean,std_image = x_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter.reset()\n",
    "valid_iter.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============Optimizer=================                        \n",
    "# We also need to create an optimizer for updating weights\n",
    "opt = mx.optimizer.SGD(\n",
    "    learning_rate=.01,momentum=0.99,wd=0.000001)\n",
    "    \n",
    "updater = mx.optimizer.get_updater(opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = mx.metric.CustomMetric(feval=nn.dice_coef2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Network Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Pretrained Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    model_path = \"/home/mas/x110/model/oct22\"\n",
    "    model_epoch=499\n",
    "    network, arg_params, aux_params = mx.model.load_checkpoint(model_path , model_epoch)\n",
    "\n",
    "    # Binding\n",
    "    exe = network.simple_bind(ctx=mx.gpu(0), **input_shapes)\n",
    "    # get handle to input arrays\n",
    "    arg_arrays = dict(zip(network.list_arguments(), exe.arg_arrays))\n",
    "    data = arg_arrays[train_iter.provide_data[0][0]]\n",
    "    label = arg_arrays[train_iter.provide_label[0][0]]\n",
    "\n",
    "\n",
    "    exe.copy_params_from(arg_params, aux_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize network to random Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = nn.get_net_317()\n",
    "init = mx.init.Normal(0.01) #note biases and gamma/beta are not affected\n",
    "\n",
    "# Binding\n",
    "exe = network.simple_bind(ctx=mx.gpu(), **input_shapes)\n",
    "# get handle to input arrays\n",
    "arg_arrays = dict(zip(network.list_arguments(), exe.arg_arrays))\n",
    "data = arg_arrays[train_iter.provide_data[0][0]]\n",
    "label = arg_arrays[train_iter.provide_label[0][0]]\n",
    "\n",
    "# Binding\n",
    "exe = network.simple_bind(ctx=mx.gpu(), **input_shapes)\n",
    "# get handle to input arrays\n",
    "arg_arrays = dict(zip(network.list_arguments(), exe.arg_arrays))\n",
    "data = arg_arrays[train_iter.provide_data[0][0]]\n",
    "label = arg_arrays[train_iter.provide_label[0][0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find a good learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb=train_iter.num_data//train_iter.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sched=nn.lr_find(1e-4,nb,end_lr=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also need to create an optimizer for updating weights\n",
    "opt = mx.optimizer.SGD(\n",
    "    learning_rate=.01,\n",
    "    momentum=0.9,\n",
    "    wd=0.00001,\n",
    "    lr_scheduler=sched)\n",
    "\n",
    "updater = mx.optimizer.get_updater(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape of labels 1 does not match shape of predictions 5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-aa0ccd5d3eda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m# predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0me\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0merr_valid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/colab/lib/python3.5/site-packages/mxnet/metric.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, labels, preds)\u001b[0m\n\u001b[1;32m   1370\u001b[0m         \"\"\"\n\u001b[1;32m   1371\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_allow_extra_outputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1372\u001b[0;31m             \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_label_shapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/colab/lib/python3.5/site-packages/mxnet/metric.py\u001b[0m in \u001b[0;36mcheck_label_shapes\u001b[0;34m(labels, preds, wrap, shape)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlabel_shape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mpred_shape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         raise ValueError(\"Shape of labels {} does not match shape of \"\n\u001b[0;32m---> 58\u001b[0;31m                          \"predictions {}\".format(label_shape, pred_shape))\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shape of labels 1 does not match shape of predictions 5"
     ]
    }
   ],
   "source": [
    "for epoch in range(0,1):\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    metric.reset()\n",
    "\n",
    "    train_iter.reset()\n",
    "    \n",
    "    valid_iter.reset()\n",
    "    \n",
    "    sched.reset()\n",
    "        \n",
    "    sched.on_train_begin()\n",
    "    \n",
    "\n",
    "    for batch in train_iter:\n",
    "        # Copy data to executor input. Note the [:].\n",
    "        data[:] = batch.data[0]\n",
    "        label[:] = batch.label[0]\n",
    "\n",
    "        # Forward\n",
    "        outputs=exe.forward(is_train=True)\n",
    "        # Backward\n",
    "        exe.backward()\n",
    "\n",
    "        # Update\n",
    "        for i, pair in enumerate(zip(exe.arg_arrays, exe.grad_arrays)):\n",
    "            weight, grad = pair\n",
    "            updater(i, grad, weight)   \n",
    "        metric.update(batch.label[0], exe.outputs[0])#metric.update(label,p)\n",
    "        \n",
    "        e=metric.get()\n",
    "        err_train=-e[1].asnumpy()[0]\n",
    "        sched.on_batch_end(err_train)\n",
    "    \n",
    "    if epoch % 100== 0:       \n",
    "        #print(\"do_checkpoint\")\n",
    "        arg={k:v for k, v in arg_arrays.items() if k not in input_shapes}\n",
    "        aux = dict(zip(network.list_auxiliary_states(), exe.aux_arrays))\n",
    "        mx.model.save_checkpoint(prefix, epoch, network, arg, aux)\n",
    "        \n",
    "        \n",
    "    #compute valid loss per epoch    \n",
    "    metric.reset()\n",
    "    for batch in valid_iter:        \n",
    "        data[:] = batch.data[0]       \n",
    "        label[:] = batch.label[0]\n",
    "        # predict\n",
    "        outputs = exe.forward(is_train=False)\n",
    "        metric.update(batch.label[0], exe.outputs[0])\n",
    "    e=metric.get()\n",
    "    err_valid=-e[1].asnumpy()[0]\n",
    "    end = time.time()\n",
    "    print('time:',end-start,'Epoch:',epoch,'trainloss:',err_train,'validloss:',err_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sched.lrs, sched.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also need to create an optimizer for updating weights\n",
    "opt = mx.optimizer.SGD(\n",
    "    learning_rate=.06,rescale_grad=1.0/train_iter.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updater.optimizer.lr_scheduler=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updater = mx.optimizer.get_updater(opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 38.903106927871704 Epoch: 0 trainloss: -0.03908517 validloss: -0.038967926\n",
      "time: 26.442142248153687 Epoch: 1 trainloss: -0.039042007 validloss: -0.039013416\n",
      "time: 28.107386350631714 Epoch: 2 trainloss: -0.039057143 validloss: -0.038939286\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(0,epochs+1):\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    metric.reset()\n",
    "\n",
    "    train_iter.reset()\n",
    "    \n",
    "    valid_iter.reset()\n",
    "    \n",
    "\n",
    "    for batch in train_iter:\n",
    "        # Copy data to executor input. Note the [:].\n",
    "        data[:] = batch.data[0]\n",
    "        label[:] = batch.label[0]\n",
    "\n",
    "        # Forward\n",
    "        outputs=exe.forward(is_train=True)\n",
    "        # Backward\n",
    "        exe.backward()\n",
    "\n",
    "        # Update\n",
    "        for i, pair in enumerate(zip(exe.arg_arrays, exe.grad_arrays)):\n",
    "            weight, grad = pair\n",
    "            updater(i, grad, weight)   \n",
    "        metric.update(batch.label[0], exe.outputs[0])#metric.update(label,p)\n",
    "        \n",
    "    e=metric.get()\n",
    "    err_train=-e[1].asnumpy()[0]\n",
    "    \n",
    "    if epoch % 100== 0:       \n",
    "        #print(\"do_checkpoint\")\n",
    "        arg={k:v for k, v in arg_arrays.items() if k not in input_shapes}\n",
    "        aux = dict(zip(network.list_auxiliary_states(), exe.aux_arrays))\n",
    "        mx.model.save_checkpoint(prefix, epoch, network, arg, aux)\n",
    "        \n",
    "        \n",
    "    #compute valid loss per epoch    \n",
    "    metric.reset()\n",
    "    for batch in valid_iter:        \n",
    "        data[:] = batch.data[0]       \n",
    "        label[:] = batch.label[0]\n",
    "        # predict\n",
    "        outputs = exe.forward(is_train=False)\n",
    "        metric.update(batch.label[0], exe.outputs[0])\n",
    "    e=metric.get()\n",
    "    err_valid=-e[1].asnumpy()[0]\n",
    "    end = time.time()\n",
    "    print('time:',end-start,'Epoch:',epoch,'trainloss:',err_train,'validloss:',err_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=1\n",
    "valid_iter=nn.FileIter(train_data_path,train_idx_path,batch_size=BATCH_SIZE,do_shuffle=False,mean_image=x_mean,std_image = x_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_at(valid_iter,n):\n",
    "    valid_iter.ind2=[n]\n",
    "    return valid_iter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = valid_iter.num_data\n",
    "n = random.randint(0,K)\n",
    "print('index = '+ str(n))\n",
    "batch = get_sample_at(valid_iter,n)\n",
    "\n",
    "\n",
    "data[:] = batch.data[0]       \n",
    "label[:] = batch.label[0]\n",
    "# predict\n",
    "outputs = exe.forward(is_train=False)\n",
    "\n",
    "\n",
    "p = outputs[0][0].asnumpy().reshape(32,32,32)\n",
    "\n",
    "\n",
    "\n",
    "X = batch.data[0][0][0].asnumpy()\n",
    "Y = batch.label[0][0].asnumpy().reshape((32,32,32))\n",
    "                              \n",
    "img = X*x_std+x_mean\n",
    "msk1 = Y\n",
    "msk2 = p>.5#.001\n",
    "msk2=msk2*1\n",
    "msk1= np.ma.masked_where(msk1 == 0, msk1)\n",
    "msk2= np.ma.masked_where(msk2 == 0, msk2)\n",
    "\n",
    "zs=32\n",
    "num_rows=np.ceil(zs/8).astype(int)\n",
    "f, plots = plt.subplots(num_rows, 8, sharex='col', sharey='row', figsize=(10, 8))\n",
    "for i in range(zs):\n",
    "    plots[i // 8, i % 8].axis('off')\n",
    "    plots[i // 8, i % 8].imshow(img[i], 'gray',vmin=0,vmax=1)\n",
    "    plots[i // 8, i % 8].imshow(msk1[i],interpolation='none', cmap=plt.cm.Reds, alpha=.7, vmin=0, vmax=1)\n",
    "    plots[i // 8, i % 8].imshow(msk2[i],interpolation='none',  alpha=0.4, vmin=0, vmax=1)\n",
    "\n",
    "\n",
    "      \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Book Keeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "442.667px",
    "left": "819px",
    "top": "110.567px",
    "width": "317.667px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
