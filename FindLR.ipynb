{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FindLR.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/x110/DLToolboxImg/blob/master/FindLR.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "JuER_M7AbaOq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Below I show an implementation of how to find the learning rate to use when training deep neural networks. "
      ]
    },
    {
      "metadata": {
        "id": "9urz3fDLItZ_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Install dependencies on Google colab\n"
      ]
    },
    {
      "metadata": {
        "id": "7jwaXzyc0eDG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Prepare Machine\n",
        "## install dependencies\n",
        "!apt install libnvrtc8.0\n",
        "!pip install mxnet-cu80\n",
        "!apt-get install ffmpeg\n",
        "!apt-get install imagemagick\n",
        "!pip install pylidc\n",
        "!pip install dicom"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q1lXTQUabylr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Imports and functions"
      ]
    },
    {
      "metadata": {
        "id": "Hgh08EcVgidE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import mxnet as mx\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import logging\n",
        "import math \n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.DEBUG)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IFpHbWPd8EzH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class LRScheduler(object):\n",
        "    \"\"\"Base class of a learning rate scheduler.\n",
        "\n",
        "    A scheduler returns a new learning rate based on the number of updates that have\n",
        "    been performed.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    base_lr : float, optional\n",
        "        The initial learning rate.\n",
        "    \"\"\"\n",
        "    def __init__(self, base_lr=0.01):\n",
        "        self.base_lr = base_lr\n",
        "\n",
        "    def __call__(self, num_update):\n",
        "        \"\"\"Return a new learning rate.\n",
        "\n",
        "        The ``num_update`` is the upper bound of the number of updates applied to\n",
        "        every weight.\n",
        "\n",
        "        Assume the optimizer has updated *i*-th weight by *k_i* times, namely\n",
        "        ``optimizer.update(i, weight_i)`` is called by *k_i* times. Then::\n",
        "\n",
        "            num_update = max([k_i for all i])\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        num_update: int\n",
        "            the maximal number of updates applied to a weight.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"must override this\")\n",
        "class lr_find(LRScheduler):\n",
        "    \"\"\"Reduce the learning rate by a factor for every *n* steps.\n",
        "\n",
        "    It returns a new learning rate by::\n",
        "\n",
        "        base_lr * pow(factor, floor(num_update/step))\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    step : int\n",
        "        Changes the learning rate for every n updates.\n",
        "    factor : float, optional\n",
        "        The factor to change the learning rate.\n",
        "    stop_factor_lr : float, optional\n",
        "        Stop updating the learning rate if it is less than this value.\n",
        "        \n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, layer_opt_lr, nb, end_lr=10, linear=True):\n",
        "        super(lr_find,self).__init__()\n",
        "\n",
        "        self.linear = linear\n",
        "        ratio = end_lr/layer_opt_lr\n",
        "        self.lr_mult = (ratio/nb) if linear else ratio**(1/nb)\n",
        "        self.iteration = 1\n",
        "        self.losses=[]\n",
        "        self.lrs=[]\n",
        "        self.init_lrs=layer_opt_lr\n",
        "        self.new_lr = self.init_lrs\n",
        "\n",
        "    def on_train_begin(self):\n",
        "        self.best=1e9\n",
        "        \n",
        "        \n",
        "    def __call__(self,b):\n",
        "        return self.new_lr\n",
        "\n",
        "\n",
        "\n",
        "    def on_batch_end(self, loss):\n",
        "        self.losses.append(loss)\n",
        "        mult = self.lr_mult*self.iteration if self.linear else self.lr_mult**self.iteration\n",
        "        self.iteration +=1\n",
        "        self.new_lr = self.init_lrs * mult\n",
        "        self.lrs.append(self.new_lr)\n",
        "        return self.init_lrs * mult\n",
        "        if math.isnan(loss) or loss>self.best*4:\n",
        "            return True\n",
        "        if (loss<self.best and self.iteration>10): self.best=loss\n",
        "\n",
        "    def plot(self, n_skip=10):\n",
        "        plt.ylabel(\"loss\")\n",
        "        plt.xlabel(\"learning rate (log scale)\")\n",
        "        plt.plot(self.lrs[n_skip:-5], self.losses[n_skip:-5])\n",
        "        plt.xscale('log')\n",
        "        \n",
        "    def reset(self):\n",
        "        self.iteration = 1\n",
        "        self.losses=[]\n",
        "        self.lrs=[]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3iwfoqHreiK2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data"
      ]
    },
    {
      "metadata": {
        "id": "6uKnRvlUhLUM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "e15ce1ab-b72b-4c40-f69e-845038a36d08"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_mldata\n",
        "mnist = fetch_mldata('MNIST original')\n",
        "np.random.seed(1234) # set seed for deterministic ordering\n",
        "p = np.random.permutation(mnist.data.shape[0])\n",
        "X = mnist.data[p]\n",
        "Y = mnist.target[p]\n",
        "\n",
        "for i in range(10):\n",
        "    plt.subplot(1,10,i+1)\n",
        "    plt.imshow(X[i].reshape((28,28)), cmap='Greys_r')\n",
        "    plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "X = X.astype(np.float32)/255\n",
        "X_train = X[:60000]\n",
        "X_test = X[60000:]\n",
        "Y_train = Y[:60000]\n",
        "Y_test = Y[60000:]\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAA/CAYAAAD9lUMPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAE69JREFUeJztnXlMFPf7x4fLcIYjhMukVsi3BWKD\npElpbTBibNSUaELFFsS0UmlqQUkqivGilUaiUQzRrCm2ARsEUrW2EKlJrRWVikCK1HKWcz04DHIs\ny7LAzPv3h90Je8/uzgKT3/NKnogwzLz3+XzmPZ9zcADAEARBENLFcaEFEARBELZBRk4QBCFxyMgJ\ngiAkDhk5QRCExCEjJwiCkDhk5ARBEBKHjJwgCELikJETBEFIHDJygiAIieM8HxdxcHCY9+2jABwW\nq47FpIV0kA7SIT0dulCLnCAIQuKQkUsUDw8PJj8/nxkdHV1oKQRBLDBk5BLDy8uL8fLyYmpqapjM\nzEwmKSlpoSUtKCkpKUxKSgozOTnJAGDOnTvHBAQELLQsgphfANg9GIaBGOHj44Pp6WkoFAq8/fbb\nJo+1pw5LQuyclJeXo7y8HAAwPDxssxZrdXh7e8Pb2xsHDhzAw4cPAQAcx+HatWvw8fGxu44lS5bg\n7Nmz4DhOL+7evTvv+RC7jpAO0mFKh54uKRl5dnY2f7P29fXBw8NDsoVgzbk2b94MlmXBsizGx8cR\nEhKyIBXT1dUVTU1NaGpqMmikjx49grOzs910BAcH4+nTp1rX7OnpQUZGBuRyOf7+++95zYc96ojQ\n3w0LC0NmZiZqa2v1yqGjowPffPPNvNePhcyHqXBxcUF4eDjCw8Ph5+c3Lzra2trQ1taGhIQEJCQk\ngGEYJCQkQCaTQSaToa2tDRra2toE61i0Rr5lyxYEBwebPGaukVdXV0u2UlqjxdXVFa2trbyRHzt2\nbMFukNTUVD3T6OzsREtLC0ZGRsBxHNzd3e2iIyQkBHK5nL/ujRs3cOPGDbi6uoJhXpr8n3/+abZX\nMF91xNnZGYGBgQgMDMShQ4dw9epVdHd3IyIiwmYdBQUFmJiYAMuy4DiO/3fu1yzLoqmpCYGBgXbN\nR0BAAG7evIna2lrU1tbi+vXruH79Ot58880FqacrVqxAQ0ODVrS0tPD5GRgYQENDA3744QckJiYi\nMTERX375pag65pq0UGQymaB86Omy1aQFXcTEh127di1UKhVmZ2dx7949ODo6Gj12vo3c09MTDQ0N\nfJJHR0cRFxdnUqPQQrBEh7OzMx48eAAA/I1i6WcRKycMw8DBwQFZWVnIyspCRkYGwsPD+RZ4WVkZ\nOI7D3r17RdcREhKCx48fg+M4qNVqHDp0CI6OjnrlERwcbLK3JnY+dCM2Nha7du1CXV0dmpubDfZa\nRkZGrNYRFBSEyspKTExMGDy3oRgYGLBLPmJjY/H7779jYGDA4HVVKhUuXLhg93q6cuVKZGdn45NP\nPgHDMNixY4fg3MzVmpeXJ0r9kMlkph3bCJI18rnmzHGcybHv+TTy+Ph4dHd3g2VZDA0NYWhoiG/9\nCDEJc4VgiZYTJ04AAIaGhuDh4WHR9cXMydzw9/eHv78//Pz88N5774FhGJw9exYzMzMYGxvDG2+8\nIaoOJycn1NXV8SaelJRklW5bdNy6dQtyuRzFxcUoLi7GxYsX+a8rKipQUVEBpVKJ6elpk4YxNDSE\n+vp6q3RER0fj4cOHeq1uzb8aHRUVFRgYGND6WXR0tGj5cHV1RU5ODlQqFf+56urqUF1drTfMo1Kp\nsH37druVS0xMDMbHx8FxHC5dugSGeTmH0tLSAoVCAYVCgampKUxMTKCqqgr9/f1QKBSYnJzUK5uL\nFy+Kcr+YM3JDP5f00IqukRcWFho9dsOGDZidnbW7ka9duxaTk5NgWRZPnz5FQEAAAgICEB8fD6VS\nOa9GvmXLFnAcB6VSiVWrVlltWrbmxMXFBTk5OTh//jyePXsGpVLJx8zMDMbHxzEzM4P79++bHX+0\nRodMJuNNPCUlxaY8WKtjcHDQ4lbe3PHq3NxcbNy4ES4uLlbp2Lx5s1kNlZWVqKysRFJSEtasWYPK\nykoolUpwHIfbt2+Llo+tW7fy1xwdHcXRo0f5XpmTkxNiY2MRGxvLD7Wp1WoEBQWJXi7BwcEYGxvj\ntaSnpxs8LioqSu97gYGBSE9PR2trK7q6ulBdXQ0vLy9R7peEhAR+aKWtrY0fE9f5nFpoxtCF5GPR\nG7mhroWhY81N4lhbCOvXr4dKpQLLsujt7dUbW3R3d8d/u7usrpRCtYSHh2NkZAQAkJubK/iaYueE\nYRhcuXJFkGGdPn1adB2RkZF8y+/bb7+1OQ/W6ti7d6/WZwVgMAdTU1Nobm7Gvn37kJaWJpoOzbCV\noZa4sTHyiYkJviepVquxcuVKUfLx77//guM4lJeXw83Nzeg5Nb0ojuMET85bomPnzp38+Q8cOICA\ngIAFqx9CY67JmzNxYzoWnZFHRUVhZmaGLwxT3T+NkQ8PD5udGLWmELy9vfHs2TOwLIuRkRGjFS8q\nKgrV1dWorq62aqmdEC2urq6oqakBAFRXV1v08LBHxZzbhdaNsbExDAwMYGBgAOPj44iLixNNh7Oz\nM+rr6/nrGGpZzVc+tm3bpvfZx8fHUVRUhLS0NKSlpSEqKgqhoaGi6/D09NQah+7r6+Nbu0KjtrZW\ntJVe09PTyMrKMvvZ4uPj7WrkmpVTd+7cEbQSZb7uF93QrFoxhLEhFVM6Fp2Rv/XWW7yRV1VVITw8\n3OixGiNXqVRoaWkxOQxjTSGUlpaCZVnMzMwY7KItXboUhYWFGBsb41ePmOsuWmvkubm5AAC5XC5o\nBUZYWBjy8vLw6quv2qVi5ubmQqFQgOM4NDQ0oKioCEVFRXj//fe1ei2//fabqF34jz76iG9hbtmy\nxazO5cuX45dffuFj9+7douhISkrC1NSUljGeOHFCqytubQjRsWbNGq2WN8O8HBpYvXo1H3v27DE5\nRq4xczHyYWwIQzfmrnCyh5H39/fz529vb4efnx8iIiKwZMkSxMTEICwsDGFhYXYrFyFharzc1AiE\nKR2LzsjT09P5SjY+Po4TJ04YPfbIkSP8GPnQ0BCKi4tFK4SgoCBMT0+DZVl+wkQTKSkpSElJgVwu\nx8zMDG/i9+/fN7uCxZqcODs7o7u7GxzHYefOnUaPc3R0xOeff46uri6+q29qotHWiuns7AxnZ2eT\nvYPvv/9eVCMPDQ3ljejw4cNGz+nk5ITMzEx+0ksTMzMz+O6772zWMXfdukKhwI4dOyy+oW3Jx/Hj\nx7UMS8h5o6Oj+eGYuVFQUGCX+sEwDDZu3Ij4+HikpqYiNTWVX1kDADk5OaLlQxO6E6v9/f1QqVTo\n7OzUGlrKyclBTEwMli5dKmq5GAvNWnFjaNaWW6tj0Rn5gQMHtArC1JDJ3GPFnuwMCQnhDTohIQFu\nbm6IiYnBpUuXtCas4uPjMTU1BbVaLahVYk1O0tLSAAD37t0zeW7NcZqHT1lZGQAgNjbWbjequfj+\n++8xOTkpWhfe09MTHMdhdnYWmzZt0vv5qVOncOrUKf7Bp+mxdXR0oKOjA83NzUbriiU6dIdT9u3b\nh/3792Pfvn1aYekmLaE6Kisr+QZPRUWF4HNv2rQJSUlJvKGaGiu3tn4EBwfj9OnT6Ozs1BqnNzQM\nZaoXbY2Od999V9CwkkqlgkqlgkKhMLmiSqz7xZSJCzVwUzp0g961QhAEIXUWukWekpKi9RTfv3+/\nweP8/f21dmbZs0Xe0dHBryEfHh5GXl4e8vLyEBgYiJSUFMzOzpodPjClw5yWS5cuAQDKy8uNHhMV\nFYXm5matjSWaFvpCt8g5jsOuXbtEKRtHR0f89ddfUCqVWt93cnJCTU2NXstrcnISmzdv5o8rKirC\n1NSUwaWblujo6OjQuxZgeNVKR0eHRUtFhbbINee3pEWuiejoaC2Nhs5hTf24fPkyRkdHjbaENTma\nm6uenh7s3bsXy5YtE6WehoaGIjc3l49t27Zh3bp1yMzM5Cehc3NzMTIyArVajeTkZEGb+my5X3TR\nbNW35p4S5LELbeQMw6Cnp4cv5ImJCfzxxx+or69HdnY2srOzce3aNa0hDo7jcObMGVELYa6RsyyL\nR48e4euvv9ZazrR8+XIMDg6CZVnBkz3W5ESzAUgul+ttcd66dSu2bt0KlmXx4sULfiInIyMDarUa\nT548MboES4gOBwcHLFmyxKoK5+LigsbGRjQ0NJi8USzNR3V1tZ6RFxUV6a2cOXPmDLy9vbWO6+3t\nBccZfmWAJTpWrVqFxsZGfsx1amoKSqWS///c73Pcy5VVQiapheqYuxXfGiNnGEZr0tPQOLsl+XBz\nc8OjR4+0VpxVVVWhtLRU6xUKmlyUlpZCrVZrff/Jkyf4+OOPba4fQsPX1xfPnj0Dx3H48MMPRSkX\nE7/LI2RC01Idi9LIly1bZnJ8C9Bu+czOzmL16tV2KYQvvvgCGzduNPizxMRE3jSELoOzJifBwcEY\nHBwEAKjVauTl5WHdunXYvn07vwkHAEpKSrB+/XrU1tbyx0ZGRtqUkyNHjphdpmYsNGvNxZzsZJiX\nDzaWZbXWqL948QIcx6GxsRGNjY1YsWKF3u/l5+eDZVnU1dUZfLDYcqMaW2IYGhrKLwvs7u4W9FAU\nosNca1pI6N5XtuTD399f61y3bt2Cg4MDPvjgA62GGcuy/LZ3FxcXJCcn6xn61atX4evrK0q5mIuG\nhgZwHIeSkhJRysXE7/7/M3J/f39+95mQePDggV0LwViUl5eDZVlkZmbaXAjmfi80NBTV1dWYnp6G\nEDo6OkyauNCcaFosxlY2GIv8/HxMT09jZGTE7KYMS/MRExMDtVqNiYkJpKamgmEYeHl5oaKiAsnJ\nyUhOTjaoRzPBJ8aqFaHh5+entcJFyAYVITpqa2tNtqaFxNwW+f37923Kx1wjHxgYQGhoKEJDQ/mN\nQpo4d+6c3u96e3tDJpNhdHSUb9HPHXawtlwcHR3h7e2t9VDQjU8//RQcx6GmpkaUcjEWupOdttQp\nQR4rhlGbvYgAsZGRkSgtLcXJkycxPDysVRnkcrmgbbj2ukl9fHzg4+ODx48fz/u7ViIjI7Fjxw70\n9fUBAD/0U1VVhZKSEhw9ehSxsbGijflpTEgulwvS5+Ligq+++op/v8i1a9fsUjZ3797ljai3t5df\nwx4REYGIiAi4uroiJCQEERERqKmp4Q3r119/1dsSL3YdCQsL49etP3/+XKvubtiwQZR8XLhwQeu8\nQre7a2LuDkgxxsh9fX35d5UoFAp+Caymx9zc3Izm5maj4+CaSEpKQltbG/++HlvK5fTp0xgaGsLI\nyIjBYS03Nzdcvnx5Xoxcd/OPoS36QkNSRj43vL29ERwczIeHhwffJeI4Dq2trXYtBN0oKSlBSUkJ\nWJZFT0+PUWOwpBAs1ZCcnAwA/A5KJycnUSqE7jGHDx/mDbO4uNjgZgpfX19+uV1jYyPfKktPTzf5\n+lpbyiYsLAz5+flGe2l9fX38hiXNcFxbW5vemLmYdWTnzp24ffu20aHApqYmk9vXLdHh4eHBb35h\nWRZ9fX2CXk3LMC8nOjXDGZoHnKFXPliaj/j4eL03MLa2tlq8vE6scpnbG5DL5WhsbERGRgYfWVlZ\nfCNRTCPXTGTqGrWhJYhi5UMSRq4bQUFBWjPjT58+NdsqFlOH5q/yqFQqg914awrBUg2rVq3C7Ows\nRkdHMTo6KngizdKcODk5oaKigs/19PQ0bty4gd27d2PPnj34+eefMTQ0pHXz3rlzR5Bh2Vo2Tk5O\n2LRpE7q6ujA5Ockbpm6Mjo4iNzfX7MPOGh2urq7Ytm0b2tvbjT5UxsbGIJPJBO/6FKpDt1U9MDCA\nzMxMozsXPT09kZmZqfeAMfYQsCYfmp70+fPnsXbtWqsaGGLVj9TUVL26aSh6e3tNDr9YosPYWwx1\n/2gEGTnzcmv83NbW5OSk2UX9YurQvArzn3/+Ea0QbM2JPW8QJycnlJWVmXy/iuaB0tXVZfHNa2s+\nAgIC8Nprr6GwsBBVVVWoqqpCZ2cnFAoFqqqqsGbNGrvo0Lz/xlA+NBtOHjx4gPj4eLvlo7CwUG/b\n/cTEBPr6+tDX14eCggIUFBSgr6/P4Bb9/v5+oy15qdVT3XB3d0dBQYHeZKrug0zoHJe1Rm4KsfKh\np0sMozZ7EREKNy4uDuXl5ZidncXBgwftVhl04+DBg3xFMLU22tJCkMINkpiYyP+lF42BKZVKHD9+\nnP+rN/OhY7Hkw93dHVeuXOGHtwYHB3Hv3j2cPHkSISEhVu3qtDYft2/fNmpYxuLgwYMmy0yq5aIb\ncXFxOHXqFOrr67WioaEBV69eFbWnZOxFWMawZi25EI91+E+cXfnv3RzzCgAHMXTk5uYyn332GcMw\nDPPOO+8w3d3dNuuwVoutiJUT0rE4dKxcuZI5duwY8/rrrzP/+9//GADM8+fPGYZhmJ6eHsbPz49p\nb29nysrKGABMeXm5XXSIjRR1yGQy/utdu3YZPef58+eZmzdvMj/99JNNOvR0kZGb5scff2Ta29sZ\nhmGYI0eOiKLDWi22IsUbhHSQDtJh3sid50OIlHnllVeY0tLShZZBEARhFGqRL4COxaSFdJAO0iE9\nHbrMi5ETBEEQ9oNeY0sQBCFxyMgJgiAkDhk5QRCExCEjJwiCkDhk5ARBEBKHjJwgCELikJETBEFI\nHDJygiAIiUNGThAEIXHIyAmCICQOGTlBEITEISMnCIKQOGTkBEEQEoeMnCAIQuKQkRMEQUgcMnKC\nIAiJQ0ZOEAQhccjICYIgJA4ZOUEQhMQhIycIgpA4ZOQEQRASh4ycIAhC4pCREwRBSJz/A5iXevda\n0rgaAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fd122967fd0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "Yyw4JU0qb6AK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model"
      ]
    },
    {
      "metadata": {
        "id": "uH2ogBmsjgsW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Variables are place holders for input arrays. We give each variable a unique name.\n",
        "data = mx.symbol.Variable('data')\n",
        "\n",
        "# The input is fed to a fully connected layer that computes Y=WX+b.\n",
        "# This is the main computation module in the network.\n",
        "# Each layer also needs an unique name. We'll talk more about naming in the next section.\n",
        "fc1  = mx.symbol.FullyConnected(data = data, name='fc1', num_hidden=128)\n",
        "# Activation layers apply a non-linear function on the previous layer's output.\n",
        "# Here we use Rectified Linear Unit (ReLU) that computes Y = max(X, 0).\n",
        "act1 = mx.symbol.Activation(data = fc1, name='relu1', act_type=\"relu\")\n",
        "\n",
        "fc2  = mx.symbol.FullyConnected(data = act1, name = 'fc2', num_hidden = 64)\n",
        "act2 = mx.symbol.Activation(data = fc2, name='relu2', act_type=\"relu\")\n",
        "\n",
        "fc3  = mx.symbol.FullyConnected(data = act2, name='fc3', num_hidden=10)\n",
        "# Finally we have a loss layer that compares the network's output with label and generates gradient signals.\n",
        "mlp  = mx.symbol.SoftmaxOutput(data = fc3, name = 'softmax')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r5OuMYjlidYs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 100\n",
        "train_iter = mx.io.NDArrayIter(X_train, Y_train, batch_size=batch_size)\n",
        "test_iter = mx.io.NDArrayIter(X_test, Y_test, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5RIkcnamc56E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "fe5ab456-a5b2-4ead-c983-f34943ccb692"
      },
      "cell_type": "code",
      "source": [
        "# ==================Binding=====================\n",
        "# The symbol we created is only a graph description.\n",
        "# To run it, we first need to allocate memory and create an executor by 'binding' it.\n",
        "# In order to bind a symbol, we need at least two pieces of information: context and input shapes.\n",
        "# Context specifies which device the executor runs on, e.g. cpu, GPU0, GPU1, etc.\n",
        "# Input shapes define the executor's input array dimensions.\n",
        "# MXNet then run automatic shape inference to determine the dimensions of intermediate and output arrays.\n",
        "\n",
        "# data iterators defines shapes of its output with provide_data and provide_label property.\n",
        "input_shapes = dict(train_iter.provide_data+train_iter.provide_label)\n",
        "print ('input_shapes', input_shapes)\n",
        "# We use simple_bind to let MXNet allocate memory for us.\n",
        "# You can also allocate memory youself and use bind to pass it to MXNet.\n",
        "exe = mlp.simple_bind(ctx=mx.gpu(0), **input_shapes)\n",
        "\n",
        "# ===============Initialization=================\n",
        "# First we get handle to input arrays\n",
        "arg_arrays = dict(zip(mlp.list_arguments(), exe.arg_arrays))\n",
        "data = arg_arrays[train_iter.provide_data[0][0]]\n",
        "label = arg_arrays[train_iter.provide_label[0][0]]\n",
        "\n",
        "# We initialize the weights with uniform distribution on (-0.01, 0.01).\n",
        "init = mx.init.Uniform(scale=0.01)\n",
        "for name, arr in arg_arrays.items():\n",
        "    if name not in input_shapes:\n",
        "        init(name, arr)\n",
        "    \n",
        "# We also need to create an optimizer for updating weights\n",
        "#opt = mx.optimizer.SGD(\n",
        "    #learning_rate=0.1,rescale_grad=1.0/train_iter.batch_size,lr_scheduler=sched)\n",
        "    #momentum=0.9,\n",
        "    #wd=0.00001,\n",
        "    #rescale_grad=1.0/train_iter.batch_size)\n",
        "#updater = mx.optimizer.get_updater(opt)\n",
        "\n",
        "# Finally we need a metric to print out training progress\n",
        "metric = mx.metric.CrossEntropy() "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input_shapes {'data': (100, 784), 'softmax_label': (100,)}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: DeprecationWarning: \u001b[91mCalling initializer with init(str, NDArray) has been deprecated.please use init(mx.init.InitDesc(...), NDArray) instead.\u001b[0m\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "kz3Vl4fEBMHc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2a194800-4182-41fa-b747-7406fb01aabb"
      },
      "cell_type": "code",
      "source": [
        "nb = len(X_train)/batch_size\n",
        "nb"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "600.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "0F0xvefTBYIn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sched=lr_find(1e-3,nb,end_lr=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Jx1Odqm3iUv1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# We also need to create an optimizer for updating weights\n",
        "opt = mx.optimizer.SGD(\n",
        "    learning_rate=1,\n",
        "    momentum=0.9,\n",
        "    wd=0.00001,\n",
        "    rescale_grad=1.0/train_iter.batch_size,lr_scheduler=sched)\n",
        "\n",
        "updater = mx.optimizer.get_updater(opt)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SL1_5crjqzYB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "metric = mx.metric.CrossEntropy() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v0YaADpfjOfn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "50b25afc-13ac-42cf-ad27-961eb81f26da"
      },
      "cell_type": "code",
      "source": [
        "counter=0\n",
        "# Training loop begines\n",
        "for epoch in range(1):\n",
        "    train_iter.reset()\n",
        "    metric.reset()\n",
        "    sched.reset()\n",
        "    t = 0\n",
        "    sched.on_train_begin()\n",
        "\n",
        "    for batch in train_iter:\n",
        "\n",
        "        data[:] = batch.data[0]\n",
        "        label[:] = batch.label[0]\n",
        "        outputs=exe.forward(is_train=True)\n",
        "        exe.backward()\n",
        "        \n",
        "        # Update\n",
        "        tmp=0\n",
        "\n",
        "        for i, pair in enumerate(zip(exe.arg_arrays, exe.grad_arrays)):\n",
        "            weight, grad = pair\n",
        "            updater(i, grad, weight)\n",
        "            tmp+=1\n",
        "        counter+=1   \n",
        "        metric.update(batch.label, exe.outputs)\n",
        "        e=metric.get()\n",
        "        err_train=e[1]\n",
        "        sched.on_batch_end(err_train)#loss)\n",
        "\n",
        "        t += 1\n",
        "        if t % 100 == 0:\n",
        "            print('epoch:', epoch, 'iter:', t, 'metric:', metric.get())\n",
        "  "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0 iter: 100 metric: ('cross-entropy', 2.299421466064453)\n",
            "epoch: 0 iter: 200 metric: ('cross-entropy', 1.7072034585952758)\n",
            "epoch: 0 iter: 300 metric: ('cross-entropy', 1.3090668846766154)\n",
            "epoch: 0 iter: 400 metric: ('cross-entropy', 1.1494513531684876)\n",
            "epoch: 0 iter: 500 metric: ('cross-entropy', 1.831294879837036)\n",
            "epoch: 0 iter: 600 metric: ('cross-entropy', 1.9204962469100952)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_2-5xWIEkDNX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "6a0a4d0c-9000-4f43-dabb-eb9f766fc0b2"
      },
      "cell_type": "code",
      "source": [
        "plt.plot(sched.lrs, sched.losses)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fd0fc953a58>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD4CAYAAADxeG0DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n5xTkIiJxTkEuIhLn/j80eP4gGH9D+AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fd120376828>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "6QLCzeUKaloq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "From the graph, we can see that the loss rate drop is larges at .5. Hence, we choose it to be our learning rate"
      ]
    },
    {
      "metadata": {
        "id": "mObvMi_3fAgn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Learning with the suggested learning rate"
      ]
    },
    {
      "metadata": {
        "id": "Tt18if5ka1He",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# We also need to create an optimizer for updating weights\n",
        "opt = mx.optimizer.SGD(\n",
        "    learning_rate=.5,rescale_grad=1.0/train_iter.batch_size)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AWrSRwpjiubo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "updater.optimizer.lr_scheduler=None\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ls_P3uZBi1ja",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "updater = mx.optimizer.get_updater(opt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GDD1lU71nP81",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "metric = mx.metric.Accuracy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H89BuIoiyovn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "3bff27be-2ac9-435e-f0c9-4a4031a8514d"
      },
      "cell_type": "code",
      "source": [
        "# We initialize the weights with uniform distribution on (-0.01, 0.01).\n",
        "init = mx.init.Uniform(scale=0.01)\n",
        "for name, arr in arg_arrays.items():\n",
        "    if name not in input_shapes:\n",
        "        init(name, arr)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: \u001b[91mCalling initializer with init(str, NDArray) has been deprecated.please use init(mx.init.InitDesc(...), NDArray) instead.\u001b[0m\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "k6c9fkQTc5Wi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1025
        },
        "outputId": "9ce8cf40-fdc5-47e9-a5b2-8dee9180a3b1"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Training loop begines\n",
        "for epoch in range(10):\n",
        "    train_iter.reset()\n",
        "    metric.reset()\n",
        "    t = 0\n",
        "    for batch in train_iter:\n",
        "        # Copy data to executor input. Note the [:].\n",
        "        data[:] = batch.data[0]\n",
        "        label[:] = batch.label[0]\n",
        "        \n",
        "        # Forward\n",
        "        outputs=exe.forward(is_train=True)\n",
        "        \n",
        "        # You perform operations on exe.outputs here if you need to.\n",
        "        # For example, you can stack a CRF on top of a neural network.\n",
        "        \n",
        "        # Backward\n",
        "        exe.backward()\n",
        "        \n",
        "        # Update\n",
        "        for i, pair in enumerate(zip(exe.arg_arrays, exe.grad_arrays)):\n",
        "            weight, grad = pair\n",
        "            updater(i, grad, weight)\n",
        "        metric.update(batch.label, exe.outputs)\n",
        "        t += 1\n",
        "        if t % 100 == 0:\n",
        "            print('epoch:', epoch, 'iter:', t, 'metric:', metric.get())"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0 iter: 100 metric: ('accuracy', 0.1229)\n",
            "epoch: 0 iter: 200 metric: ('accuracy', 0.279)\n",
            "epoch: 0 iter: 300 metric: ('accuracy', 0.45813333333333334)\n",
            "epoch: 0 iter: 400 metric: ('accuracy', 0.56845)\n",
            "epoch: 0 iter: 500 metric: ('accuracy', 0.63782)\n",
            "epoch: 0 iter: 600 metric: ('accuracy', 0.6868833333333333)\n",
            "epoch: 1 iter: 100 metric: ('accuracy', 0.9413)\n",
            "epoch: 1 iter: 200 metric: ('accuracy', 0.93995)\n",
            "epoch: 1 iter: 300 metric: ('accuracy', 0.9443666666666667)\n",
            "epoch: 1 iter: 400 metric: ('accuracy', 0.946875)\n",
            "epoch: 1 iter: 500 metric: ('accuracy', 0.94878)\n",
            "epoch: 1 iter: 600 metric: ('accuracy', 0.9508833333333333)\n",
            "epoch: 2 iter: 100 metric: ('accuracy', 0.9634)\n",
            "epoch: 2 iter: 200 metric: ('accuracy', 0.9616)\n",
            "epoch: 2 iter: 300 metric: ('accuracy', 0.9643)\n",
            "epoch: 2 iter: 400 metric: ('accuracy', 0.9652)\n",
            "epoch: 2 iter: 500 metric: ('accuracy', 0.96646)\n",
            "epoch: 2 iter: 600 metric: ('accuracy', 0.9677666666666667)\n",
            "epoch: 3 iter: 100 metric: ('accuracy', 0.9737)\n",
            "epoch: 3 iter: 200 metric: ('accuracy', 0.97225)\n",
            "epoch: 3 iter: 300 metric: ('accuracy', 0.9747)\n",
            "epoch: 3 iter: 400 metric: ('accuracy', 0.97465)\n",
            "epoch: 3 iter: 500 metric: ('accuracy', 0.9755)\n",
            "epoch: 3 iter: 600 metric: ('accuracy', 0.97615)\n",
            "epoch: 4 iter: 100 metric: ('accuracy', 0.9812)\n",
            "epoch: 4 iter: 200 metric: ('accuracy', 0.97905)\n",
            "epoch: 4 iter: 300 metric: ('accuracy', 0.9801666666666666)\n",
            "epoch: 4 iter: 400 metric: ('accuracy', 0.980475)\n",
            "epoch: 4 iter: 500 metric: ('accuracy', 0.98084)\n",
            "epoch: 4 iter: 600 metric: ('accuracy', 0.98145)\n",
            "epoch: 5 iter: 100 metric: ('accuracy', 0.9851)\n",
            "epoch: 5 iter: 200 metric: ('accuracy', 0.98305)\n",
            "epoch: 5 iter: 300 metric: ('accuracy', 0.9840666666666666)\n",
            "epoch: 5 iter: 400 metric: ('accuracy', 0.98415)\n",
            "epoch: 5 iter: 500 metric: ('accuracy', 0.9843)\n",
            "epoch: 5 iter: 600 metric: ('accuracy', 0.9844333333333334)\n",
            "epoch: 6 iter: 100 metric: ('accuracy', 0.9851)\n",
            "epoch: 6 iter: 200 metric: ('accuracy', 0.98565)\n",
            "epoch: 6 iter: 300 metric: ('accuracy', 0.9866666666666667)\n",
            "epoch: 6 iter: 400 metric: ('accuracy', 0.986675)\n",
            "epoch: 6 iter: 500 metric: ('accuracy', 0.98712)\n",
            "epoch: 6 iter: 600 metric: ('accuracy', 0.9876833333333334)\n",
            "epoch: 7 iter: 100 metric: ('accuracy', 0.9883)\n",
            "epoch: 7 iter: 200 metric: ('accuracy', 0.98775)\n",
            "epoch: 7 iter: 300 metric: ('accuracy', 0.9885)\n",
            "epoch: 7 iter: 400 metric: ('accuracy', 0.9887)\n",
            "epoch: 7 iter: 500 metric: ('accuracy', 0.98918)\n",
            "epoch: 7 iter: 600 metric: ('accuracy', 0.9894333333333334)\n",
            "epoch: 8 iter: 100 metric: ('accuracy', 0.9902)\n",
            "epoch: 8 iter: 200 metric: ('accuracy', 0.99005)\n",
            "epoch: 8 iter: 300 metric: ('accuracy', 0.9897666666666667)\n",
            "epoch: 8 iter: 400 metric: ('accuracy', 0.989875)\n",
            "epoch: 8 iter: 500 metric: ('accuracy', 0.98986)\n",
            "epoch: 8 iter: 600 metric: ('accuracy', 0.9902)\n",
            "epoch: 9 iter: 100 metric: ('accuracy', 0.9905)\n",
            "epoch: 9 iter: 200 metric: ('accuracy', 0.9914)\n",
            "epoch: 9 iter: 300 metric: ('accuracy', 0.9920333333333333)\n",
            "epoch: 9 iter: 400 metric: ('accuracy', 0.991675)\n",
            "epoch: 9 iter: 500 metric: ('accuracy', 0.9919)\n",
            "epoch: 9 iter: 600 metric: ('accuracy', 0.9919166666666667)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Mtz29_n_fM_R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Learning with the a guessed learning rate"
      ]
    },
    {
      "metadata": {
        "id": "Vb_ppn7dcAHI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# We also need to create an optimizer for updating weights\n",
        "opt = mx.optimizer.SGD(\n",
        "    learning_rate=0.1,\n",
        "    momentum=0.9,\n",
        "    wd=0.00001,\n",
        "    rescale_grad=1.0/train_iter.batch_size)\n",
        "#updater.optimizer.lr_scheduler=None\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LczKf5uycAL5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "updater = mx.optimizer.get_updater(opt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5_Hix2Znbv8g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "metric = mx.metric.Accuracy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bTiwj9wTyy8m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "678fabfa-3d95-4531-924f-b7bf5aa39c9d"
      },
      "cell_type": "code",
      "source": [
        "# We initialize the weights with uniform distribution on (-0.01, 0.01).\n",
        "init = mx.init.Uniform(scale=0.01)\n",
        "for name, arr in arg_arrays.items():\n",
        "    if name not in input_shapes:\n",
        "        init(name, arr)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: \u001b[91mCalling initializer with init(str, NDArray) has been deprecated.please use init(mx.init.InitDesc(...), NDArray) instead.\u001b[0m\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "AKn2cnMfakR6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1025
        },
        "outputId": "789408c8-2af6-46b9-dd6f-3832d2a1543e"
      },
      "cell_type": "code",
      "source": [
        "# Training loop begines\n",
        "for epoch in range(10):\n",
        "    train_iter.reset()\n",
        "    metric.reset()\n",
        "    t = 0\n",
        "    for batch in train_iter:\n",
        "        # Copy data to executor input. Note the [:].\n",
        "        data[:] = batch.data[0]\n",
        "        label[:] = batch.label[0]\n",
        "        \n",
        "        # Forward\n",
        "        outputs=exe.forward(is_train=True)\n",
        "        \n",
        "        # You perform operations on exe.outputs here if you need to.\n",
        "        # For example, you can stack a CRF on top of a neural network.\n",
        "        \n",
        "        # Backward\n",
        "        exe.backward()\n",
        "        \n",
        "        # Update\n",
        "        for i, pair in enumerate(zip(exe.arg_arrays, exe.grad_arrays)):\n",
        "            weight, grad = pair\n",
        "            updater(i, grad, weight)\n",
        "        metric.update(batch.label, exe.outputs)\n",
        "        t += 1\n",
        "        if t % 100 == 0:\n",
        "            print('epoch:', epoch, 'iter:', t, 'metric:', metric.get())"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0 iter: 100 metric: ('accuracy', 0.1451)\n",
            "epoch: 0 iter: 200 metric: ('accuracy', 0.4331)\n",
            "epoch: 0 iter: 300 metric: ('accuracy', 0.5851)\n",
            "epoch: 0 iter: 400 metric: ('accuracy', 0.670275)\n",
            "epoch: 0 iter: 500 metric: ('accuracy', 0.72354)\n",
            "epoch: 0 iter: 600 metric: ('accuracy', 0.7607333333333334)\n",
            "epoch: 1 iter: 100 metric: ('accuracy', 0.9475)\n",
            "epoch: 1 iter: 200 metric: ('accuracy', 0.9506)\n",
            "epoch: 1 iter: 300 metric: ('accuracy', 0.9530666666666666)\n",
            "epoch: 1 iter: 400 metric: ('accuracy', 0.955)\n",
            "epoch: 1 iter: 500 metric: ('accuracy', 0.95706)\n",
            "epoch: 1 iter: 600 metric: ('accuracy', 0.9583666666666667)\n",
            "epoch: 2 iter: 100 metric: ('accuracy', 0.9688)\n",
            "epoch: 2 iter: 200 metric: ('accuracy', 0.96735)\n",
            "epoch: 2 iter: 300 metric: ('accuracy', 0.9693333333333334)\n",
            "epoch: 2 iter: 400 metric: ('accuracy', 0.970075)\n",
            "epoch: 2 iter: 500 metric: ('accuracy', 0.97054)\n",
            "epoch: 2 iter: 600 metric: ('accuracy', 0.9711)\n",
            "epoch: 3 iter: 100 metric: ('accuracy', 0.9741)\n",
            "epoch: 3 iter: 200 metric: ('accuracy', 0.9748)\n",
            "epoch: 3 iter: 300 metric: ('accuracy', 0.9758333333333333)\n",
            "epoch: 3 iter: 400 metric: ('accuracy', 0.97635)\n",
            "epoch: 3 iter: 500 metric: ('accuracy', 0.97616)\n",
            "epoch: 3 iter: 600 metric: ('accuracy', 0.9768166666666667)\n",
            "epoch: 4 iter: 100 metric: ('accuracy', 0.978)\n",
            "epoch: 4 iter: 200 metric: ('accuracy', 0.978)\n",
            "epoch: 4 iter: 300 metric: ('accuracy', 0.9794)\n",
            "epoch: 4 iter: 400 metric: ('accuracy', 0.979425)\n",
            "epoch: 4 iter: 500 metric: ('accuracy', 0.98004)\n",
            "epoch: 4 iter: 600 metric: ('accuracy', 0.98045)\n",
            "epoch: 5 iter: 100 metric: ('accuracy', 0.9821)\n",
            "epoch: 5 iter: 200 metric: ('accuracy', 0.98175)\n",
            "epoch: 5 iter: 300 metric: ('accuracy', 0.9822333333333333)\n",
            "epoch: 5 iter: 400 metric: ('accuracy', 0.9826)\n",
            "epoch: 5 iter: 500 metric: ('accuracy', 0.9832)\n",
            "epoch: 5 iter: 600 metric: ('accuracy', 0.9838)\n",
            "epoch: 6 iter: 100 metric: ('accuracy', 0.9841)\n",
            "epoch: 6 iter: 200 metric: ('accuracy', 0.98475)\n",
            "epoch: 6 iter: 300 metric: ('accuracy', 0.9855333333333334)\n",
            "epoch: 6 iter: 400 metric: ('accuracy', 0.98565)\n",
            "epoch: 6 iter: 500 metric: ('accuracy', 0.98552)\n",
            "epoch: 6 iter: 600 metric: ('accuracy', 0.9857666666666667)\n",
            "epoch: 7 iter: 100 metric: ('accuracy', 0.9867)\n",
            "epoch: 7 iter: 200 metric: ('accuracy', 0.9862)\n",
            "epoch: 7 iter: 300 metric: ('accuracy', 0.9855)\n",
            "epoch: 7 iter: 400 metric: ('accuracy', 0.98575)\n",
            "epoch: 7 iter: 500 metric: ('accuracy', 0.98588)\n",
            "epoch: 7 iter: 600 metric: ('accuracy', 0.9864666666666667)\n",
            "epoch: 8 iter: 100 metric: ('accuracy', 0.9862)\n",
            "epoch: 8 iter: 200 metric: ('accuracy', 0.9862)\n",
            "epoch: 8 iter: 300 metric: ('accuracy', 0.9864333333333334)\n",
            "epoch: 8 iter: 400 metric: ('accuracy', 0.986925)\n",
            "epoch: 8 iter: 500 metric: ('accuracy', 0.9874)\n",
            "epoch: 8 iter: 600 metric: ('accuracy', 0.9875666666666667)\n",
            "epoch: 9 iter: 100 metric: ('accuracy', 0.987)\n",
            "epoch: 9 iter: 200 metric: ('accuracy', 0.98745)\n",
            "epoch: 9 iter: 300 metric: ('accuracy', 0.9883)\n",
            "epoch: 9 iter: 400 metric: ('accuracy', 0.9886)\n",
            "epoch: 9 iter: 500 metric: ('accuracy', 0.98896)\n",
            "epoch: 9 iter: 600 metric: ('accuracy', 0.9891333333333333)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mD8iulTmn_F2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "with the learn rate finder, we got an accuracy of .9919 whereas with the guessed learning rate we got .98896 accuracy."
      ]
    },
    {
      "metadata": {
        "id": "btQp60t-ldpG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}